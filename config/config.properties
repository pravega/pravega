#
# Copyright Pravega Authors.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

## Instructions for using this file:
# 1. Settings are namespaced. The prefix indicates the component for which the configuration applies.
# 2. Most settings have default values hardcoded inside the code. Those are commented out (with the default value set)
#    in this file.
# 3. Some settings do not have overrides and require a value be set in this file (or via command-line). Those settings
#    are labeled with the label 'Required.' in this file.
# 4. Each setting (especially non-string ones) have a label named 'Valid values' which describes how a valid value should
#    look like.
# 5. Most settings have 'Recommended values'. These are guidelines (but not requirements) as to how to properly set the
#    values for those settings based on desired use cases.
#
# Each of these settings can be overridden from the command like by specifying a Java System Property.
# Example: setting 'pravegaservice.service.listener.port' has a default value of 12345, but it can be overridden to a different
#          value via the command-line:
#                java -Dpravegaservice.service.listener.port=5689 ...
#          which will set the value for that setting to 5689, regardless of what this file contains.

##region General Segment Store Settings

# Name of the cluster to which this instance of the SegmentStore belongs.
# Required.
pravegaservice.cluster.name=pravega-cluster

# Number of Segment Containers in the system. This value must be the same across all SegmentStore instances in this cluster.
# This value cannot be changed dynamically. See documentation for details.
# Valid values: Positive integer.
# Required.
pravegaservice.container.count=4

# Number of Segment Containers that a Segment Store will start (and recover) in parallel. The main objective of this
# parameter is to prevent multiple Segment Container recoveries in parallel to overload the cache. A higher value may
# speed up recoveries, but sufficient cache size should be configured for safety.
# Valid values: Positive integer.
#pravegaservice.container.parallelStarts=2

# Maximum number of threads in the Core SegmentStore Thread Pool. This pool is used for all SegmentStore-related
# activities, except Netty-related tasks and Long Term Storage activities. Examples include: handling inbound requests,
# processing reads, background maintenance operations and background operation processing.
# Valid values: Positive integer.
# Recommended setting: 2 * Number of containers per node, minimum 20.
#pravegaservice.threadPool.core.size=30

# Maximum number of threads in the Thread Pool used for Long Term Storage tasks (reading, writing, create, delete, etc.).
# Valid values: Positive integer.
# Recommended setting: reasonably large number which does not cause thrashing, minimum 20, recommended value 200.
#pravegaservice.threadPool.storage.size=200

# TCP port where the SegmentStore will be listening for incoming requests.
# Valid values: Positive integer in the valid TCP port ranges.
pravegaservice.service.listener.port=12345

# Full URL (host:port) where to find a ZooKeeper that can be used for coordinating this Pravega Cluster.
# Required.
pravegaservice.zk.connect.uri=localhost:2181

# Number of milliseconds to wait between ZooKeeper retries.
# Valid values: Positive integer.
# Recommended values: Between 5000 and 10000 (too high will cause failover delays).
#pravegaservice.zk.connect.retries.interval.milliseconds=5000

# Number of times to retry a failed attempt to contact ZooKeeper.
# Valid values: Positive integer.
# Recommended values: Between 3 and 5.
#pravegaservice.zk.connect.retries.count.max=5

# Session timeout for ZooKeeper.
# Valid values: Positive integer.
#pravegaservice.zk.connect.sessionTimeout.milliseconds=10000

# Enable security for the ZK client.
#pravegaservice.zk.connect.security.enable=false

# Location of trust store for secure connection to ZK.
#pravegaservice.zk.connect.security.tls.trustStore.location=

#Location of password file to access the trust store.
#pravegaservice.zk.connect.security.tls.trustStore.pwd.location=

# DataLog implementation for Durable Data Log Storage.
# Valid values: BOOKKEEPER, INMEMORY.
# Default value: BOOKKEEPER
pravegaservice.dataLog.impl.name=BOOKKEEPER

# Storage implementation for Long-Term Storage.
# Valid values: HDFS, FILESYSTEM, EXTENDEDS3, INMEMORY.
# Default value: HDFS
# pravegaservice.storage.impl.name=HDFS


# Storage layout for Tier 2 storage.
# Valid values:
#   CHUNKED_STORAGE - Using ChunkedSegmentStorage.
#   ROLLING_STORAGE - Using RollingStorage.
# Default value: ROLLING_STORAGE
# pravegaservice.storage.layout=ROLLING_STORAGE
# NOTE: ChunkedSegmentStorage is an experimental feature. Its behavior and/or API should be expected to be in flux until fully released.

# Size of chunk in bytes above which it is no longer considered a small object.
# For small source objects, concat is not used.
# Valid values: non-negative long
# Default value: 0
# storage.concat.size.bytes.min=0

# Max size of chunk in bytes above which it is no longer considered for concat.
# Valid values: non-negative long
# Default value: 9223372036854775807
# storage.concat.size.bytes.max=9223372036854775807

# Maximum size for the buffer used while copying of data from one chunk to other.
# Valid values: non-negative integer
# Default value: 1048576 (1 MB)
# storage.appends.buffer.size.bytes.max=1048576

# Max number of indexed segments to keep in read cache.
# Valid values: non-negative integer
# Default value: 1024 (1 K)
# storage.readindex.segments.max=1024

# Max number of indexed chunks to keep per segment in read cache.
# Valid values: non-negative integer
# Default value: 1024 (1 K)
# storage.readindex.chunksPerSegment.max=1024

# Max number of indexed chunks to keep in cache.
# Valid values: non-negative integer
# Default value: 16384 (16 K)
# storage.readindex.chunks.max=16384

# The maximum size of a single Segment Chunk in Storage for metadata segments.
# Valid values: non-negative long less than 4611686018427387904.
# Default value: 134217728 (128 MB)
# storage.metadata.rollover.size.bytes.max=134217728

# Whether the append functionality is enabled.
# Valid values: true, false
# Default value: true
# storage.appends.enable=true

# Storage NO-OP Mode: in No-Op mode, user stream segment writing is no-oped; user stream segment reading is not supported.
# This mode is used to avoid storage interference in testing while still keep the system functioning as usual.
# NOTE: pravegaservice.storage.impl.name is still used to store metadata and system segments, which are required for the functioning of the Pravega Cluster.
# WARNING: Do NOT set NO-OP Mode to true in production environment! Data loss will happen!
# Valid values: true, false
# Default value: false
#storageextra.noOp.mode.enable=false

# When Storage NO-OP Mode is true, storageWriteNoOpModeLatencyMillis is applied to write operation.
# It is used to compensate the supposed latency if the storage is in operation, in order to make the test as real as possible.
# This latency has no effect if storage is not in No-Op mode.
# Valid values: non-negative integer
# Default vale: 20 (milliseconds)
#storageextra.noOp.write.latency.milliseconds=20

# Whether to start the SegmentStore in ReadOnly mode. ReadOnly mode means that only Read and GetSegmentInfo are allowed
# and all requests are served directly from Long-Term Storage. There is no Durable Data Log access, nor are any modify operations allowed.
# If set to 'true', the SegmentStore will not host any SegmentContainers and will register itself under the Controller
# as 'ReadOnly', so that the Client can request access to it when needed.
# NOTE: This is an experimental feature. Its behavior and/or API should be expected to be in flux until fully released.
# Valid values: true, false
# Default value: false
#pravegaservice.readOnly.enable=false

# Maximum size (in bytes) for the Local Shared Cache (shared by all Segment Containers on this Segment Store instance).
# Valid values: Positive integer.
# Recommended values: Multiples of 1GB, but less than XX:MaxDirectMemorySize. Choosing a lower size will conserve memory
# at the expense of a lot of cache misses (resulting in Long Term Storage reads and possibly high latencies). Choosing a higher size
# will keep data in the cache for longer, but at the expense of memory. The Segment Store uses a Direct Memory Cache backed
# by direct ByteBuffers, so if XX:MaxDirectMemorySize is not set to a value higher than this one, the process will
# eventually crash with an OutOfMemoryError.
#pravegaservice.cache.size.max=4294967296

# Percentage (of pravegaservice.cache.size.max) that defines target Local Shared Cache. The Segment Store will try to keep
# the cache utilization at or below this value, and may apply throttling on new operations if it exceeds it.
# Valid values: 1 to 100 (inclusive).
# Recommended values: Between 75 and 85. It is not recommended to choose a very high value as that may leave very little
# buffer for the Segment Store to attempt any cleaning, apply back-pressure or serve other random requests. Consequently,
# a very low value may have negative impact on the ingestion performance.
#pravegaservice.cache.utilization.percent.target=75

# Percentage (of pravegaservice.cache.size.max) that defines the maximum usable size of the Local Shared Cache. The Segment
# Store will apply full throttling if reaching or exceeding this value.
# Valid values: 1 to 100 (inclusive).
# Recommended values: Between {pravegaservice.cache.utilization.percent.target + 10} and 95. Setting a value too close to
# pravegaservice.cache.utilization.percent.target will cause the Segment Store to apply maximum throttling as soon as that
# limit is reached, which may cause it to have sudden, multi-second stalls in the ingestion pipeline. It is not recommended
# to set this value ever to 100, since that would mean the cache is already full when max throttling kicks in, and the
# Segment Store will likely shut down Segment Containers in order to alleviate cache pressure.
#pravegaservice.cache.utilization.percent.max=90

# Maximum amount of time (in seconds) to keep a block of data in the Cache.
# Valid values: Positive integer.
# Recommended values: Between 300 and 3600 seconds. Choosing lower values will keep the cache size low, ensuring that an
# eviction is less likely to happen should the load pick up quickly, at the expense of possibly increasing cache misses.
# Choosing a higher value will keep data in the cache for longer, reducing chances of cache misses, but possibly incurring
# cache evictions when load picks up again.
#pravegaservice.cache.time.seconds.max=1800

# The length of a Cache Generation, in seconds.
# Valid values: positive integer.
# Recommended values: multiples of 1 second. Choosing lower values will cause Cache evictions to happen more frequently,
# thus increasing overhead, but it will provide more granularity for busy systems.
#pravegaservice.cache.generation.duration.seconds=1

# This setting allows Pravega to send server-side stack traces to client as part of the response message on errors. This
# setting may be useful for debugging purposes, as users may understand the root cause of a server exception inspecting
# only client-side logs. However, we recommend to be conservative on activating this option as it exposes server-side
# information to clients, which may raise security concerns.
#pravegaservice.request.replyWithStackTraceOnError.enable=false

##endregion

##region AutoScaler Settings

# URI for the Pravega Controller
#autoScale.controller.connect.uri=pravega://localhost:9090

# A Stream name for internal purposes. Not externally visible.
#autoScale.requestStream.name=_requeststream

#autoScale.cooldown.time.seconds=600
#autoScale.mute.time.seconds=600
#autoScale.cache.cleanUp.interval.seconds=300
#autoScale.cache.expiry.seconds=1200

##endregion

##region Metrics Settings

# Whether to enable Metrics Reporting from this Pravega SegmentStore. This is the master switch for all metrics-related
# activities (if this is disabled, then all metrics-related services are too, regardless of their individual settings).
# Valid values: 'true' or 'false'.
metrics.statistics.enable=false

# Number of dynamic metrics statistic objects to keep in memory at once.
# Valid values: Positive integer.
# Recommended values: 10000 to 1000000. The higher the value, the more memory used, but it may improve performance slightly
# in scenarios where there are large number of active Stream Segments.
#metrics.dynamicCache.size=100000

# Number of minutes to keep dynamic metric statistic objects in memory.
# Valid values: Positive integer.
# Recommended values: 1 to 60. The higher the value, the more memory used, but it may improve performance slightly
# in scenarios where there are large number of active Stream Segments.
#metrics.dynamicCache.eviction.duration.minutes=3

# Number of seconds between statistics reports.
# Valid values: Positive integer.
# Recommended values: A smaller value will produce more fine-grained reports, at the possible expense of extra CPU and IO
# overhead, while larger values will reduce overhead at the expense of more coarse reports.
#metrics.output.frequency.seconds=60

# Prefix to add to all Metrics counters.
#metrics.metrics.prefix=pravega

# Whether to enable StatsD reporting.
# Valid values: 'true' or 'false'.
#metrics.statsD.reporter.enable=false

# StatsD endpoint host name.
#metrics.statsD.connect.host=localhost

# StatsD endpoint port.
# Valid values: Positive integer.
#metrics.statsD.connect.port=8125

# Whether to enable direct InfluxDB reporting (without local agent).
#metrics.influxDB.reporter.enable=true

# InfluxDB endpoint URI.
#metrics.influxDB.connect.uri=http://localhost:8086

# InfluxDB database name.
#metrics.influxDB.connect.db.name=pravega

# InfluxDB user name.
#metrics.influxDB.connect.credentials.username=

# InfluxDB password.
#metrics.influxDB.connect.credentials.pwd=

# InfluxDB retention policy (e.g. 2h, 52w).
#metrics.influxDB.retention=

# Whether to enable Graphite reporting.
# Valid values: 'true' or 'false'.
#metrics.enableGraphiteReporter=false

# Graphite endpoint host name.
#metrics.graphiteHost=localhost

# Graphite endpoint port.
# Valid values: Positive integer.
#metrics.graphitePort=2003

# Whether to enable JMX reporting.
# Valid values: 'true' or 'false'.
#metrics.enableJMXReporter=false

# JMX domain.
#metrics.jmxDomain=domain

# Whether to enable Ganglia reporting.
# Valid values: 'true' or 'false'.
#metrics.enableGangliaReporter=false

# Ganglia endpoint host name.
#metrics.gangliaHost=localhost

# Graphite endpoint port.
# Valid values: Positive integer.
#metrics.gangliaPort=8649

# Whether to enable Console reporting.
# Recommended values: 'false'. Set to 'true' only for debugging purposes.
#metrics.enableConsoleReporter=false

##endregion

##region BookKeeper Settings

# Endpoint address (hostname:port) where the ZooKeeper controlling BookKeeper for this cluster can be found at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
#bookkeeper.zk.connect.uri=localhost:2181

# Default root path for BookKeeper Ledger metadata. This property tells the BookKeeper client where to look up
# Ledger Metadata for the BookKeeper cluster it is connecting to. If this property isn't uncommented, then the
# default value for the path is the Pravega cluster namespace ("/pravega/<cluster-name>/") with "ledgers" appended:
# "/pravega/<cluster-name>/ledgers". Otherwise, it will take the value specified below.
#bookkeeper.ledger.path=/pravega/bookkeeper/ledgers

# ZooKeeper session timeout (millis) for BookKeeper. This is only used for BookKeeper and has no influence on other
# components configured in this class.
#bookkeeper.zk.connect.sessionTimeout.milliseconds=10000

# ZooKeeper connection timeout (millis) for BookKeeper. This is only used for BookKeeper and has no influence on other
# components configured in this class.
#bookkeeper.zk.connect.sessionTimeout.milliseconds=10000

# Hierarchy depth used for storing BookKeeper log metadata in ZooKeeper. This value is not to be confused with the
# BookKeeper ledger node structure (which also has a flat/hierarchical structure) - that is configured in the BookKeeper
# configuration file and deals with Ledger metadata. This deals strictly with the information that the Pravega Segment
# Store keeps in ZooKeeper.
# The hierarchy is based off indexing the last digits in the log ids in reverse order (depth == 2 means there will be
# two levels, first one based off the last digit and the second one based off the second-to-last digit). This value
# should be set such that there are no more than 100 nodes at the terminal nodes (i.e., if container count == 1024, then
# depth 0 would mean 1024 sibling nodes, depth=1 would mean 103 sibling nodes, depth 3 would mean about 10, and depth=4+
# would mean 1).
# This value must be the same for all Pravega SegmentStore instances in this cluster.
#bookkeeper.zk.metadata.hierarchy.depth=2

# Ensemble size for BookKeeper ledgers.
# This value need not be the same for all Pravega SegmentStore instances in this cluster, but it highly recommended for
# consistency.
#bookkeeper.ensemble.size=3

# Write Ack Quorum size for BookKeeper ledgers.
# This value need not be the same for all Pravega SegmentStore instances in this cluster, but it highly recommended for
# consistency.
#bookkeeper.ack.quorum.size=3

# Write Quorum size for BookKeeper ledgers.
# This value need not be the same for all Pravega SegmentStore instances in this cluster, but it highly recommended for
# consistency.
#bookkeeper.write.quorum.size=3

# Write Timeout, in milliseconds.
# This value is also used for throttling purposes. Once BookKeeper Write Latencies exceed 10% of this value, the Segment
# Store will begin throttling in order to manage the BookKeeper write backlog and reduce the chance of write timeouts.
# Note: BookKeeper only allows multiples of 1 second (1000 millis). This value will be rounded up to the nearest second.
#bookkeeper.write.timeout.milliseconds=60000

# Read Timeout, in milliseconds.
# Note: BookKeeper only allows multiples of 1 second (1000 millis). This value will be rounded up to the nearest second.
#bookkeeper.read.timeout.milliseconds=30000

# Number of Ledger Entries to fetch from BookKeeper at once.
# Recommended values: between 32 and 256. The more items to read at once, the more Java Heap will be required during
# Segment Container recovery (hence the large likelihood of running out of Heap). Setting this to Integer.MAX_VALUE will
# essentially load the entire Ledger in memory before processing it.
# Valid values: at least 1.
#bookkeeper.read.batch.size=64

# Maximum number of bytes that can be outstanding per BookKeeperLog at any given time. This value is used for throttling
# purposes. This value is not set on the BookKeeper Client Configuration, rather it is used internally by the Segment
# Store throttler to manage the BookKeeper write backlog and reduce the chance of write timeouts.
# Recommended Value: 256MB. A smaller value will make throttling more aggressive but increase the overall stability of
# the system. A larger value will increase the likelihood of BookKeeper write timeouts if BookKeeper is unable to keep
# up with the load the Segment Store sends its way.
#bookkeeper.write.outstanding.bytes.max=268435456

# Maximum Ledger size (bytes) in BookKeeper. Once a Ledger reaches this size, it will be closed and another one open.
# Note that ledgers will not be cut off at this size, rather them reaching this size will trigger a rollover; in-flight
# writes will continue to get written to the previous ledger.
# Recommended values: at least 128MB, preferably 1GB.
# Maximum value: 2GB.
# This value need not be the same for all Pravega SegmentStore instances in this cluster, but it highly recommended for
# consistency.
#bookkeeper.ledger.size.max=1073741824

# The ZooKeeper sub-namespace where to store SegmentContainer Log Metadata. This will be rooted under the value of
# 'pravegaservice.clusterName' defined above.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
#bookkeeper.zk.metadata.path=/segmentstore/containers

# The maximum number of times to attempt to write an entry to BookKeeper (for retriable errors).
#bookkeeper.write.attempts.count.max=5

# If Pravega uses a Bookkeeper deployment exploiting local disks, then it is desirable to use a data placement policy
# that takes into account the physical racks or nodes where Bookies are located. This can prevent data loss for a given
# piece of data in the case that all the Bookies are running on the same node/rack and there is a hardware failure. By
# default, we use org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicy as the ensemble placement policy.

# Instructs the Bookkeeper client whether or not it should enforce writing data replicas on at least
# minNumRacksPerWriteQuorum different racks/nodes.
#bookkeeper.write.quorum.racks.minimumCount.enable=false

# Minimum number of physical racks/nodes that every entry replicas should be stored at.
#bookkeeper.write.quorum.racks.count.min=2

# The Bookkeeper client requires a script (or a class) that performs the actual mapping between Bookie IPs and their
# physical location. This script will get as input a (possibly empty) list of Bookie IPs and the expected output should
# be an equal list with their physical locations. For example, the sample file included (only for test purposes) outputs
# elements such as "/datacenter/rack-X", where "/datacenter" can be the datacenter prefix and "/rack-X" the actual
# rack/node where the Bookie is running. To create your own script that actually represents the location of Bookies,
# please refer to: https://github.com/apache/bookkeeper/blob/master/bookkeeper-server/src/main/java/org/apache/bookkeeper/net/DNSToSwitchMapping.java
#bookkeeper.networkTopology.script.location=/opt/pravega/scripts/sample-bookkeeper-topology.sh

# Configure the digest type for messages sent to Bookkeeper. Note that there is a relevant computational cost associated
# to this option, which impacts on write throughput. Current options are: CRC32C (default), MAC, CRC32, DUMMY (no digest).
# Please, see https://bookkeeper.apache.org/docs/latest/api/ledger-api/
#bookkeeper.digest.type.name=CRC32C

##endregion

##region HDFS Settings

# URL where the HDFS cluster is accessible at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
hdfs.connect.uri=localhost:9000

# Root path in HDFS where all Pravega-related data for this cluster is located at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# hdfs.root=

# The replication factor for all the data stored in HDFS. This can be used to control the availability of HDFS data.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# Valid values: Positive integer.
# Recommended values: At least 3.
#hdfs.replication.factor=3

# The default block size, in bytes, that will be used to exchange data with HDFS.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# Valid values: Positive integer.
# Recommended values: Multiples of 1MB.
#hdfs.block.size=1048576

# Whether to replace DataNodes on failure or not.
# Valid values: Boolean.
# Recommended values: false for small clusters, true otherwise. This should be set to true for deployments where
# sufficient data nodes are available(More than Max(3, replication)), otherwise set to false.
#hdfs.replaceDataNodesOnFailure.enable=false

##endregion

##region Extended S3 settings

# URI to specify the complete extended S3 client configurations (excludes bucket and prefix), in the format of
# <protocol>://<host>[:<port>][/path][?<param-key>=<param-value>][&<param-key>=<param-value>][...]
# Refer to https://github.com/EMCECS/ecs-object-client-java/wiki/Config-URI-format for more details.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
#
# Supported parameters include but are not limited to:
#   url: where the extended S3 cluster is accessible at, e.g. http://localhost:9020
#   identity: the access key to access the extended S3 cluster, e.g. identity=user
#   secretKey: the secret key to access the extended S3 cluster, e.g. secretKey=password
extendeds3.connect.config.uri=http://localhost:9020?identity=user&secretKey=password

# Shared extended S3 bucket where the data is stored.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# extendeds3.bucket=

# Prefix of extended S3 is a prefix that will be added to every object created in the bucket by all the
# Pravega SegmentStore in this cluster.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# Prefix is optional.
# extendeds3.prefix=

# Size of ECS objects in bytes above which it is no longer considered a small object.
# This value is used to optimize transactions performance when size of transaction segments is small.
# For small transaction segments, to implement concat ExtendedS3Storage reads complete source segment and appends it to target
# instead of using multipart upload.
# smallObjectSizeLimitForConcat is optional.
# Valid values: Positive integer.
# Default value: 1048576 (1MB).
# Recommended values: 1 MB.
# extendeds3.concat.smallObject.threshold.size=1048576

##endregion

##region filesystem settings

# Root path where NFS shared directory needs to be mounted before segmentstore starts execution.
# filesystem.root=

##endregion

##region DurableLog Settings

# A Metadata Checkpoint is a special internal write inside the DurableLog (persisted to Durable Data Log) that takes
# a snapshot of the current state of the Container Metadata (all active Stream Segments, with all of their states). This
# represents point in the DurableDataLog where recovery can start from (recovery cannot start from an arbitrary
# location since we need to know the current state of the metadata before we begin that). As such, we can truncate
# DurableDataLog only where these Checkpoints are located. Serializing a Checkpoint will cause all outstanding
# write operations to wait on it, so when it does happen, it adds a bit of latency. They also have a direct effect on the
# recovery time (upon recovery, the entire DurableDataLog needs to be processed, so the further apart such Checkpoints
# are, the more time it will take). Based on the application requirements, it may be desirable to choose more frequent
# Checkpoints (if a speedy recovery is desired and no discernible impact on online performance is noted), or less frequent
# Checkpoints should online performance play a very important factor.


# The minimum number of commits that must be to be accumulated in order to trigger a Metadata Checkpoint. This is used
# in case we have accumulated enough bytes to trigger a Checkpoint, but not enough commits (this is to ensure that a single
# large write doesn't trigger such a checkpoint by itself).
# Valid values: Positive integer.
# Recommended values: Larger than 100. Choosing a higher value will cause larger gaps between checkpoints, improving
# online performance but increasing failover recovery time.
#durablelog.checkpoint.commit.count.min=300

# The number of commits that would trigger a Checkpoint. This is used in case we were not able to accumulate enough bytes
# to trigger a Checkpoint, as we do want to eventually be able to truncate.
# Valid values: Positive integer, but larger than or equal to 'durablelog.checkpointMinCommitCount'.
# Recommended values: Larger than 100. Choosing a higher value will cause larger gaps between checkpoints, improving
# online performance but increasing failover recovery time.
#durablelog.checkpoint.commit.threshold.count=300

# The number of bytes written that would trigger a Checkpoint.
# Valid values: Positive integer.
# Recommended values: Larger than 100MB. Choosing a higher value will cause larger gaps between checkpoints, improving
# online performance but increasing failover recovery time.
#durablelog.checkpoint.commit.length.total=268435456

##endregion

##region ReadIndex Settings

# A note on Cache Management. Each append to a Stream Segment and each cache miss will cause an entry to be written to
# the Cache (as such, entries have a variable length). The Cache is shared across all Segment Containers in a Pravega
# SegmentStore instance and its entry lifecycle is determined by three major variables: cacheMaxSize, cacheMaxTimeMillis
# and cacheGenerationTimeMillis. A Cache Generation is a proxy for time passage (instead of using a single time-based stamp,
# each cache entry has a generation id. All hits (or additions) during a generation will get that generation id. The Cache
# Manager adjusts the current generation and oldest permissible generation to manage the cache (entries with a generation
# older than the oldest ones are candidates for eviction). The oldest generation is increased either when the physical
# amount of time between now and the oldest generation exceeds cacheMaxTimeMillis or when there is cache pressure and
# the maximum amount of data in the cache exceeds cacheMaxSize.

# A value to align all Long Term Storage Reads to. When a Long Term Storage Read is issued, the read length is adjusted (if possible)
# to end on a multiple of this value. This value can be used to configure read-ahead size.
# Valid values: Positive integer.
# Recommended values: Multiples of 1MB. The higher the value, the better the catch-up read-ahead performance will be, at
# the expense of the size of the initial read. However, choosing too large of a value may have adverse effects in case
# catch-up reads are not sequential.
#readindex.storageRead.alignment=1048576

# Minimum number of bytes to return from reads if all these bytes are readily available in memory. This value should only
# be changed if it is suspected that tail-read performance suffers because of an unusual high number of very small (<100b)
# appends.
# Valid values: Positive integer.
# Recommended values: Multiples of 1KB. The higher the value, the better the read performance will be in cases of very
# small tail writes.
#readindex.memoryRead.length.min=4096

##endregion

##region AttributeIndex Settings

# The maximum page size for the Attribute B+Tree index. Once a page (index or leaf) exceeds this limit, it will be split
# into two or more pages.
# Valid values: Positive integer, greater than or equal to 1024 and smaller than or equal to 32767.
# Recommended values: 16KB. Various load tests showed that this maximum page size produces the smallest B+Tree index
# segment when loaded with either sequential values (very rare) or arbitrary values in arbitrary order (most common); this
# value also produces the best balance between read/write performance and index fragmentation.
#attributeindex.indexPage.size.bytes.max=32767

# The Attribute Rolling Segment Rolling Policy (size of each individual segment chunk, in bytes).
# Valid values: Positive integer.
# Recommended values: (approximately) 1000 x maxIndexPageSizeBytes.
#attributeindex.attributeSegment.rolling.size.bytes=33554432

##region Writer Settings

# The minimum number of bytes to wait for before flushing aggregated data for a Segment to Long Term Storage. The trigger to
# flush is either when this condition is met or when the 'flushThresholdMillis' condition is met.
# Valid values: Positive integer.
# Recommended values: multiples of 1MB. A lower value will cause more frequent writes to Long Term Storage, which could cause
# data to pile up if Long Term Storage has a high per-write latency. A higher value will cause less frequent writes to
# Long Term Storage, which could cause less frequent truncations of DurableDataLog data (and as such, more time for failover recovery).
#writer.flush.threshold.bytes=4194304

# The minimum amount of time (in milliseconds) to wait for before flushing aggregated data for a Segment to Long Term Storage.
# The trigger to flush is either when this condition is met or when the 'flushThresholdBytes' condition is met.
# Valid values: Positive integer.
# Recommended values: multiples of 1 second. A lower value will cause more frequent writes to Long Term Storage, which could cause
# data to pile up if Long Term Storage has a high per-write latency. A higher value will cause less frequent writes to Long Term
# Storage, which could cause less frequent truncations of DurableDataLog data (and as such, more time for failover recovery).
#writer.flush.threshold.milliseconds=30000

# The maximum number of bytes that can be flushed with a single write operation.
# Valid values: Positive integer.
#writer.flush.size.bytes.max=4194304

# The maximum number of items to read every time a read is issued to the DurableLog.
# Valid values: Positive integer.
# Recommended values: 100-1000. Lower values for systems where there is a high incidence of large appends, and higher values
# for systems where there is a high incidence of smaller appends.
#writer.itemsToReadAtOnce.max=1000

# The minimum timeout (in milliseconds) to wait for new items to come into the DurableLog (in case we reached the tail).
# Valid values: Positive integer.
#writer.read.timeout.milliseconds.min=2000

# The maximum timeout (in milliseconds) to wait for new items to come into the DurableLog (in case we reached the tail).
# Valid values: Positive integer.
#writer.read.timeout.milliseconds.max=1800000

# The amount of time to sleep (in milliseconds) if an exception was encountered during the execution of a Writer's iteration.
# This can be used as a way to "back off" in cases a transient error occurred and not go into a very tight loop while
# the exception condition is still active.
# Valid values: Positive integer.
#writer.error.sleep.milliseconds=1000

# The timeout (in milliseconds) for flushing data to Long Term Storage.
# Valid values: Positive integer.
#writer.flush.timeout.milliseconds=60000

# The timeout (in milliseconds) for acknowledging data writes (and subsequently truncating DurableDataLog).
# Valid values: Positive integer.
#writer.ack.timeout.milliseconds=15000

# The timeout (in milliseconds) for waiting for a proper shutdown.
# Valid values: Positive integer.
#writer.shutDown.timeout.milliseconds=10000

# The maximum size of a single Segment Chunk in Storage. While this can be configured on a per-segment basis via the
# Segment Store API, this setting establishes a ceiling to that value. No Segment Chunk may exceed this length regardless
# of how it was configured via the API (if at all). Use this setting if you need to meet some hardware limitation with
# the underlying Storage implementation (i.e., maximum file size).
# Valid values: Positive number.
#writer.rollover.size.bytes.max=9223372036854775807

##endregion

##region Admin Gateway

# Whether to enable the Pravega Admin Gateway on this host or not. Enabling the Admin Gateway allows administrators to
# debug and repair Pravega Clusters. However, it may induce some extra cost associated to resolve the container id of
# Segments, which is in the hot path. For this reason, it is preferable to enable this only when repair/debug is needed.
# Valid values: true, false. Default value: false.
pravegaservice.admin.gateway.enable=false

# TCP port where the SegmentStore will be listening for incoming administration requests.
# Valid values: Positive integer in the valid TCP port ranges.
pravegaservice.admin.gateway.port=9999

##endregion