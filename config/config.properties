#
#  Copyright (c) 2017 Dell Inc., or its subsidiaries.
#

## Instructions for using this file:
# 1. Settings are namespaced. The prefix indicates the component for which the configuration applies.
# 2. Most settings have default values hardcoded inside the code. Those are commented out (with the default value set)
#    in this file.
# 3. Some settings do not have overrides and require a value be set in this file (or via command-line). Those settings
#    are labeled with 'Optional=false' in this file.
# 4. Each setting (especially non-string ones) have a label named 'Valid values' which describes how a valid value should
#    look like.
# 5. Most settings have 'Recommended values'. These are guidelines (but not requirements) as to how to properly set the
#    values for those settings based on desired use cases.
#
# Each of these settings can be overridden from the command like by specifying a Java System Property.
# Example: setting 'pravegaservice.listeningPort' has a default value of 12345, but it can be overridden to a different
#          value via the command-line:
#                java -Dpravegaservice.listeningPort=5689 ...
#          which will set the value for that setting to 5689, regardless of what this file contains.

##region General Segment Store Settings

# Name of the cluster to which this instance of the SegmentStore belongs.
# Required.
pravegaservice.clusterName=pravega-cluster

# Number of Segment Containers in the system. This value must be the same across all SegmentStore instances in this cluster.
# This value cannot be changed dynamically, it will require special administrative tasks when changing. See documentation
# for details.
# Valid values: Positive integer.
# Required.
pravegaservice.containerCount=1

# Maximum number of threads in the common SegmentStore thread pool. This pool is used for all SegmentStore-related
# activities, except Netty-related tasks. Examples include: handling inbound requests, processing reads, reading and writing
# to Tier 2 Storage, background maintenance operations and background operation processing.
# Valid values: Positive integer.
# Recommended setting: 3 * Number of containers per node, minimum 20.
#pravegaservice.threadPoolSize=50

# TCP port where the SegmentStore will be listening for incoming requests.
# Valid values: Positive integer in the valid TCP port ranges.
pravegaservice.listeningPort=12345

# Full URL (host:port) where to find a ZooKeeper that can be used for coordinating this Pravega Cluster.
# Required.
pravegaservice.zkURL=localhost:2181

# Number of milliseconds to wait between ZooKeeper retries.
# Valid values: Positive integer.
# Recommended values: Between 5000 and 10000 (too high will cause failover delays).
#pravegaservice.zkRetrySleepMs=5000

# Number of times to retry a failed attempt to contact ZooKeeper.
# Valid values: Positive integer.
# Recommended values: Between 3 and 5.
#pravegaservice.zkRetryCount=5

# URI for the Pravega Controller
#pravegaservice.controllerUri=tcp://localhost:9090

# A Stream name for internal purposes. Not externally visible.
#pravegaservice.internalRequestStream=requeststream

# A Scope name for internal purposes. Not externally visible.
#pravegaservice.internalScope=pravega

##endregion

##region Metrics Settings

# Whether to enable Metrics Reporting from this Pravega SegmentStore. This is the master switch for all metrics-related
# activities (if this is disabled, then all metrics-related services are too, regardless of their individual settings).
# Valid values: 'true' or 'false'.
metrics.enableStatistics=false

# Number of dynamic metrics statistic objects to keep in memory at once.
# Valid values: Positive integer.
# Recommended values: 10000 to 1000000. The higher the value, the more memory used, but it may improve performance slightly
# in scenarios where there are large number of active Stream Segments.
#metrics.dynamicCacheSize=100000

# Number of seconds to keep dynamic metric statistic objects in memory.
# Valid values: Positive integer.
# Recommended values: 60 to 300. The higher the value, the more memory used, but it may improve performance slightly
# in scenarios where there are large number of active Stream Segments.
#metrics.dynamicTTLSeconds=120

# Number of seconds between statistics reports.
# Valid values: Positive integer.
# Recommended values: A smaller value will produce more fine-grained reports, at the possible expense of extra CPU and IO
# overhead, while larger values will reduce overhead at the expense of more coarse reports.
#metrics.yammerStatsOutputFrequencySeconds=60

# Prefix to add to all Metrics counters.
#metrics.yammerMetricsPrefix=pravega

# Whether to enable CSV reporting.
# Valid values: 'true' or 'false'.
#metrics.enableCSVReporter=true

# CSV reporter target.
# Valid values: write-accessible filesystem directory.
#metrics.yammerCSVEndpoint=/tmp/csv

# Whether to enable StatsD reporting.
# Valid values: 'true' or 'false'.
#metrics.enableStatsdReporter=true

# StatsD endpoint host name.
#metrics.yammerStatsDPort=localhost

# StatsD endpoint port.
# Valid values: Positive integer.
#metrics.yammerStatsDHost=8125

# Whether to enable Graphite reporting.
# Valid values: 'true' or 'false'.
#metrics.enableGraphiteReporter=false

# Graphite endpoint host name.
#metrics.yammerGraphiteHost=localhost

# Graphite endpoint port.
# Valid values: Positive integer.
#metrics.yammerGraphitePort=2003

# Whether to enable JMX reporting.
# Valid values: 'true' or 'false'.
#metrics.enableJMXReporter=false

# JMX domain.
#metrics.yammerJMXDomain=domain

# Whether to enable Ganglia reporting.
# Valid values: 'true' or 'false'.
#metrics.enableGangliaReporter=false

# Ganglia endpoint host name.
#metrics.yammerGangliaHost=localhost

# Graphite endpoint port.
# Valid values: Positive integer.
#metrics.yammerGangliaPort=8649

# Whether to enable Console reporting.
# Recommended values: 'false'. Set to 'true' only for debugging purposes.
#metrics.enableConsoleReporter=false

##endregion

##region DistributedLog Settings

# Endpoint hostname where the ZooKeeper controlling DistributedLog for this cluster can be found at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
dlog.hostname=master.mesos

# Endpoint port where the ZooKeeper controlling DistributedLog for this cluster can be found at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
dlog.port=2181

# The DistributedLog namespace for this cluster.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
#dlog.namespace=pravega/segmentstore/containers

##endregion

##region HDFS Settings

# URL where the HDFS cluster is accessible at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
hdfs.hdfsUrl=localhost:9000

# Root path in HDFS where all Pravega-related data for this cluster is located at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# hdfs.hdfsRoot=

# The replication factor for all the data stored in HDFS. This can be used to control the availability of HDFS data.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# Valid values: Positive integer.
# Recommended values: At least 3.
#hdfs.replication=3

# The default block size, in bytes, that will be used to exchange data with HDFS.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# Valid values: Positive integer.
# Recommended values: Multiples of 1MB.
#hdfs.blockSize=1048576

##endregion

##region RocksDB Settings

# Path to the working directory where RocksDB can store its databases. The contents of this folder can be discarded after
# the process exits (and it will be cleaned up upon startup), but Pravega requires exclusive use of this while running.
# Recommended values: a path to a locally mounted directory that sits on top of a fast SSD.
#rocksdb.dbDir=/tmp/pravega/cache

##endregion

##region DurableLog Settings

# A Metadata Checkpoint is a special internal write inside the DurableLog (persisted to Tier1 DurableDataLog) that takes
# a snapshot of the current state of the Container Metadata (all active Stream Segments, with all of their states). This
# represents point in the Tier1 DurableDataLog where recovery can start from (recovery cannot start from an arbitrary
# location since we need to know the current state of the metadata before we begin that). As such, we can only truncate
# Tier1 DurableDataLog only where these Checkpoints are located. Serializing a Checkpoint will cause all outstanding
# write operations to wait on it, so when it does happen, it adds a bit of latency. They also have a direct effect on the
# recovery time (upon recovery, the entire Tier1 DurableDataLog needs to be processed, so the further apart such Checkpoints
# are, the more time it will take). Based on the application requirements, it may be desirable to choose more frequent
# Checkpoints (if a speedy recovery is desired and no discernible impact on online performance is noted), or less frequent
# Checkpoints should online performance play a very important factor.


# The minimum number of commits that must be to be accumulated in order to trigger a Metadata Checkpoint. This is used
# in case we have accumulated enough bytes to trigger a Checkpoint, but not enough commits (this is to ensure that a single
# large write doesn't trigger such a checkpoint by itself).
# Valid values: Positive integer.
# Recommended values: Larger than 100. Choosing a higher value will cause larger gaps between checkpoints, improving
# online performance but increasing failover recovery time.
#durablelog.checkpointMinCommitCount=300

# The number of commits that would trigger a Checkpoint. This is used in case we were not able to accumulate enough bytes
# to trigger a Checkpoint, as we do want to eventually be able to truncate.
# Valid values: Positive integer, but larger than or equal to 'durablelog.checkpointMinCommitCount'.
# Recommended values: Larger than 100. Choosing a higher value will cause larger gaps between checkpoints, improving
# online performance but increasing failover recovery time.
#durablelog.checkpointCommitCountThreshold=300

# The number of bytes written that would trigger a Checkpoint.
# Valid values: Positive integer.
# Recommended values: Larger than 100MB. Choosing a higher value will cause larger gaps between checkpoints, improving
# online performance but increasing failover recovery time.
#durablelog.checkpointTotalCommitLengthThreshold=268435456

##endregion

##region ReadIndex Settings

# A note on Cache Management. Each append to a Stream Segment and each cache miss will cause an entry to be written to
# the Cache (as such, entries have a variable length). The Cache is shared across all Segment Containers in a Pravega
# SegmentStore instance and its entry lifecycle is determined by three major variables: cacheMaxSize, cacheMaxTimeMillis
# and cacheGenerationTimeMillis. A Cache Generation is a proxy for time passage (instead of using a single time-based stamp,
# each cache entry has a generation id. All hits (or additions) during a generation will get that generation id. The Cache
# Manager adjusts the current generation and oldest permissible generation to manage the cache (entries with a generation
# older than the oldest ones are candidates for eviction). The oldest generation is increased either when the physical
# amount of time between now and the oldest generation exceeds cacheMaxTimeMillis or when there is cache pressure and
# the maximum amount of data in the cache exceeds cacheMaxSize.

# A value to align all Tier2 Storage Reads to. When a Tier2 Storage Read is issued, the read length is adjusted (if possible)
# to end on a multiple of this value. This value can be used to configure read-ahead size.
# Valid values: Positive integer.
# Recommended values: Multiples of 1MB. The higher the value, the better the catch-up read-ahead performance will be, at
# the expense of the size of the initial read. However, choosing too large of a value may have adverse effects in case
# catch-up reads are not sequential.
#readindex.storageReadAlignment=1048576

# Minimum number of bytes to return from reads if all these bytes are readily available in memory. This value should only
# be changed if it is suspected that tail-read performance suffers because of an unusual high number of very small (<100b)
# appends.
# Valid values: Positive integer.
# Recommended values: Multiples of 1KB. The higher the value, the better the read performance will be in cases of very
# small tail writes.
#readindex.memoryReadMinLength=4096

# Maximum size (in bytes) for the Read Index cache (shared by all Segment Containers on this Segment Store instance).
# Valid values: Positive integer.
# Recommended values: Multiples of 1GB. Choosing a lower size will conserve memory and disk resources at the expense of
# a lot of cache misses (resulting in Tier2 reads and possibly high latencies). Choosing a higher size will keep data
# in the cache for longer, but larger values may not be appropriate for all Cache implementations. For an In-Memory cache,
# the Java Heap will fill up very quickly, possibly causing the process to crash. For a Memory-Disk hybrid cache (such as
# RocksDB) a larger value will be appropriate, as that cache implementation should be able to offload extra memory to disk
# and fetch it back when needed.
#readindex.cacheMaxSize=17179869184

# Maximum amount of time (in milliseconds) to keep a block of data in the Cache.
# Valid values: Positive integer.
# Recommended values: Between 5 and 60 minutes. Choosing lower values will keep the cache size low, ensuring that an eviction
# is less likely to happen should the load pick up quickly, at the expense of possibly increasing cache misses. Choosing
# a higher value will keep data in the cache for longer, reducing chances of cache misses, but possibly incurring cache
# evictions when load picks up again.
#readindex.cacheMaxTimeMillis=1800000

# The length of a Cache Generation, in milliseconds.
# Valid values: positive integer.
# Recommended values: multiples of 1 second. Choosing lower values will cause Cache evictions to happen more frequently,
# thus increasing overhead, but it will provide more granularity for busy systems.
#readindex.cacheGenerationTimeMillis=5000

##endregion

##region Writer Settings

# The minimum number of bytes to wait for before flushing aggregated data for a Segment to Tier2 Storage. The trigger to
# flush is either when this condition is met or when the 'flushThresholdMillis' condition is met.
# Valid values: Positive integer.
# Recommended values: multiples of 1MB. A lower value will cause more frequent writes to Tier2 Storage, which could cause
# data to pile up if Tier2 Storage has a high per-write latency. A higher value will cause less frequent writes to Tier2
# Storage, which could cause less frequent truncations of Tier1 data (and as such, more time for failover recovery).
#writer.flushThresholdBytes=4194304

# The minimum amount of time (in milliseconds) to wait for before flushing aggregated data for a Segment to Tier2 Storage.
# The trigger to flush is either when this condition is met or when the 'flushThresholdBytes' condition is met.
# Valid values: Positive integer.
# Recommended values: multiples of 1 second. A lower value will cause more frequent writes to Tier2 Storage, which could cause
# data to pile up if Tier2 Storage has a high per-write latency. A higher value will cause less frequent writes to Tier2
# Storage, which could cause less frequent truncations of Tier1 data (and as such, more time for failover recovery).
#writer.flushThresholdMillis=30000

# The maximum number of bytes that can be flushed with a single write operation.
# Valid values: Positive integer.
#writer.maxFlushSizeBytes=4194304

# The maximum number of items to read every time a read is issued to the DurableLog.
# Valid values: Positive integer.
# Recommended values: 100-1000. Lower values for systems where there is a high incidence of large appends, and higher values
# for systems where there is a high incidence of smaller appends.
#writer.maxItemsToReadAtOnce=1000

# The minimum timeout (in milliseconds) to wait for new items to come into the DurableLog (in case we reached the tail).
# Valid values: Positive integer.
#writer.minReadTimeoutMillis=2000

# The maximum timeout (in milliseconds) to wait for new items to come into the DurableLog (in case we reached the tail).
# Valid values: Positive integer.
#writer.maxReadTimeoutMillis=1800000

# The amount of time to sleep (in milliseconds) if an exception was encountered during the execution of a Writer's iteration.
# This can be used as a way to "back off" in cases a transient error occurred and not go into a very tight loop while
# the exception condition is still active.
# Valid values: Positive integer.
#writer.errorSleepMillis=1000

# The timeout (in milliseconds) for flushing data to Tier2 Storage.
# Valid values: Positive integer.
#writer.flushTimeoutMillis=60000

# The timeout (in milliseconds) for acknowledging data writes (and subsequently truncating Tier1 DurableDataLog).
# Valid values: Positive integer.
#writer.ackTimeoutMillis=15000

# The timeout (in milliseconds) for waiting for a proper shutdown.
# Valid values: Positive integer.
#writer.shutdownTimeoutMillis=10000

##endregion