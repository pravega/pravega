#
# Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#

## Instructions for using this file:
# 1. Settings are namespaced. The prefix indicates the component for which the configuration applies.
# 2. Most settings have default values hardcoded inside the code. Those are commented out (with the default value set)
#    in this file.
# 3. Some settings do not have overrides and require a value be set in this file (or via command-line). Those settings
#    are labeled with the label 'Required.' in this file.
# 4. Each setting (especially non-string ones) have a label named 'Valid values' which describes how a valid value should
#    look like.
# 5. Most settings have 'Recommended values'. These are guidelines (but not requirements) as to how to properly set the
#    values for those settings based on desired use cases.
#
# Each of these settings can be overridden from the command like by specifying a Java System Property.
# Example: setting 'pravegaservice.listeningPort' has a default value of 12345, but it can be overridden to a different
#          value via the command-line:
#                java -Dpravegaservice.listeningPort=5689 ...
#          which will set the value for that setting to 5689, regardless of what this file contains.

##region General Segment Store Settings

# Name of the cluster to which this instance of the SegmentStore belongs.
# Required.
pravegaservice.clusterName=pravega-cluster

# Number of Segment Containers in the system. This value must be the same across all SegmentStore instances in this cluster.
# This value cannot be changed dynamically, it will require special administrative tasks when changing. See documentation
# for details.
# Valid values: Positive integer.
# Required.
pravegaservice.containerCount=4

# Maximum number of threads in the Core SegmentStore Thread Pool. This pool is used for all SegmentStore-related
# activities, except Netty-related tasks and Tier2 Storage activities. Examples include: handling inbound requests,
# processing reads, background maintenance operations and background operation processing.
# Valid values: Positive integer.
# Recommended setting: 2 * Number of containers per node, minimum 20.
#pravegaservice.threadPoolSize=30

# Maximum number of threads in the Thread Pool used for Tier2 tasks (reading, writing, create, delete, etc.).
# Valid values: Positive integer.
# Recommended setting: reasonably large number which does not cause thrashing, minimum 20, recommended value 200.
#pravegaservice.storageThreadPoolSize=200

# TCP port where the SegmentStore will be listening for incoming requests.
# Valid values: Positive integer in the valid TCP port ranges.
pravegaservice.listeningPort=12345

# Full URL (host:port) where to find a ZooKeeper that can be used for coordinating this Pravega Cluster.
# Required.
pravegaservice.zkURL=localhost:2181

# Number of milliseconds to wait between ZooKeeper retries.
# Valid values: Positive integer.
# Recommended values: Between 5000 and 10000 (too high will cause failover delays).
#pravegaservice.zkRetrySleepMs=5000

# Number of times to retry a failed attempt to contact ZooKeeper.
# Valid values: Positive integer.
# Recommended values: Between 3 and 5.
#pravegaservice.zkRetryCount=5

# Session timeout for ZooKeeper.
# Valid values: Positive integer.
#pravegaservice.zkSessionTimeoutMs=10000

# Enable security for the ZK client.
#pravegaservice.secureZK=false

# Location of trust store for secure connection to ZK.
#pravegaservice.zkTrustStore=

#Location of password file to access the trust store.
#pravegaservice.zkTrustStorePasswordPath=

# DataLog implementation for Tier 1 storage.
# Valid values: BOOKKEEPER, INMEMORY.
# Default value: BOOKKEEPER
pravegaservice.dataLogImplementation=BOOKKEEPER

# Storage implementation for Tier 2 storage.
# Valid values: HDFS, FILESYSTEM, EXTENDEDS3, INMEMORY.
# Default value: HDFS
# pravegaservice.storageImplementation=HDFS

# Storage NO-OP Mode: in No-Op mode, user stream segment writing is no-oped; user stream segment reading is not supported.
# This mode is used to avoid storage interference in testing while still keep the system functioning as usual.
# NOTE: pravegaservice.storageImplementation is still used to store metadata and system segments, which are required for the functioning of the Pravega Cluster.
# WARNING: Do NOT set NO-OP Mode to true in production environment! Data loss will happen!
# Valid values: true, false
# Default value: false
#storageextra.storageNoOpMode=false

# When Storage NO-OP Mode is true, storageWriteNoOpModeLatencyMillis is applied to write operation.
# It is used to compensate the supposed latency if the storage is in operation, in order to make the test as real as possible.
# This latency has no effect if storage is not in No-Op mode.
# Valid values: non-negative integer
# Default vale: 20 (milliseconds)
#storageextra.storageWriteNoOpLatencyMillis=20

# Whether to start the SegmentStore in ReadOnly mode. ReadOnly mode means that only Read and GetSegmentInfo are allowed
# and all requests are served directly from Tier 2 Storage. There is no Tier 1 access, nor are any modify operations allowed.
# If set to 'true', the SegmentStore will not host any SegmentContainers and will register itself under the Controller
# as 'ReadOnly', so that the Client can request access to it when needed.
# NOTE: This is an experimental feature. Its behavior and/or API should be expected to be in flux until fully released.
# Valid values: true, false
# Default value: false
#pravegaservice.readOnlySegmentStore=false

# Maximum size (in bytes) for the Local Shared Cache (shared by all Segment Containers on this Segment Store instance).
# Valid values: Positive integer.
# Recommended values: Multiples of 1GB, but less than XX:MaxDirectMemorySize. Choosing a lower size will conserve memory
# at the expense of a lot of cache misses (resulting in Tier2 reads and possibly high latencies). Choosing a higher size
# will keep data in the cache for longer, but at the expense of memory. The Segment Store uses a Direct Memory Cache backed
# by direct ByteBuffers, so if XX:MaxDirectMemorySize is not set to a value higher than this one, the process will
# eventually crash with an OutOfMemoryError.
#pravegaservice.cacheMaxSize=4294967296

# Percentage (of pravegaservice.cacheMaxSize) that defines target Local Shared Cache. The Segment Store will try to keep
# the cache utilization at or below this value, and may apply throttling on new operations if it exceeds it.
# Valid values: 1 to 100 (inclusive).
# Recommended values: Between 75 and 85. It is not recommended to choose a very high value as that may leave very little
# buffer for the Segment Store to attempt any cleaning, apply back-pressure or serve other random requests. Consequently,
# a very low value may have negative impact on the ingestion performance.
#pravegaservice.cacheTargetUtilizationPercent=75

# Percentage (of pravegaservice.cacheMaxSize) that defines the maximum usable size of the Local Shared Cache. The Segment
# Store will apply full throttling if reaching or exceeding this value.
# Valid values: 1 to 100 (inclusive).
# Recommended values: Between {pravegaservice.cacheTargetUtilizationPercent + 10} and 95. Setting a value too close to
# pravegaservice.cacheTargetUtilizationPercent will cause the Segment Store to apply maximum throttling as soon as that
# limit is reached, which may cause it to have sudden, multi-second stalls in the ingestion pipeline. It is not recommended
# to set this value ever to 100, since that would mean the cache is already full when max throttling kicks in, and the
# Segment Store will likely shut down Segment Containers in order to alleviate cache pressure.
#pravegaservice.cacheMaxUtilizationPercent=90

# Maximum amount of time (in seconds) to keep a block of data in the Cache.
# Valid values: Positive integer.
# Recommended values: Between 300 and 3600 seconds. Choosing lower values will keep the cache size low, ensuring that an
# eviction is less likely to happen should the load pick up quickly, at the expense of possibly increasing cache misses.
# Choosing a higher value will keep data in the cache for longer, reducing chances of cache misses, but possibly incurring
# cache evictions when load picks up again.
#pravegaservice.cacheMaxTimeSeconds=1800

# The length of a Cache Generation, in seconds.
# Valid values: positive integer.
# Recommended values: multiples of 1 second. Choosing lower values will cause Cache evictions to happen more frequently,
# thus increasing overhead, but it will provide more granularity for busy systems.
#pravegaservice.cacheGenerationTimeSeconds=1

# This setting allows Pravega to send server-side stack traces to client as part of the response message on errors. This
# setting may be useful for debugging purposes, as users may understand the root cause of a server exception inspecting
# only client-side logs. However, we recommend to be conservative on activating this option as it exposes server-side
# information to clients, which may raise security concerns.
#pravegaservice.replyWithStackTraceOnError=false

##endregion

##region AutoScaler Settings

# URI for the Pravega Controller
#autoScale.controllerUri=pravega://localhost:9090

# A Stream name for internal purposes. Not externally visible.
#autoScale.requestStream=_requeststream

#autoScale.cooldownInSeconds=600
#autoScale.muteInSeconds=600
#autoScale.cacheCleanUpInSeconds=300
#autoScale.cacheExpiryInSeconds=1200

##endregion

##region Metrics Settings

# Whether to enable Metrics Reporting from this Pravega SegmentStore. This is the master switch for all metrics-related
# activities (if this is disabled, then all metrics-related services are too, regardless of their individual settings).
# Valid values: 'true' or 'false'.
metrics.enableStatistics=false

# Number of dynamic metrics statistic objects to keep in memory at once.
# Valid values: Positive integer.
# Recommended values: 10000 to 1000000. The higher the value, the more memory used, but it may improve performance slightly
# in scenarios where there are large number of active Stream Segments.
#metrics.dynamicCacheSize=100000

# Number of minutes to keep dynamic metric statistic objects in memory.
# Valid values: Positive integer.
# Recommended values: 1 to 60. The higher the value, the more memory used, but it may improve performance slightly
# in scenarios where there are large number of active Stream Segments.
#metrics.dynamicCacheEvictionDurationMinutes=3

# Number of seconds between statistics reports.
# Valid values: Positive integer.
# Recommended values: A smaller value will produce more fine-grained reports, at the possible expense of extra CPU and IO
# overhead, while larger values will reduce overhead at the expense of more coarse reports.
#metrics.outputFrequencySeconds=60

# Prefix to add to all Metrics counters.
#metrics.metricsPrefix=pravega

# Whether to enable CSV reporting.
# Valid values: 'true' or 'false'.
#metrics.enableCSVReporter=true

# CSV reporter target.
# Valid values: write-accessible filesystem directory.
#metrics.csvEndpoint=/tmp/csv

# Whether to enable StatsD reporting.
# Valid values: 'true' or 'false'.
#metrics.enableStatsDReporter=true

# StatsD endpoint host name.
#metrics.statsDHost=localhost

# StatsD endpoint port.
# Valid values: Positive integer.
#metrics.statsDPort=8125

# Whether to enable direct InfluxDB reporting (without local agent).
#metrics.enableInfluxDBReporter=true

# InfluxDB endpoint URI.
#metrics.influxDBURI=http://localhost:8086

# InfluxDB database name.
#metrics.influxDBName=pravega

# InfluxDB user name.
#metrics.influxDBUserName=

# InfluxDB password.
#metrics.influxDBPassword=

# InfluxDB retention policy (e.g. 2h, 52w).
#metrics.influxDBRetention=

# Whether to enable Graphite reporting.
# Valid values: 'true' or 'false'.
#metrics.enableGraphiteReporter=false

# Graphite endpoint host name.
#metrics.graphiteHost=localhost

# Graphite endpoint port.
# Valid values: Positive integer.
#metrics.graphitePort=2003

# Whether to enable JMX reporting.
# Valid values: 'true' or 'false'.
#metrics.enableJMXReporter=false

# JMX domain.
#metrics.jmxDomain=domain

# Whether to enable Ganglia reporting.
# Valid values: 'true' or 'false'.
#metrics.enableGangliaReporter=false

# Ganglia endpoint host name.
#metrics.gangliaHost=localhost

# Graphite endpoint port.
# Valid values: Positive integer.
#metrics.gangliaPort=8649

# Whether to enable Console reporting.
# Recommended values: 'false'. Set to 'true' only for debugging purposes.
#metrics.enableConsoleReporter=false

##endregion

##region BookKeeper Settings

# Endpoint address (hostname:port) where the ZooKeeper controlling BookKeeper for this cluster can be found at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
#bookkeeper.zkAddress=localhost:2181

# Default root path for BookKeeper Ledger metadata.  This property tells the BookKeeper client where to look up
# Ledger Metadata for the BookKeeper cluster it is connecting to. If this property isn't uncommented, then the
# default value for the path is the Pravega cluster namespace ("/pravega/<cluster-name>/") with "ledgers" appended:
# "/pravega/<cluster-name>/ledgers". Otherwise, it will take the value specified below.
#bookkeeper.bkLedgerPath=/pravega/bookkeeper/ledgers

# ZooKeeper session timeout (millis) for BookKeeper. This is only used for BookKeeper and has no influence on other
# components configured in this class.
#bookkeeper.zkSessionTimeoutMillis=10000

# ZooKeeper connection timeout (millis) for BookKeeper. This is only used for BookKeeper and has no influence on other
# components configured in this class.
#bookkeeper.zkConnectionTimeoutMillis=10000

# Hierarchy depth used for storing BookKeeper log metadata in ZooKeeper. This value is not to be confused with the
# BookKeeper ledger node structure (which also has a flat/hierarchical structure) - that is configured in the BookKeeper
# configuration file and deals with Ledger metadata. This deals strictly with the information that the Pravega Segment
# Store keeps in ZooKeeper.
# The hierarchy is based off indexing the last digits in the log ids in reverse order (depth == 2 means there will be
# two levels, first one based off the last digit and the second one based off the second-to-last digit). This value
# should be set such that there are no more than 100 nodes at the terminal nodes (i.e., if container count == 1024, then
# depth 0 would mean 1024 sibling nodes, depth=1 would mean 103 sibling nodes, depth 3 would mean about 10, and depth=4+
# would mean 1).
# This value must be the same for all Pravega SegmentStore instances in this cluster.
#bookkeeper.zkHierarchyDepth=2

# Ensemble size for BookKeeper ledgers.
# This value need not be the same for all Pravega SegmentStore instances in this cluster, but it highly recommended for
# consistency.
#bookkeeper.bkEnsembleSize=3

# Write Ack Quorum size for BookKeeper ledgers.
# This value need not be the same for all Pravega SegmentStore instances in this cluster, but it highly recommended for
# consistency.
#bookkeeper.bkAckQuorumSize=3

# Write Quorum size for BookKeeper ledgers.
# This value need not be the same for all Pravega SegmentStore instances in this cluster, but it highly recommended for
# consistency.
#bookkeeper.bkWriteQuorumSize=3

# Write Timeout, in milliseconds.
# This value is also used for throttling purposes. Once BookKeeper Write Latencies exceed 10% of this value, the Segment
# Store will begin throttling in order to manage the BookKeeper write backlog and reduce the chance of write timeouts.
# Note: BookKeeper only allows multiples of 1 second (1000 millis). This value will be rounded up to the nearest second.
#bookkeeper.bkWriteTimeoutMillis=60000

# Read Timeout, in milliseconds.
# Note: BookKeeper only allows multiples of 1 second (1000 millis). This value will be rounded up to the nearest second.
#bookkeeper.bkReadTimeoutMillis=30000

# Number of Ledger Entries to fetch from BookKeeper at once.
# Recommended values: between 32 and 256. The more items to read at once, the more Java Heap will be required during
# Segment Container recovery (hence the large likelihood of running out of Heap). Setting this to Integer.MAX_VALUE will
# essentially load the entire Ledger in memory before processing it.
# Valid values: at least 1.
#bookkeeper.readBatchSize=64

# Maximum number of bytes that can be outstanding per BookKeeperLog at any given time. This value is used for throttling
# purposes. This value is not set on the BookKeeper Client Configuration, rather it is used internally by the Segment
# Store throttler to manage the BookKeeper write backlog and reduce the chance of write timeouts.
# Recommended Value: 256MB. A smaller value will make throttling more aggressive but increase the overall stability of
# the system. A larger value will increase the likelihood of BookKeeper write timeouts if BookKeeper is unable to keep
# up with the load the Segment Store sends its way.
#bookkeeper.maxOutstandingBytes=268435456

# Maximum Ledger size (bytes) in BookKeeper. Once a Ledger reaches this size, it will be closed and another one open.
# Note that ledgers will not be cut off at this size, rather them reaching this size will trigger a rollover; in-flight
# writes will continue to get written to the previous ledger.
# Recommended values: at least 128MB, preferably 1GB.
# Maximum value: 2GB.
# This value need not be the same for all Pravega SegmentStore instances in this cluster, but it highly recommended for
# consistency.
#bookkeeper.bkLedgerMaxSize=1073741824

# The ZooKeeper sub-namespace where to store SegmentContainer Log Metadata. This will be rooted under the value of
# 'pravegaservice.clusterName' defined above.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
#bookkeeper.zkMetadataPath=/segmentstore/containers

# The maximum number of times to attempt to write an entry to BookKeeper (for retriable errors).
#bookkeeper.maxWriteAttempts=5

# If Pravega uses a Bookkeeper deployment exploiting local disks, then it is desirable to use a data placement policy
# that takes into account the physical racks or nodes where Bookies are located. This can prevent data loss for a given
# piece of data in the case that all the Bookies are running on the same node/rack and there is a hardware failure. By
# default, we use org.apache.bookkeeper.client.RackawareEnsemblePlacementPolicy as the ensemble placement policy.

# Instructs the Bookkeeper client whether or not it should enforce writing data replicas on at least
# minNumRacksPerWriteQuorum different racks/nodes.
#bookkeeper.enforceMinNumRacksPerWriteQuorum=false

# Minimum number of physical racks/nodes that every entry replicas should be stored at.
#bookkeeper.minNumRacksPerWriteQuorum=2

# The Bookkeeper client requires a script (or a class) that performs the actual mapping between Bookie IPs and their
# physical location. This script will get as input a (possibly empty) list of Bookie IPs and the expected output should
# be an equal list with their physical locations. For example, the sample file included (only for test purposes) outputs
# elements such as "/datacenter/rack-X", where "/datacenter" can be the datacenter prefix and "/rack-X" the actual
# rack/node where the Bookie is running. To create your own script that actually represents the location of Bookies,
# please refer to: https://github.com/apache/bookkeeper/blob/master/bookkeeper-server/src/main/java/org/apache/bookkeeper/net/DNSToSwitchMapping.java
#bookkeeper.networkTopologyScriptFileName=/opt/pravega/scripts/sample-bookkeeper-topology.sh

# Configure the digest type for messages sent to Bookkeeper. Note that there is a relevant computational cost associated
# to this option, which impacts on write throughput. Current options are: CRC32C (default), MAC, CRC32, DUMMY (no digest).
# Please, see https://bookkeeper.apache.org/docs/latest/api/ledger-api/
#bookkeeper.digestType=CRC32C

##endregion

##region HDFS Settings

# URL where the HDFS cluster is accessible at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
hdfs.hdfsUrl=localhost:9000

# Root path in HDFS where all Pravega-related data for this cluster is located at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# hdfs.hdfsRoot=

# The replication factor for all the data stored in HDFS. This can be used to control the availability of HDFS data.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# Valid values: Positive integer.
# Recommended values: At least 3.
#hdfs.replication=3

# The default block size, in bytes, that will be used to exchange data with HDFS.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# Valid values: Positive integer.
# Recommended values: Multiples of 1MB.
#hdfs.blockSize=1048576

# Whether to replace DataNodes on failure or not.
# Valid values: Boolean.
# Recommended values: false for small clusters, true otherwise. This should be set to true for deployments where
# sufficient data nodes are available(More than Max(3, replication)), otherwise set to false.
#hdfs.replaceDataNodesOnFailure=false

##endregion

##region Extended S3 settings

# URL where the extended S3 cluster is accessible at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
extendeds3.url=localhost:9020

# Prefix in extended S3 cluster where all Pravega-related data for this cluster is following.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# extendeds3.prefix=

# ACCESS_KEY_ID to access the extended S3 cluster
# extendeds3.accessKey=

# SECRET_KEY to access the extended S3 cluster
# extendeds3.secretKey=

# Shared extended S3 bucket where the data is stored.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# extendeds3.bucket=

##endregion

##region filesystem settings

# Root path where NFS shared directory needs to be mounted before segmentstore starts execution.
# filesystem.root=

##endregion

##region DurableLog Settings

# A Metadata Checkpoint is a special internal write inside the DurableLog (persisted to Tier1 DurableDataLog) that takes
# a snapshot of the current state of the Container Metadata (all active Stream Segments, with all of their states). This
# represents point in the Tier1 DurableDataLog where recovery can start from (recovery cannot start from an arbitrary
# location since we need to know the current state of the metadata before we begin that). As such, we can only truncate
# Tier1 DurableDataLog only where these Checkpoints are located. Serializing a Checkpoint will cause all outstanding
# write operations to wait on it, so when it does happen, it adds a bit of latency. They also have a direct effect on the
# recovery time (upon recovery, the entire Tier1 DurableDataLog needs to be processed, so the further apart such Checkpoints
# are, the more time it will take). Based on the application requirements, it may be desirable to choose more frequent
# Checkpoints (if a speedy recovery is desired and no discernible impact on online performance is noted), or less frequent
# Checkpoints should online performance play a very important factor.


# The minimum number of commits that must be to be accumulated in order to trigger a Metadata Checkpoint. This is used
# in case we have accumulated enough bytes to trigger a Checkpoint, but not enough commits (this is to ensure that a single
# large write doesn't trigger such a checkpoint by itself).
# Valid values: Positive integer.
# Recommended values: Larger than 100. Choosing a higher value will cause larger gaps between checkpoints, improving
# online performance but increasing failover recovery time.
#durablelog.checkpointMinCommitCount=300

# The number of commits that would trigger a Checkpoint. This is used in case we were not able to accumulate enough bytes
# to trigger a Checkpoint, as we do want to eventually be able to truncate.
# Valid values: Positive integer, but larger than or equal to 'durablelog.checkpointMinCommitCount'.
# Recommended values: Larger than 100. Choosing a higher value will cause larger gaps between checkpoints, improving
# online performance but increasing failover recovery time.
#durablelog.checkpointCommitCountThreshold=300

# The number of bytes written that would trigger a Checkpoint.
# Valid values: Positive integer.
# Recommended values: Larger than 100MB. Choosing a higher value will cause larger gaps between checkpoints, improving
# online performance but increasing failover recovery time.
#durablelog.checkpointTotalCommitLengthThreshold=268435456

##endregion

##region ReadIndex Settings

# A note on Cache Management. Each append to a Stream Segment and each cache miss will cause an entry to be written to
# the Cache (as such, entries have a variable length). The Cache is shared across all Segment Containers in a Pravega
# SegmentStore instance and its entry lifecycle is determined by three major variables: cacheMaxSize, cacheMaxTimeMillis
# and cacheGenerationTimeMillis. A Cache Generation is a proxy for time passage (instead of using a single time-based stamp,
# each cache entry has a generation id. All hits (or additions) during a generation will get that generation id. The Cache
# Manager adjusts the current generation and oldest permissible generation to manage the cache (entries with a generation
# older than the oldest ones are candidates for eviction). The oldest generation is increased either when the physical
# amount of time between now and the oldest generation exceeds cacheMaxTimeMillis or when there is cache pressure and
# the maximum amount of data in the cache exceeds cacheMaxSize.

# A value to align all Tier2 Storage Reads to. When a Tier2 Storage Read is issued, the read length is adjusted (if possible)
# to end on a multiple of this value. This value can be used to configure read-ahead size.
# Valid values: Positive integer.
# Recommended values: Multiples of 1MB. The higher the value, the better the catch-up read-ahead performance will be, at
# the expense of the size of the initial read. However, choosing too large of a value may have adverse effects in case
# catch-up reads are not sequential.
#readindex.storageReadAlignment=1048576

# Minimum number of bytes to return from reads if all these bytes are readily available in memory. This value should only
# be changed if it is suspected that tail-read performance suffers because of an unusual high number of very small (<100b)
# appends.
# Valid values: Positive integer.
# Recommended values: Multiples of 1KB. The higher the value, the better the read performance will be in cases of very
# small tail writes.
#readindex.memoryReadMinLength=4096

##endregion

##region AttributeIndex Settings

# The maximum page size for the Attribute B+Tree index. Once a page (index or leaf) exceeds this limit, it will be split
# into two or more pages.
# Valid values: Positive integer, greater than or equal to 1024 and smaller than or equal to 32767.
# Recommended values: 16KB. Various load tests showed that this maximum page size produces the smallest B+Tree index
# segment when loaded with either sequential values (very rare) or arbitrary values in arbitrary order (most common); this
# value also produces the best balance between read/write performance and index fragmentation.
#attributeindex.maxIndexPageSizeBytes=32767

# The Attribute Rolling Segment Rolling Policy (size of each individual segment chunk, in bytes).
# Valid values: Positive integer.
# Recommended values: (approximately) 1000 x maxIndexPageSizeBytes.
#attributeindex.attributeSegmentRollingSizeBytes=33554432

##region Writer Settings

# The minimum number of bytes to wait for before flushing aggregated data for a Segment to Tier2 Storage. The trigger to
# flush is either when this condition is met or when the 'flushThresholdMillis' condition is met.
# Valid values: Positive integer.
# Recommended values: multiples of 1MB. A lower value will cause more frequent writes to Tier2 Storage, which could cause
# data to pile up if Tier2 Storage has a high per-write latency. A higher value will cause less frequent writes to Tier2
# Storage, which could cause less frequent truncations of Tier1 data (and as such, more time for failover recovery).
#writer.flushThresholdBytes=4194304

# The minimum amount of time (in milliseconds) to wait for before flushing aggregated data for a Segment to Tier2 Storage.
# The trigger to flush is either when this condition is met or when the 'flushThresholdBytes' condition is met.
# Valid values: Positive integer.
# Recommended values: multiples of 1 second. A lower value will cause more frequent writes to Tier2 Storage, which could cause
# data to pile up if Tier2 Storage has a high per-write latency. A higher value will cause less frequent writes to Tier2
# Storage, which could cause less frequent truncations of Tier1 data (and as such, more time for failover recovery).
#writer.flushThresholdMillis=30000

# The maximum number of bytes that can be flushed with a single write operation.
# Valid values: Positive integer.
#writer.maxFlushSizeBytes=4194304

# The maximum number of items to read every time a read is issued to the DurableLog.
# Valid values: Positive integer.
# Recommended values: 100-1000. Lower values for systems where there is a high incidence of large appends, and higher values
# for systems where there is a high incidence of smaller appends.
#writer.maxItemsToReadAtOnce=1000

# The minimum timeout (in milliseconds) to wait for new items to come into the DurableLog (in case we reached the tail).
# Valid values: Positive integer.
#writer.minReadTimeoutMillis=2000

# The maximum timeout (in milliseconds) to wait for new items to come into the DurableLog (in case we reached the tail).
# Valid values: Positive integer.
#writer.maxReadTimeoutMillis=1800000

# The amount of time to sleep (in milliseconds) if an exception was encountered during the execution of a Writer's iteration.
# This can be used as a way to "back off" in cases a transient error occurred and not go into a very tight loop while
# the exception condition is still active.
# Valid values: Positive integer.
#writer.errorSleepMillis=1000

# The timeout (in milliseconds) for flushing data to Tier2 Storage.
# Valid values: Positive integer.
#writer.flushTimeoutMillis=60000

# The timeout (in milliseconds) for acknowledging data writes (and subsequently truncating Tier1 DurableDataLog).
# Valid values: Positive integer.
#writer.ackTimeoutMillis=15000

# The timeout (in milliseconds) for waiting for a proper shutdown.
# Valid values: Positive integer.
#writer.shutdownTimeoutMillis=10000

# The maximum size of a single Segment Chunk in Storage. While this can be configured on a per-segment basis via the
# Segment Store API, this setting establishes a ceiling to that value. No Segment Chunk may exceed this length regardless
# of how it was configured via the API (if at all). Use this setting if you need to meet some hardware limitation with
# the underlying Storage implementation (i.e., maximum file size).
# Valid values: Positive number.
#writer.maxRolloverSizeBytes=9223372036854775807

##endregion
