#
# Copyright (c) 2017 Dell Inc., or its subsidiaries. All Rights Reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#

## Instructions for using this file:
# 1. Settings are namespaced. The prefix indicates the component for which the configuration applies.
# 2. Most settings have default values hardcoded inside the code. Those are commented out (with the default value set)
#    in this file.
# 3. Some settings do not have overrides and require a value be set in this file (or via command-line). Those settings
#    are labeled with the label 'Required.' in this file.
# 4. Each setting (especially non-string ones) have a label named 'Valid values' which describes how a valid value should
#    look like.
# 5. Most settings have 'Recommended values'. These are guidelines (but not requirements) as to how to properly set the
#    values for those settings based on desired use cases.
#
# Each of these settings can be overridden from the command like by specifying a Java System Property.
# Example: setting 'pravegaservice.listeningPort' has a default value of 12345, but it can be overridden to a different
#          value via the command-line:
#                java -Dpravegaservice.listeningPort=5689 ...
#          which will set the value for that setting to 5689, regardless of what this file contains.

##region General Segment Store Settings

# Name of the cluster to which this instance of the SegmentStore belongs.
# Required.
pravegaservice.clusterName=pravega-cluster

# Number of Segment Containers in the system. This value must be the same across all SegmentStore instances in this cluster.
# This value cannot be changed dynamically, it will require special administrative tasks when changing. See documentation
# for details.
# Valid values: Positive integer.
# Required.
pravegaservice.containerCount=4

# Maximum number of threads in the Core SegmentStore Thread Pool. This pool is used for all SegmentStore-related
# activities, except Netty-related tasks and Tier2 Storage activities. Examples include: handling inbound requests,
# processing reads, background maintenance operations and background operation processing.
# Valid values: Positive integer.
# Recommended setting: 2 * Number of containers per node, minimum 20.
#pravegaservice.threadPoolSize=30

# Maximum number of threads in the Thread Pool used for Tier2 tasks (reading, writing, create, delete, etc.).
# Valid values: Positive integer.
# Recommended setting: 2 * Number of containers per node, minimum 20.
#pravegaservice.storageThreadPoolSize=20

# TCP port where the SegmentStore will be listening for incoming requests.
# Valid values: Positive integer in the valid TCP port ranges.
pravegaservice.listeningPort=12345

# Full URL (host:port) where to find a ZooKeeper that can be used for coordinating this Pravega Cluster.
# Required.
pravegaservice.zkURL=localhost:2181

# Number of milliseconds to wait between ZooKeeper retries.
# Valid values: Positive integer.
# Recommended values: Between 5000 and 10000 (too high will cause failover delays).
#pravegaservice.zkRetrySleepMs=5000

# Number of times to retry a failed attempt to contact ZooKeeper.
# Valid values: Positive integer.
# Recommended values: Between 3 and 5.
#pravegaservice.zkRetryCount=5

# Session timeout for ZooKeeper.
# Valid values: Positive integer.
#pravegaservice.zkSessionTimeoutMs=10000

# Enable security for the ZK client.
#pravegaservice.secureZK=false

# Location of trust store for secure connection to ZK.
#pravegaservice.zkTrustStore=

#Location of password file to access the trust store.
#pravegaservice.zkTrustStorePasswordPath=

# DataLog implementation for Tier 1 storage.
# Valid values: BOOKKEEPER, INMEMORY.
# Default value: BOOKKEEPER
pravegaservice.dataLogImplementation=BOOKKEEPER

# Storage implementation for Tier 2 storage.
# Valid values: HDFS, FILESYSTEM, EXTENDEDS3, INMEMORY.
# Default value: HDFS
# pravegaservice.storageImplementation=HDFS

# Whether to start the SegmentStore in ReadOnly mode. ReadOnly mode means that only Read and GetSegmentInfo are allowed
# and all requests are served directly from Tier 2 Storage. There is no Tier 1 access, nor are any modify operations allowed.
# If set to 'true', the SegmentStore will not host any SegmentContainers and will register itself under the Controller
# as 'ReadOnly', so that the Client can request access to it when needed.
# NOTE: This is an experimental feature. Its behavior and/or API should be expected to be in flux until fully released.
# Valid values: true, false
# Default value: false
#pravegaservice.readOnlySegmentStore=false

# Maximum size (in bytes) for the Local Shared Cache (shared by all Segment Containers on this Segment Store instance).
# Valid values: Positive integer.
# Recommended values: Multiples of 1GB. Choosing a lower size will conserve memory and disk resources at the expense of
# a lot of cache misses (resulting in Tier2 reads and possibly high latencies). Choosing a higher size will keep data
# in the cache for longer, but larger values may not be appropriate for all Cache implementations. For an In-Memory cache,
# the Java Heap will fill up very quickly, possibly causing the process to crash. For a Memory-Disk hybrid cache (such as
# RocksDB) a larger value will be appropriate, as that cache implementation should be able to offload extra memory to disk
# and fetch it back when needed.
#pravegaservice.cacheMaxSize=17179869184

# Maximum amount of time (in seconds) to keep a block of data in the Cache.
# Valid values: Positive integer.
# Recommended values: Between 300 and 3600 seconds. Choosing lower values will keep the cache size low, ensuring that an
# eviction is less likely to happen should the load pick up quickly, at the expense of possibly increasing cache misses.
# Choosing a higher value will keep data in the cache for longer, reducing chances of cache misses, but possibly incurring
# cache evictions when load picks up again.
#pravegaservice.cacheMaxTimeSeconds=1800

# The length of a Cache Generation, in seconds.
# Valid values: positive integer.
# Recommended values: multiples of 1 second. Choosing lower values will cause Cache evictions to happen more frequently,
# thus increasing overhead, but it will provide more granularity for busy systems.
#pravegaservice.cacheGenerationTimeSeconds=5

# This setting allows Pravega to send server-side stack traces to client as part of the response message on errors. This
# setting may be useful for debugging purposes, as users may understand the root cause of a server exception inspecting
# only client-side logs. However, we recommend to be conservative on activating this option as it exposes server-side
# information to clients, which may raise security concerns.
#pravegaservice.replyWithStackTraceOnError=false

##endregion

##region AutoScaler Settings

# URI for the Pravega Controller
#autoScale.controllerUri=pravega://localhost:9090

# A Stream name for internal purposes. Not externally visible.
#autoScale.requestStream=_requeststream

#autoScale.cooldownInSeconds=600
#autoScale.muteInSeconds=600
#autoScale.cacheCleanUpInSeconds=300
#autoScale.cacheExpiryInSeconds=1200

##endregion

##region Metrics Settings

# Whether to enable Metrics Reporting from this Pravega SegmentStore. This is the master switch for all metrics-related
# activities (if this is disabled, then all metrics-related services are too, regardless of their individual settings).
# Valid values: 'true' or 'false'.
metrics.enableStatistics=false

# Number of dynamic metrics statistic objects to keep in memory at once.
# Valid values: Positive integer.
# Recommended values: 10000 to 1000000. The higher the value, the more memory used, but it may improve performance slightly
# in scenarios where there are large number of active Stream Segments.
#metrics.dynamicCacheSize=100000

# Number of seconds to keep dynamic metric statistic objects in memory.
# Valid values: Positive integer.
# Recommended values: 60 to 300. The higher the value, the more memory used, but it may improve performance slightly
# in scenarios where there are large number of active Stream Segments.
#metrics.dynamicTTLSeconds=120

# Number of seconds between statistics reports.
# Valid values: Positive integer.
# Recommended values: A smaller value will produce more fine-grained reports, at the possible expense of extra CPU and IO
# overhead, while larger values will reduce overhead at the expense of more coarse reports.
#metrics.statsOutputFrequencySeconds=60

# Prefix to add to all Metrics counters.
#metrics.metricsPrefix=metrics.host

# Whether to enable CSV reporting.
# Valid values: 'true' or 'false'.
#metrics.enableCSVReporter=true

# CSV reporter target.
# Valid values: write-accessible filesystem directory.
#metrics.csvEndpoint=/tmp/csv

# Whether to enable StatsD reporting.
# Valid values: 'true' or 'false'.
#metrics.enableStatsdReporter=true

# StatsD endpoint host name.
#metrics.statsDPort=localhost

# StatsD endpoint port.
# Valid values: Positive integer.
#metrics.statsDHost=8125

# Whether to enable Graphite reporting.
# Valid values: 'true' or 'false'.
#metrics.enableGraphiteReporter=false

# Graphite endpoint host name.
#metrics.graphiteHost=localhost

# Graphite endpoint port.
# Valid values: Positive integer.
#metrics.graphitePort=2003

# Whether to enable JMX reporting.
# Valid values: 'true' or 'false'.
#metrics.enableJMXReporter=false

# JMX domain.
#metrics.jmxDomain=domain

# Whether to enable Ganglia reporting.
# Valid values: 'true' or 'false'.
#metrics.enableGangliaReporter=false

# Ganglia endpoint host name.
#metrics.gangliaHost=localhost

# Graphite endpoint port.
# Valid values: Positive integer.
#metrics.gangliaPort=8649

# Whether to enable Console reporting.
# Recommended values: 'false'. Set to 'true' only for debugging purposes.
#metrics.enableConsoleReporter=false

##endregion

##region BookKeeper Settings

# Endpoint address (hostname:port) where the ZooKeeper controlling BookKeeper for this cluster can be found at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
#bookkeeper.zkAddress=localhost:2181

# Default root path for BookKeeper Ledger metadata.  This property tells the BookKeeper client where to look up
# Ledger Metadata for the BookKeeper cluster it is connecting to. If this property isn't uncommented, then the
# default value for the path is the Pravega cluster namespace ("/pravega/<cluster-name>/") with "ledgers" appended:
# "/pravega/<cluster-name>/ledgers". Otherwise, it will take the value specified below.
#bookkeeper.bkLedgerPath=/pravega/bookkeeper/ledgers

# ZooKeeper session timeout (millis) for BookKeeper. This is only used for BookKeeper and has no influence on other
# components configured in this class.
#bookkeeper.zkSessionTimeoutMillis=10000

# ZooKeeper connection timeout (millis) for BookKeeper. This is only used for BookKeeper and has no influence on other
# components configured in this class.
#bookkeeper.zkConnectionTimeoutMillis=10000

# Hierarchy depth used for storing BookKeeper log metadata in ZooKeeper. This value is not to be confused with the
# BookKeeper ledger node structure (which also has a flat/hierarchical structure) - that is configured in the BookKeeper
# configuration file and deals with Ledger metadata. This deals strictly with the information that the Pravega Segment
# Store keeps in ZooKeeper.
# The hierarchy is based off indexing the last digits in the log ids in reverse order (depth == 2 means there will be
# two levels, first one based off the last digit and the second one based off the second-to-last digit). This value
# should be set such that there are no more than 100 nodes at the terminal nodes (i.e., if container count == 1024, then
# depth 0 would mean 1024 sibling nodes, depth=1 would mean 103 sibling nodes, depth 3 would mean about 10, and depth=4+
# would mean 1).
# This value must be the same for all Pravega SegmentStore instances in this cluster.
#bookkeeper.zkHierarchyDepth=2

# Ensemble size for BookKeeper ledgers.
# This value need not be the same for all Pravega SegmentStore instances in this cluster, but it highly recommended for
# consistency.
#bookkeeper.bkEnsembleSize=3

# Write Ack Quorum size for BookKeeper ledgers.
# This value need not be the same for all Pravega SegmentStore instances in this cluster, but it highly recommended for
# consistency.
#bookkeeper.bkAckQuorumSize=3

# Write Quorum size for BookKeeper ledgers.
# This value need not be the same for all Pravega SegmentStore instances in this cluster, but it highly recommended for
# consistency.
#bookkeeper.bkWriteQuorumSize=3

# Write Timeout, in milliseconds.
# Note: BookKeeper only allows multiples of 1 second (1000 millis). This value will be rounded up to the nearest second.
#bookkeeper.bkWriteTimeoutMillis=5000

# Read Timeout, in milliseconds.
# Note: BookKeeper only allows multiples of 1 second (1000 millis). This value will be rounded up to the nearest second.
#bookkeeper.bkReadTimeoutMillis=5000

# Maximum Ledger size (bytes) in BookKeeper. Once a Ledger reaches this size, it will be closed and another one open.
# Note that ledgers will not be cut off at this size, rather them reaching this size will trigger a rollover; in-flight
# writes will continue to get written to the previous ledger.
# Recommended values: at least 128MB, preferably 1GB.
# Maximum value: 2GB.
# This value need not be the same for all Pravega SegmentStore instances in this cluster, but it highly recommended for
# consistency.
#bookkeeper.bkLedgerMaxSize=1073741824

# The ZooKeeper sub-namespace where to store SegmentContainer Log Metadata. This will be rooted under the value of
# 'pravegaservice.clusterName' defined above.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
#bookkeeper.zkMetadataPath=/segmentstore/containers

# The maximum number of times to attempt to write an entry to BookKeeper (for retriable errors).
#bookkeeper.maxWriteAttempts=5


##endregion

##region HDFS Settings

# URL where the HDFS cluster is accessible at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
hdfs.hdfsUrl=localhost:9000

# Root path in HDFS where all Pravega-related data for this cluster is located at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# hdfs.hdfsRoot=

# The replication factor for all the data stored in HDFS. This can be used to control the availability of HDFS data.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# Valid values: Positive integer.
# Recommended values: At least 3.
#hdfs.replication=3

# The default block size, in bytes, that will be used to exchange data with HDFS.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# Valid values: Positive integer.
# Recommended values: Multiples of 1MB.
#hdfs.blockSize=1048576

##endregion

##region Extended S3 settings

# URL where the extended S3 cluster is accessible at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
extendeds3.url=localhost:9020

# Root path in extended S3 cluster where all Pravega-related data for this cluster is located at.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# extendeds3.root=

# ACCESS_KEY_ID to access the extended S3 cluster
# extendeds3.accessKey=

# SECRET_KEY to access the extended S3 cluster
# extendeds3.secretKey=

# Shared extended S3 bucket where the data is stored.
# This value must be the same for all Pravega SegmentStore instances in this cluster.
# extendeds3.bucket=

##endregion

##region filesystem settings

# Root path where NFS shared directory needs to be mounted before segmentstore starts execution.
# filesystem.root=

##endregion

##region RocksDB Settings

# Path to the working directory where RocksDB can store its databases. The contents of this folder can be discarded after
# the process exits (and it will be cleaned up upon startup), but Pravega requires exclusive use of this while running.
# Recommended values: a path to a locally mounted directory that sits on top of a fast SSD.
#rocksdb.dbDir=/tmp/pravega/cache

# Parameters to bound the memory consumption of RocksDB. The total memory consumption of the database is mostly dictated
# by writeBufferSizeMB (memtable) and readCacheSizeMB (block cache). However, RocksDB also stores data in memory related
# to internal indexes in addition to the previous parameters (e.g., it may range between 5% to 30% of the total memory
# consumption depending on the configuration and data at hand). The size of the internal indexes in RocksDB mainly
# depend on the size of data blocks in the cache (cacheBlockSizeKB). That is, if you increase cacheBlockSizeKB, the
# number of blocks will decrease, so the index size will also reduce linearly. For an explanation in depth of these
# parameters, we refer to: https://github.com/facebook/rocksdb/wiki/Memory-usage-in-RocksDB. To help users, we provide
# guidelines to configure RocksDB in Pravega: https://github.com/pravega/pravega/wiki/RocksDB-Configuration.

# RocksDB allows to buffer writes in-memory (memtables) to improve write performance, thus executing an async flush
# process of writes to disk. This parameter bounds the maximum amount of memory devoted to absorb writes.
#rocksdb.writeBufferSizeMB=64

# RocksDB caches (uncompressed) data blocks in memory to serve read requests with high performance in case of a cache
# hit. This parameter bounds the maximum amount of memory devoted to cache uncompressed data blocks.
#rocksdb.readCacheSizeMB=8

# RocksDB stores data in memory related to internal indexes (e.g., it may range between 5% to 30% of the total memory
# consumption depending on the configuration and data at hand). The size of the internal indexes in RocksDB mainly
# depend on the size of cached data blocks (cacheBlockSizeKB). If you increase cacheBlockSizeKB, the number of blocks
# will decrease, so the index size will also reduce linearly (but increasing read amplification).
#rocksdb.cacheBlockSizeKB=32

##endregion

##region DurableLog Settings

# A Metadata Checkpoint is a special internal write inside the DurableLog (persisted to Tier1 DurableDataLog) that takes
# a snapshot of the current state of the Container Metadata (all active Stream Segments, with all of their states). This
# represents point in the Tier1 DurableDataLog where recovery can start from (recovery cannot start from an arbitrary
# location since we need to know the current state of the metadata before we begin that). As such, we can only truncate
# Tier1 DurableDataLog only where these Checkpoints are located. Serializing a Checkpoint will cause all outstanding
# write operations to wait on it, so when it does happen, it adds a bit of latency. They also have a direct effect on the
# recovery time (upon recovery, the entire Tier1 DurableDataLog needs to be processed, so the further apart such Checkpoints
# are, the more time it will take). Based on the application requirements, it may be desirable to choose more frequent
# Checkpoints (if a speedy recovery is desired and no discernible impact on online performance is noted), or less frequent
# Checkpoints should online performance play a very important factor.


# The minimum number of commits that must be to be accumulated in order to trigger a Metadata Checkpoint. This is used
# in case we have accumulated enough bytes to trigger a Checkpoint, but not enough commits (this is to ensure that a single
# large write doesn't trigger such a checkpoint by itself).
# Valid values: Positive integer.
# Recommended values: Larger than 100. Choosing a higher value will cause larger gaps between checkpoints, improving
# online performance but increasing failover recovery time.
#durablelog.checkpointMinCommitCount=300

# The number of commits that would trigger a Checkpoint. This is used in case we were not able to accumulate enough bytes
# to trigger a Checkpoint, as we do want to eventually be able to truncate.
# Valid values: Positive integer, but larger than or equal to 'durablelog.checkpointMinCommitCount'.
# Recommended values: Larger than 100. Choosing a higher value will cause larger gaps between checkpoints, improving
# online performance but increasing failover recovery time.
#durablelog.checkpointCommitCountThreshold=300

# The number of bytes written that would trigger a Checkpoint.
# Valid values: Positive integer.
# Recommended values: Larger than 100MB. Choosing a higher value will cause larger gaps between checkpoints, improving
# online performance but increasing failover recovery time.
#durablelog.checkpointTotalCommitLengthThreshold=268435456

##endregion

##region ReadIndex Settings

# A note on Cache Management. Each append to a Stream Segment and each cache miss will cause an entry to be written to
# the Cache (as such, entries have a variable length). The Cache is shared across all Segment Containers in a Pravega
# SegmentStore instance and its entry lifecycle is determined by three major variables: cacheMaxSize, cacheMaxTimeMillis
# and cacheGenerationTimeMillis. A Cache Generation is a proxy for time passage (instead of using a single time-based stamp,
# each cache entry has a generation id. All hits (or additions) during a generation will get that generation id. The Cache
# Manager adjusts the current generation and oldest permissible generation to manage the cache (entries with a generation
# older than the oldest ones are candidates for eviction). The oldest generation is increased either when the physical
# amount of time between now and the oldest generation exceeds cacheMaxTimeMillis or when there is cache pressure and
# the maximum amount of data in the cache exceeds cacheMaxSize.

# A value to align all Tier2 Storage Reads to. When a Tier2 Storage Read is issued, the read length is adjusted (if possible)
# to end on a multiple of this value. This value can be used to configure read-ahead size.
# Valid values: Positive integer.
# Recommended values: Multiples of 1MB. The higher the value, the better the catch-up read-ahead performance will be, at
# the expense of the size of the initial read. However, choosing too large of a value may have adverse effects in case
# catch-up reads are not sequential.
#readindex.storageReadAlignment=1048576

# Minimum number of bytes to return from reads if all these bytes are readily available in memory. This value should only
# be changed if it is suspected that tail-read performance suffers because of an unusual high number of very small (<100b)
# appends.
# Valid values: Positive integer.
# Recommended values: Multiples of 1KB. The higher the value, the better the read performance will be in cases of very
# small tail writes.
#readindex.memoryReadMinLength=4096

##endregion

##region AttributeIndex Settings

# The maximum page size for the Attribute B+Tree index. Once a page (index or leaf) exceeds this limit, it will be split
# into two or more pages.
# Valid values: Positive integer, greater than or equal to 1024 and smaller than or equal to 32767.
# Recommended values: 16KB. Various load tests showed that this maximum page size produces the smallest B+Tree index
# segment when loaded with either sequential values (very rare) or arbitrary values in arbitrary order (most common); this
# value also produces the best balance between read/write performance and index fragmentation.
#attributeindex.maxIndexPageSizeBytes=32767

# The Attribute Rolling Segment Rolling Policy (size of each individual segment chunk, in bytes).
# Valid values: Positive integer.
# Recommended values: (approximately) 1000 x maxIndexPageSizeBytes.
#attributeindex.attributeSegmentRollingSizeBytes=33554432

##region Writer Settings

# The minimum number of bytes to wait for before flushing aggregated data for a Segment to Tier2 Storage. The trigger to
# flush is either when this condition is met or when the 'flushThresholdMillis' condition is met.
# Valid values: Positive integer.
# Recommended values: multiples of 1MB. A lower value will cause more frequent writes to Tier2 Storage, which could cause
# data to pile up if Tier2 Storage has a high per-write latency. A higher value will cause less frequent writes to Tier2
# Storage, which could cause less frequent truncations of Tier1 data (and as such, more time for failover recovery).
#writer.flushThresholdBytes=4194304

# The minimum amount of time (in milliseconds) to wait for before flushing aggregated data for a Segment to Tier2 Storage.
# The trigger to flush is either when this condition is met or when the 'flushThresholdBytes' condition is met.
# Valid values: Positive integer.
# Recommended values: multiples of 1 second. A lower value will cause more frequent writes to Tier2 Storage, which could cause
# data to pile up if Tier2 Storage has a high per-write latency. A higher value will cause less frequent writes to Tier2
# Storage, which could cause less frequent truncations of Tier1 data (and as such, more time for failover recovery).
#writer.flushThresholdMillis=30000

# The maximum number of bytes that can be flushed with a single write operation.
# Valid values: Positive integer.
#writer.maxFlushSizeBytes=4194304

# The maximum number of items to read every time a read is issued to the DurableLog.
# Valid values: Positive integer.
# Recommended values: 100-1000. Lower values for systems where there is a high incidence of large appends, and higher values
# for systems where there is a high incidence of smaller appends.
#writer.maxItemsToReadAtOnce=1000

# The minimum timeout (in milliseconds) to wait for new items to come into the DurableLog (in case we reached the tail).
# Valid values: Positive integer.
#writer.minReadTimeoutMillis=2000

# The maximum timeout (in milliseconds) to wait for new items to come into the DurableLog (in case we reached the tail).
# Valid values: Positive integer.
#writer.maxReadTimeoutMillis=1800000

# The amount of time to sleep (in milliseconds) if an exception was encountered during the execution of a Writer's iteration.
# This can be used as a way to "back off" in cases a transient error occurred and not go into a very tight loop while
# the exception condition is still active.
# Valid values: Positive integer.
#writer.errorSleepMillis=1000

# The timeout (in milliseconds) for flushing data to Tier2 Storage.
# Valid values: Positive integer.
#writer.flushTimeoutMillis=60000

# The timeout (in milliseconds) for acknowledging data writes (and subsequently truncating Tier1 DurableDataLog).
# Valid values: Positive integer.
#writer.ackTimeoutMillis=15000

# The timeout (in milliseconds) for waiting for a proper shutdown.
# Valid values: Positive integer.
#writer.shutdownTimeoutMillis=10000

##endregion
