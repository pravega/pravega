{
    "docs": [
        {
            "location": "/",
            "text": "Pravega Overview\n\n\nPravega is an open source storage primitive implementing \nStreams\n for continuous and unbounded data. A Pravega stream is a durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency.  \n\n\nRead \nPravega Concepts\n for more details.\n\n\nKey Features\n\n\n\n\n\n\nExactly-Once Semantics -\u00a0Ensure that each event is delivered and processed exactly once, with exact ordering guarantees, despite failures in clients, servers or the network.\n\n\n\n\n\n\nAuto Scaling -\u00a0Unlike systems with static partitioning, Pravega can automatically scale individual data streams to accommodate changes in data ingestion rate.\n\n\n\n\n\n\nDistributed Computing Primitive - Pravega is great for distributed computing; it can be used as a data storage mechanism, for messaging between processes and for other distributed computing services such as leader election.\n\n\n\n\n\n\nWrite Efficiency -\u00a0Pravega shrinks write latency to milliseconds, and seamlessly scales to handle high throughput reads and writes from thousands of concurrent clients, making it ideal for IoT and other time sensitive applications.\n\n\n\n\n\n\nUnlimited Retention -\u00a0Ingest, process and retain data in streams forever. Use same paradigm to access both real-time and historical events stored in Pravega.\n\n\n\n\n\n\nStorage Efficiency - Use Pravega to build pipelines of data processing, combining batch, real-time and other applications without duplicating data for every step of the pipeline.\n\n\n\n\n\n\nDurability -\u00a0Don't compromise between performance, durability and consistency.\n    Pravega persists and protects data before the write operation is acknowledged to the client.\n\n\n\n\n\n\nTransaction Support -\u00a0A developer uses a Pravega Transaction to ensure that a set of events are written to a stream atomically.\n\n\n\n\n\n\nReleases\n\n\nThe latest pravega releases can be found on the \nGithub Release\n project page.\n\n\nQuick Start\n\n\nRead \nGetting Started\n page for more information, and also visit \nsample-apps\n repo for more applications. \n\n\nFrequently Asked Questions\n\n\nYou can find a list of frequently asked questions \nhere\n.\n\n\nRunning Pravega\n\n\nPravega can be installed locally or in a distributed environment. The installation and deployment of Pravega is covered in the \nRunning Pravega\n guide.\n\n\nSupport\n\n\nDon\u2019t hesitate to ask! Contact the developers and community on the \nmailing lists\n or on \nslack\n if you need any help. \nOpen an issue if you found a bug on \nGithub\nIssues\n\n\nContributing\n\n\nBecome one of the contributors! We thrive to build a welcoming and open\ncommunity for anyone who wants to use the system or contribute to it.\n\nHere\n\u00a0we describe how to\ncontribute to Pravega! You can see the roadmap document \nhere\n\n\nAbout\n\n\nPravega is 100% open source and community-driven. All components are available\nunder \nApache 2 License\n on\nGitHub.",
            "title": "Overview"
        },
        {
            "location": "/#pravega-overview",
            "text": "Pravega is an open source storage primitive implementing  Streams  for continuous and unbounded data. A Pravega stream is a durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency.    Read  Pravega Concepts  for more details.",
            "title": "Pravega Overview"
        },
        {
            "location": "/#key-features",
            "text": "Exactly-Once Semantics -\u00a0Ensure that each event is delivered and processed exactly once, with exact ordering guarantees, despite failures in clients, servers or the network.    Auto Scaling -\u00a0Unlike systems with static partitioning, Pravega can automatically scale individual data streams to accommodate changes in data ingestion rate.    Distributed Computing Primitive - Pravega is great for distributed computing; it can be used as a data storage mechanism, for messaging between processes and for other distributed computing services such as leader election.    Write Efficiency -\u00a0Pravega shrinks write latency to milliseconds, and seamlessly scales to handle high throughput reads and writes from thousands of concurrent clients, making it ideal for IoT and other time sensitive applications.    Unlimited Retention -\u00a0Ingest, process and retain data in streams forever. Use same paradigm to access both real-time and historical events stored in Pravega.    Storage Efficiency - Use Pravega to build pipelines of data processing, combining batch, real-time and other applications without duplicating data for every step of the pipeline.    Durability -\u00a0Don't compromise between performance, durability and consistency.\n    Pravega persists and protects data before the write operation is acknowledged to the client.    Transaction Support -\u00a0A developer uses a Pravega Transaction to ensure that a set of events are written to a stream atomically.",
            "title": "Key Features"
        },
        {
            "location": "/#releases",
            "text": "The latest pravega releases can be found on the  Github Release  project page.",
            "title": "Releases"
        },
        {
            "location": "/#quick-start",
            "text": "Read  Getting Started  page for more information, and also visit  sample-apps  repo for more applications.",
            "title": "Quick Start"
        },
        {
            "location": "/#frequently-asked-questions",
            "text": "You can find a list of frequently asked questions  here .",
            "title": "Frequently Asked Questions"
        },
        {
            "location": "/#running-pravega",
            "text": "Pravega can be installed locally or in a distributed environment. The installation and deployment of Pravega is covered in the  Running Pravega  guide.",
            "title": "Running Pravega"
        },
        {
            "location": "/#support",
            "text": "Don\u2019t hesitate to ask! Contact the developers and community on the  mailing lists  or on  slack  if you need any help. \nOpen an issue if you found a bug on  Github\nIssues",
            "title": "Support"
        },
        {
            "location": "/#contributing",
            "text": "Become one of the contributors! We thrive to build a welcoming and open\ncommunity for anyone who wants to use the system or contribute to it. Here \u00a0we describe how to\ncontribute to Pravega! You can see the roadmap document  here",
            "title": "Contributing"
        },
        {
            "location": "/#about",
            "text": "Pravega is 100% open source and community-driven. All components are available\nunder  Apache 2 License  on\nGitHub.",
            "title": "About"
        },
        {
            "location": "/getting-started/",
            "text": "Getting started\n\n\nThe best way to get to know Pravega is to start it up and run a sample Pravega\napplication.\n\n\nRunning Pravega is simple\n\n\nVerify the following prerequisite\n:\n\n\nJava 8\n\n\n\n\nDownload Pravega\n\n\nDownload the Pravega release from the \ngithub releases page\n.\nIf you prefer to build Pravega yourself, you can download the code and run \n./gradlew distribution\n. More \ndetails are shown in the Pravega \nREADME.md\n.\n\n\n$ tar xfvz pravega-0.1.0.tgz\n\n\n\n\nRun Pravega in standalone mode\n\n\nThis launches all the components of Pravega on your local machine.\nNOTE: this is for testing/demo purposes only, \ndo not\n use this mode of deployment \nin Production! More options for \nRunning Pravega\n are\ncovered in the running Pravega guide.           \n\n\n$ cd pravega-0.1.0\n$ bin/pravega-standalone\n\n\n\n\nThat's it.  Pravega should be up and running very soon.\n\n\nYou can find additional ways to run Pravega in \nRunning Pravega\n.\n\n\nRunning a sample Pravega App is simple too\n\n\nPravega maintains a separate github repository for sample applications.  It is located at:\n\nhttps://github.com/pravega/pravega-samples\n\n\nLets download and run the \"Hello World\" Pravega sample reader and writer apps. Pravega\ndependencies will be pulled from maven central.\n\n\nNote: The samples can also use a locally compiled version of Pravega. For more information\nabout this see the note on maven publishing in the \nREADME.md\n.\n\n\nDownload the Pravega-Samples git repo\n\n\n$ git clone https://github.com/pravega/pravega-samples\n$ cd pravega-samples\n\n\n\n\nGenerate the scripts to run the apps\n\n\n$ ./gradlew installDist\n\n\n\n\nRun the sample \"HelloWorldWriter\"\n\nThis runs a simple Java application that writes a \"hello world\" message\n        as an event into a Pravega stream.\n\n\n$ cd pravega-samples/standalone-examples/build/install/pravega-standalone-examples\n$ bin/helloWorldWriter\n\n\n\n\nExample HelloWorldWriter output\n\n\n...\nWriting message: 'hello world' with routing-key: 'helloRoutingKey' to stream 'examples / helloStream'\n...\n\n\n\n\nSee the \nreadme.md\n file in the standalone-examples for more details\n    on running the HelloWorldWriter with different parameters\n\n\nRun the sample \"HelloWorldReader\"\n\n\n$ cd pravega-samples/standalone-examples/build/install/pravega-standalone-examples\n$ bin/helloWorldReader\n\n\n\n\nExample HelloWorldReader output\n\n\n...\nReading all the events from examples/helloStream\n...\nRead event 'hello world'\nNo more events from examples/helloStream\n...\n\n\n\n\nSee the readme.md file in the standalone-examples for more details on running the\n    HelloWorldReader application",
            "title": "Getting Started"
        },
        {
            "location": "/getting-started/#getting-started",
            "text": "The best way to get to know Pravega is to start it up and run a sample Pravega\napplication.",
            "title": "Getting started"
        },
        {
            "location": "/getting-started/#running-pravega-is-simple",
            "text": "Verify the following prerequisite :  Java 8  Download Pravega  Download the Pravega release from the  github releases page .\nIf you prefer to build Pravega yourself, you can download the code and run  ./gradlew distribution . More \ndetails are shown in the Pravega  README.md .  $ tar xfvz pravega-0.1.0.tgz  Run Pravega in standalone mode  This launches all the components of Pravega on your local machine.\nNOTE: this is for testing/demo purposes only,  do not  use this mode of deployment \nin Production! More options for  Running Pravega  are\ncovered in the running Pravega guide.             $ cd pravega-0.1.0\n$ bin/pravega-standalone  That's it.  Pravega should be up and running very soon.  You can find additional ways to run Pravega in  Running Pravega .",
            "title": "Running Pravega is simple"
        },
        {
            "location": "/getting-started/#running-a-sample-pravega-app-is-simple-too",
            "text": "Pravega maintains a separate github repository for sample applications.  It is located at: https://github.com/pravega/pravega-samples  Lets download and run the \"Hello World\" Pravega sample reader and writer apps. Pravega\ndependencies will be pulled from maven central.  Note: The samples can also use a locally compiled version of Pravega. For more information\nabout this see the note on maven publishing in the  README.md .  Download the Pravega-Samples git repo  $ git clone https://github.com/pravega/pravega-samples\n$ cd pravega-samples  Generate the scripts to run the apps  $ ./gradlew installDist  Run the sample \"HelloWorldWriter\" \nThis runs a simple Java application that writes a \"hello world\" message\n        as an event into a Pravega stream.  $ cd pravega-samples/standalone-examples/build/install/pravega-standalone-examples\n$ bin/helloWorldWriter  Example HelloWorldWriter output  ...\nWriting message: 'hello world' with routing-key: 'helloRoutingKey' to stream 'examples / helloStream'\n...  See the  readme.md  file in the standalone-examples for more details\n    on running the HelloWorldWriter with different parameters  Run the sample \"HelloWorldReader\"  $ cd pravega-samples/standalone-examples/build/install/pravega-standalone-examples\n$ bin/helloWorldReader  Example HelloWorldReader output  ...\nReading all the events from examples/helloStream\n...\nRead event 'hello world'\nNo more events from examples/helloStream\n...  See the readme.md file in the standalone-examples for more details on running the\n    HelloWorldReader application",
            "title": "Running a sample Pravega App is simple too"
        },
        {
            "location": "/pravega-concepts/",
            "text": "Pravega Concepts\n\n\nPravega is an open source storage primitive implementing \nStreams\n for continuous and unbounded data.\n\n\nThis page is an overview of the key concepts in Pravega.\n\u00a0See\u00a0\nTerminology\n for a concise definition for many\nPravega concepts.\n\n\nStreams\n\n\nPravega organizes data into Streams. \u00a0A Stream is a durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency. \u00a0A Pravega Stream is\nsimilar to but more flexible than a \"topic\" in popular message-oriented middleware such as\n\nRabbitMQ\n or Apache Kafka.\n\n\nPravega Streams are based on an append-only log data structure. By using\nappend-only logs, Pravega can rapidly ingest data into durable storage,\nand support a large variety of application use cases such as stream processing using frameworks like \nFlink\n, publish/subscribe\nmessaging, NoSQL databases such as a Time Series\nDatabase (TSDB), workflow engines, event-oriented applications and many other\nkinds of applications.\u00a0\n\n\nWhen a developer creates a Stream in Pravega, s/he gives the Stream a meaningful\nname such as \"IoTSensorData\" or \"WebApplicationLog20170330\". \u00a0The\nStream's name helps other developers understand the kind of data that is stored\nin the Stream. \u00a0It is also worth noting that Pravega Stream names are organized\nwithin a Scope. \u00a0A Scope is a string and should convey some sort of meaning to\ndevelopers such as \"FactoryMachines\" or \"HRWebsitelogs\". \u00a0A Scope acts as a\nnamespace for Stream names\u00a0\u2013 all Stream names are unique within a Scope.\n\u00a0Therefore a Stream is uniquely identified by the combination of its Stream name\nand Scope. \u00a0Scope can be used to segregate names by tenant (in a multi tenant\nenvironment), by department in an organization, by geographic location or any\nother categorization the developer chooses.\n\n\nA Stream is unbounded in size\u00a0\u2013 Pravega itself does not impose any limits on how\nmany Events can be in a Stream or how many total bytes are stored in a Stream.\n\u00a0Pravega is built as a data storage primitive first and foremost. \u00a0Pravega is\ncarefully designed to take advantage of software defined storage so that the\namount of data stored in Pravega is limited only by the total storage capacity\nof your data center. \u00a0And like you would expect from a storage primitive, once\ndata is written to Pravega it is durably stored. \u00a0Short of a disaster that\npermanently destroys a large portion of a data center, data stored in Pravega is never\nlost.\n\n\nTo deal with a potentially large amount of data within a Stream, Pravega Streams\nare divided into Stream Segments. \u00a0A Stream Segment is a shard, or partition of\nthe data within a Stream. \u00a0We go into a lot more detail on Stream Segments a bit\nlater in this document. \u00a0Stream Segments are an important concept, but we need\nto introduce a few more things before we can dive into Stream Segments.\n\n\nApplications, such as a Java program reading from an IoT sensor, write data to\nthe tail (front) of the Stream. \u00a0Applications, such as a \nFlink\n analytics job,\ncan read from any point in the Stream. Lots of applications can read and write\nthe same Stream in parallel. \u00a0Elastic, scalable support for a large volume of\nStreams, data and applications is at the heart of Pravega's design. \u00a0We will\nget into\u00a0more\u00a0details about how applications read and write Streams a bit later\nin this document when we detail Readers and Writers.\n\n\nEvents\n\n\nPravega's client API allows applications to read and write data in Pravega in terms of an Event. \u00a0An Event is a\nset of bytes within a Stream. \u00a0An Event could be as simple as a small\nnumber of bytes containing a temperature reading from an IoT sensor composed of\na timestamp, a metric identifier and a value. \u00a0An Event could be web log data\nassociated with a user click on a website. \u00a0Events can be anything you can\nrepresent as a set of bytes. \u00a0Applications make sense of Events using\nstandard Java serializers and deserializers, allowing them to read and write\nobjects in Pravega using similar techniques to reading and writing objects from\nfiles.\n\n\nEvery Event has a Routing Key. \u00a0A Routing Key allows Pravega\nand application developers to reason about which Events are related. \u00a0\u00a0A Routing\nKey is\u00a0just\u00a0a string that developers use to group similar Events together.\u00a0A\nRouting Key is often derived from data naturally occurring in the Event,\nsomething like \"customer-id\" or \"machine-id\", but it could also be some\nartificial String. A\nRouting Key could be something\u00a0like a\u00a0date (to group Events together by\ntime) or perhaps a Routing Key could be a IoT sensor id (to group Events by\nmachine). \u00a0A Routing Key is important to defining the precise read and write\nsemantics that Pravega guarantees, we will get into that detail a bit later,\nafter we have introduced a few more key concepts.\n\n\nWriters, Readers, ReaderGroups\n\n\n\n\nPravega provides a client library, written in Java, that implements a convenient\nAPI for Writer and Reader applications to use.\u00a0 The Pravega Java Client Library\nencapsulates the wire protocol used to communicate between Pravega clients and\nPravega.\n\n\nA Writer is an application that creates Events and writes them into a Stream.\nAll data is written by appending to the tail (front) of a Stream.\n\n\nA Reader is an application that reads Events from a Stream. \u00a0Readers can read\nfrom any point in the Stream.\u00a0 Many Readers will be reading Events from the tail\nof the Stream.\u00a0 These Events will be delivered to Readers as quickly as possible.\u00a0 Some Readers will read\nfrom earlier parts of the Stream (called catch-up reads).\u00a0 The application\ndeveloper has control over where in the Stream the Reader starts reading.\n\u00a0Pravega has the concept of a Position, that represents where in a Stream a\nReader is currently located. \u00a0The Position object can be used as a recovery\nmechanism\u00a0\u2013 applications that persist the last Position a Reader has\nsuccessfully processed can use that information to initialize a replacement\nReader to pickup where a failed Reader left off. \u00a0Using this pattern of\npersisting Position objects, applications can be built that guarantee exactly\nonce Event processing in the face of Reader failure.\n\n\nReaders are organized into ReaderGroups. \u00a0A ReaderGroup is a named collection of\nReaders that together, in parallel, read Events from a given Stream.\u00a0When a\nReader is created through the Pravega data plane API, the developer includes the\nname of the ReaderGroup it is part of. \u00a0We guarantee that each Event published\nto a Stream is sent to exactly one Reader within the ReaderGroup. \u00a0There could\nbe 1 Reader in the ReaderGroup, there could be many. \u00a0There could be many\ndifferent ReaderGroups simultaneously reading from any given Stream.\n\n\nYou can think of a ReaderGroup as a \"composite Reader\" or a \"distributed\nReader\", that allows a distributed application to read and process Stream data\nin parallel, such that a massive amount of Stream data can be consumed by a\ncoordinated fleet of Readers in a ReaderGroup. \u00a0A collection of Flink tasks\nprocessing Stream data in parallel is a good example use of a ReaderGroup.\n\n\nFor more details on the basics of working with Pravega Readers and Writers,\nsee\u00a0\nWorking with Pravega: Basic Reader and\nWriter\n.\n\n\nWe need to talk in more detail about the relationship between Readers,\nReaderGroups and Streams and the ordering guarantees provided by Pravega. \u00a0But\nfirst, we need to describe what a Stream Segment is.\n\n\nStream Segments\n\n\nA Stream is decomposed into a set of Stream Segments; a Stream Segment is a\nshard or partition of a Stream.\n\n\n\u00a0\n\n\nAn Event is Stored within a Stream Segment\n\n\nThe Stream Segment is the container for Events within the Stream. \u00a0 When an\nEvent is written into a Stream, it is stored in one of the Stream Segments based\non the Event's Routing Key. \u00a0Pravega uses consistent hashing to assign Events to\nStream Segments.\u00a0Event Routing Keys are hashed to form a \"key space\" . \u00a0The key\nspace is then divided into a number of partitions, corresponding to the number\nof Stream Segments. Consistent hashing determines which Segment an Event is\nassigned to.\n\n\nAutoScaling:\u00a0The\u00a0number of Stream Segments can vary over time\n\n\nThe number of Stream Segments in a Stream\u00a0can grow\u00a0\nand shrink\n\u00a0over time as I/O\nload on the Stream increases and decreases. \u00a0 We refer to this feature as\nAutoScaling.\n\n\nConsider the following figure that shows the relationship between Routing Keys\nand time.\n\n\n\u00a0\n\n\nA Stream starts out at time t0 with a configurable number of Segments.\u00a0 If the\nrate of data written to the Stream is constant, the number of Segments won\u2019t\nchange.\u00a0 However at time t1, the system noted an increase in the ingestion rate\nand chose to split Segment 1 into two parts. \u00a0We call this a Scale-up event.\n\u00a0Before t1, Events with a Routing Key that hashes to the upper part of the key\nspace (values 200-399) would be placed in Segment 1 and those that hash into the\nlower part of the key space (values 0-199) would be placed in Segment 0. After\nt1, Segment 1 is split into Segment 2 and Segment 3. \u00a0Segment 1 is sealed and it\nno longer accepts writes. \u00a0At this point in time, Events with Routing Key\n300\u00a0and\u00a0above are written to Segment 3 and those between 200 and 299 would be\nwritten into Segment 2. \u00a0Segment 0 still keeps accepting\u00a0the\u00a0same range of\nEvents as before t1. \u00a0\n\n\nWe also see another Scale-up event at time t2, as Segment 0\u2019s range of Routing\nKey is split into Segment 5 & Segment 4. \u00a0Also at this time, Segment 0 is sealed\noff so that it accepts no further writes.\n\n\nSegments covering a contiguous range of the key space can also be merged.\u00a0 At\ntime t3, Segment 2\u2019s range and Segment 5\u2019s range are merged into Segment 6 to\naccommodate a decrease in load on the Stream.\n\n\nWhen a Stream is created, it is configured with an Scaling Policy that\ndetermines how a Stream reacts to changes in its load. \u00a0Currently there are\nthree kinds of Scaling Policy:\n\n\n\n\n\n\nFixed. \u00a0The number of Stream Segments does not vary with load\n\n\n\n\n\n\nSize-based. \u00a0As the number of bytes of data per second written to the Stream\n    increases past a certain target rate, the number of Stream Segments is\n    increased. \u00a0If it falls below a certain level, decrease the number of Stream\n    Segments.\n\n\n\n\n\n\nEvent-based. \u00a0Similar to Size-based Scaling Policy, except it uses the\n    number of Events instead of the number of bytes.\n\n\n\n\n\n\nEvents, Stream Segments and AutoScaling\n\n\nWe mentioned previously that an Event is written into one of the Stream's\nSegments. \u00a0Taking into account AutoScaling, you should think of Stream Segments\nas a bucketing of Events based on Routing Key and time. \u00a0At any given time,\nEvents published to a Stream within a given value of Routing Key will all appear\nin the same Stream Segment.\n\n\n\u00a0\n\n\nIt is also worth emphasizing that Events are written into only the active Stream\nSegments. \u00a0Segments that are sealed do not accept writes. \u00a0In the figure above,\nat time \"now\", only Stream Segments 3, 6 and 4 are active and between those\nthree Stream Segments the entire key space is covered. \u00a0\n\n\nStream Segments and ReaderGroups\n\n\nStream Segments are important to understanding the way Reader Groups work.\n\n\n\u00a0\n\n\nPravega assigns each Reader in a ReaderGroup zero or more Stream Segments to\nread from. \u00a0Pravega tries to balance out the number of Stream Segments each\nReader is assigned. \u00a0In the figure above, Reader B1 reads from 2 Stream Segments\nwhile each of the other Readers in the Reader Group have only 1 Stream Segment\nto read from. \u00a0Pravega makes sure that each Stream Segment is read by exactly\none Reader in any ReaderGroup configured to read from that Stream. As Readers\nare added to the ReaderGroup, or Readers crash and are removed from the\nReaderGroup, Pravega reassigns Stream Segments so that Stream Segments are\nbalanced amongst the Readers.\n\n\nThe number of Stream Segments in a Stream determines the upper bound of\nparallelism of readers within a ReaderGroup \u2013\u00a0the more Stream Segments, the more\nseparate, parallel sets of Readers we can have consuming the Stream.\u00a0In the\nabove figure, Stream1 has 4 Stream Segments. \u00a0That means that the largest\neffective Reader Group would contain 4 Readers. \u00a0Reader Group named \"B\" in the\nabove figure is not quite optimal. \u00a0If one more Reader was added to the\nReaderGroup, each Reader would have 1 Stream Segment to process, maximizing read\nparallelism. \u00a0However, the number of Readers in the ReaderGroup increases beyond\n4, at least one of the Readers will not be assigned a Stream Segment.\n\n\nIf Stream1 in the figure above experienced a Scale-Down event, reducing the\nnumber of Stream Segments to 3, then Reader Group B as depicted would have an\nideal number of Readers.\n\n\nWith the AutoScaling feature, Pravega developers don't have to configure their\nStreams with a fixed, pre-determined number of Stream Segments\u00a0\u2013 Pravega can\ndynamically determine the right number. \u00a0With this feature, Pravega Streams can\ngrow and shrink to match the behavior of the data input. \u00a0The size of any Stream\nis limited only by the total storage capacity made available to the Pravega\ncluster; if you need bigger streams, simply add more storage to your cluster.\n\n\nApplications can react to changes\nin the number of Segments in a Stream, adjusting the number of Readers within a\nReaderGroup, to maintain optimal read parallelism if resources allow. \u00a0This is\nuseful, for example in a Flink application, to allow Flink to increase or\ndecrease the number of task instances that are processing a Stream in parallel,\nas scale events occur over time.\n\n\nOrdering Guarantees\n\n\nA stream comprises a set of segments that can change over time. Segments that overlap in their area of keyspace have a defined order.\n\n\nAn event written to a stream is written to a single segment and it is totally ordered with respect to the events of that segment. The existance and position of an event within a segment is strongly consistent.\n\n\nReaders can be assigned multiple parallel segments (from different parts of keyspace). A reader reading from multiple segments will interleave the events of the segments, but the order of events per segment respects the one of the segment. Specifically, if s is a segment, events e~1 and e~2 of s are such that e~1 precedes e~2, and a reader reads both e~1 and e~2, then the reader will read e~1 before e~2.\n\n\nThis results in the following ordering guarantees:\n\n\n\n\n\n\nEvents with the same Routing Key are consumed in the order they were written.\n\n\n\n\n\n\nEvents with different Routing Keys sent to a specific segment will always be\n    seen in the same order even if the Reader backs up and re-reads them.\n\n\n\n\n\n\nIf an event has been acked to its writer or has been read by a reader it is guaranteed that it will continue to exist in the same place for all subsequent reads until it is deleted.\n\n\n\n\n\n\nIf there are multiple Readers reading a Stream and they all back up to any given point, they will never see any reordering with respect to that point. (It will never be the case that an event that they read before the chosen point now comes after or vice versa.)\n\n\n\n\n\n\nReaderGroup Checkpoints\n\n\nPravega provides the ability for an application to initiate a Checkpoint on a\nReaderGroup. \u00a0The idea with a Checkpoint is to create a consistent \"point in\ntime\" persistence of the state of each Reader in the ReaderGroup, by using a\nspecialized Event (a Checkpoint Event) to signal each Reader to preserve its\nstate. \u00a0Once a Checkpoint has been completed, the application can use the\nCheckpoint to reset all the Readers in the ReaderGroup to the known consistent\nstate represented by the Checkpoint.\n\n\nFor more details on working with ReaderGroups, see\u00a0\nWorking with Pravega: Reader\nGroups\n.\n\n\nTransactions\n\n\nPravega supports Transactions. \u00a0The idea of a Transaction is that a Writer can\n\"batch\" up a bunch of Events and commit them as a unit into a Stream. \u00a0This is\nuseful, for example, with Flink jobs, using Pravega as a sink. \u00a0The Flink job\ncan continuously produce results of some data processing and use the Transaction\nto durably accumulate the results of the processing. \u00a0At the end of some sort of\ntime window (for example) the Flink job can commit the Transaction and therefore\nmake the results of the processing available for downstream processing, or in\nthe case of an error, abort the Transaction and the results disappear.\n\n\nA key difference between Pravega's Transactions and similar approaches (such as\nKafka's producer-side batching) is related to durability. \u00a0Events added to a\nTransaction are durable when the Event is ack'd back to the Writer. \u00a0However,\nthe Events in the Transaction are NOT visible to readers until the Transaction\nis committed by the Writer. \u00a0A Transaction is a lot like a Stream; a Transaction\nis associated with multiple Stream Segments. \u00a0When an Event is published into a\nTransaction, the Event itself is appended to a Stream Segment of the\nTransaction. \u00a0Say a Stream had 5 Segments, when a Transaction is created on that\nStream, conceptually that Transaction also has 5 Segments. \u00a0When an Event is\npublished into the Transaction, it is routed to the same numbered Segment as if\nit were published to the Stream itself (if the Event would have been placed in\nSegment 3 in the \"real\" Stream, then it will appear in Segment 3 of the\nTransaction). \u00a0When the Transaction is committed, each of the Transaction's\nSegments is automatically appended to the corresponding Segment in the real\nStream. \u00a0If the Stream is aborted, the Transaction, all its Segments and all the\nEvents published into the Transaction are removed from Pravega.\n\n\n\u00a0\n\n\nEvents published into a Transaction are never visible to the Reader until that\nTransaction is committed.\n\n\nFor more details on working with Transactions, see\u00a0\nWorking with Pravega:\nTransactions\n.\n\n\nState Synchronizers\n\n\nPravega is a streaming storage primitive; it can also be thought of as a\nmechanism to coordinate processes in a distributed computing environment. \u00a0The\nState Synchronizer feature of Pravega falls into the latter category.\n\n\nA State Synchronizer uses a Pravega Stream\u00a0to provide a synchronization\nmechanism for state shared between multiple processes running in a cluster,\nmaking it easier to build distributed applications. \u00a0With State Synchronizer, an\napp developer can use Pravega to read and make changes to shared state with\nconsistency and optimistic locking.\u00a0\n\n\n\u00a0\n\n\nState Synchronizer could be used to maintain a single, shared copy of an\napplication's configuration property across all instances of that\u00a0application\u00a0in\na cloud. \u00a0State Synchronizer could also be used to store one piece of data or a\nmap with thousands of different key value pairs. \u00a0In fact, Pravega itself uses\nState Synchronizer internally, to manage the state of ReaderGroups and Readers\ndistributed throughout the network.\n\n\nAn app developer creates a State Synchronizer on a Stream in a fashion similar\nto how s/he creates a Writer. \u00a0The State Synchronizer keeps a local copy\nof the shared state to make access to the data really fast for the application.\n\u00a0Any changes to shared state are written through the StateSynchronizer to the\nStream keeping\u00a0track\u00a0of all changes to the shared state.\n\u00a0Each\u00a0application\u00a0instance uses the State Synchronizer to stay up to date with\nchanges by pulling updates to shared state and modifying the local copy of the\ndata. \u00a0Consistency is maintained through a conditional append style of updates\nto the shared state through the State Synchronizer, making sure that updates are\nmade only to the most recent version of the shared state.\n\n\nThe State Synchronizer can occasionally be \"compacted\", compressing and removing\nold state updates so that only the most recent version of the state is kept in\nthe backing stream. \u00a0This feature helps app developers make sure that shared\nstate does not grow unchecked.\n\n\nState Synchronizer works best when most updates to shared state are small in\ncomparison to the total data size being stored, allowing them to be written as\nsmall deltas. As with any optimistic concurrency system, State Synchronizer is\nnot at its best when many processes are all attempting to simultaneously update\nthe same piece of data.\n\n\nFor more details on working with State Synchronizers, see\u00a0\nWorking with Pravega:\nState Synchronizer\n.\n\n\nArchitecture\n\n\nThe following figure depicts the components deployed by Pravega:\n\n\n\n\nPravega is deployed as a distributed system \u2013 a cluster of servers and storage\ncoordinated to run Pravega called a \"Pravega cluster\". \u00a0\n\n\nPravega itself is composed of two kinds of workload: Controller instances and\nPravega Servers. The set of Pravega Servers is known collectively as the Segment Store.\u00a0\n\n\nThe set of Controller instances make up the control plane of Pravega, providing\nfunctionality to create, update and delete Streams, retrieve\u00a0information about\nStreams,\u00a0monitor the health of the Pravega cluster, gather metrics etc. \u00a0There\nare usually multiple (recommended at least 3) Controller instances running in a\ncluster for high availability. \u00a0\n\n\nThe \nSegment Store\n implements the Pravega data plane.\n\u00a0Pravega Servers provide the API to read and write data in Streams. \u00a0Data\nstorage is comprised of two tiers: Tier 1 Storage,\u00a0which provides short term,\nlow-latency, data storage, \u00a0guaranteeing the durability of data written to\nStreams and Tier 2 Storage providing longer term storage of Stream data.\n\u00a0Pravega uses\u00a0\nApache Bookkeeper\n\u00a0to implement\nTier 1 Storage and uses HDFS, Dell EMC's Isilon or Dell EMC's Elastic Cloud\nStorage (ECS) to implement Tier 2 Storage. \u00a0Tier 1 Storage typically runs within\nthe Pravega cluster. \u00a0Tier 2 Storage is normally deployed outside the Pravega\ncluster.\n\n\nTiering storage is important to deliver the combination of fast access to Stream\ndata but also allow Streams to store a vast amount of data. \u00a0Tier 1\u00a0storage\npersists\u00a0the most recently written Stream data. \u00a0As data in Tier 1 Storage\u00a0ages,\nit is moved\u00a0into\u00a0Tier 2 Storage.\n\n\nPravega uses \nApache Zookeeper\n as the\ncoordination mechanism for the components in the Pravega cluster. \u00a0\n\n\nPravega provides a client library, written in Java, for building client-side\napplications such as analytics applications using Flink.\u00a0The Pravega Java Client\nLibrary manages the interaction between application code and Pravega via a\ncustom TCP wire protocol.\n\n\nPutting the Concepts Together\n\n\nThe concepts in Pravega are summarized in the following figure:\n\n\n\u00a0\n\n\n\n\n\n\nPravega clients are Writers and Readers.\u00a0 Writers write Events into a\n    Stream.\u00a0 Readers read Events from a Stream.\u00a0 Readers are grouped into\n    ReaderGroups to read from a Stream in parallel.\n\n\n\n\n\n\nThe Controller is a server-side component that manages the control plane of\n    Pravega.\u00a0 Streams are created, updated and listed using the Controller API.\n\n\n\n\n\n\nThe Pravega Server is a server-side component that implements read, write\n    and other data plane operations.\n\n\n\n\n\n\nStreams are the fundamental storage primitive in Pravega. \u00a0Streams contain a\n    set of data elements called Events.\u00a0 Events are appended to the \u201ctail\u201d of\n    the Stream by Writers.\u00a0 Readers can read Events from anywhere in the Stream.\n\n\n\n\n\n\nA Stream is\u00a0partitioned into a set of Stream Segments. The number of Stream\n    Segments in a Stream can change over time. \u00a0Events are written into exactly\n    one of the Stream Segments based on Routing Key. \u00a0For any ReaderGroup\n    reading a Stream, each Stream Segment is assigned to one Reader in that\n    ReaderGroup.\u00a0\n\n\n\n\n\n\nEach Stream Segment is stored in a combination of Tier1 and Tier2 storage.\u00a0\n    The tail of the Segment is stored in Tier1 providing low latency reads and\n    writes.\u00a0 The rest of the Segment is stored in Tier2, providing high\n    throughput read access with horizontal scalibility and low cost.\u00a0\n\n\n\n\n\n\nA Note on Tiered Storage\n\n\nTo deliver an efficient implementation of Streams, Pravega is based on a tiered\nstorage model.\u00a0 Events are persisted in low latency/high IOPS storage\u00a0 (Tier 1\nStorage) and higher throughput storage (Tier 2 Storage). Writers and Readers are\noblivious to the tiered storage model from an API perspective.\u00a0\n\n\nPravega is based on an append-only Log data structure.\u00a0 As Leigh Stewart\n\nobserved\n,\nthere are really three data access mechanisms in a Log:\n\n\n\u00a0\n\n\nAll of the write activity, and much of the read activity happens at the tail of\nthe log.\u00a0 Writes are appended to the log and many clients want to read data as\nfast as it arrives in the log.\u00a0 These two data access mechanisms are dominated\nby the need for low-latency \u2013 low latency writes by Writers and near real time\naccess to the published data by Readers.\u00a0\n\n\nNot all Readers read from the tail of the log; some Readers want to read\nstarting at some arbitrary position in the log.\u00a0 These reads are known as\n\ncatch-up reads\n.\u00a0 Access to historical data traditionally was done by batch\nanalytics jobs, often using HDFS and Map/Reduce.\u00a0 However with new streaming\napplications, you can access historical data as well as current data by just\naccessing the log. \u00a0One approach would be to store all the historical data in\nSSDs like we do with the tail data, but that can get very expensive and force\ncustomers to economize by deleting historical data. \u00a0Pravega offers a mechanism\nthat allows customers to use cost-effective, highly-scalable, high-throughput\nstorage for the historical part of the log, that way they won\u2019t have to decide\nwhen to delete historical data.\u00a0 Basically, if storage is cheap enough, why not\nkeep all of the history?\n\n\nTier 1 Storage is used to make writing to Streams fast and durable and to make\nsure reading from the tail of a Stream is as fast as possible.\u00a0 Tier 1 Storage\nis based on the open source Apache BookKeeper Project. Though not essential, we\npresume that the Tier 1 Storage will be typically implemented on faster SSDs or\neven non-volatile RAM.\n\n\nTier 2 Storage provides a highly-scalable, high-throughput cost-effective\nstorage. We expect this tier to be typically deployed on spinning disks. Pravega\nasynchronously migrates Events from Tier 1 to Tier 2 to reflect the different\naccess patterns to Stream data.\u00a0 Tier 2 Storage is based on an HDFS model.",
            "title": "Pravega Concepts"
        },
        {
            "location": "/pravega-concepts/#pravega-concepts",
            "text": "Pravega is an open source storage primitive implementing  Streams  for continuous and unbounded data.  This page is an overview of the key concepts in Pravega.\n\u00a0See\u00a0 Terminology  for a concise definition for many\nPravega concepts.",
            "title": "Pravega Concepts"
        },
        {
            "location": "/pravega-concepts/#streams",
            "text": "Pravega organizes data into Streams. \u00a0A Stream is a durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency. \u00a0A Pravega Stream is\nsimilar to but more flexible than a \"topic\" in popular message-oriented middleware such as RabbitMQ  or Apache Kafka.  Pravega Streams are based on an append-only log data structure. By using\nappend-only logs, Pravega can rapidly ingest data into durable storage,\nand support a large variety of application use cases such as stream processing using frameworks like  Flink , publish/subscribe\nmessaging, NoSQL databases such as a Time Series\nDatabase (TSDB), workflow engines, event-oriented applications and many other\nkinds of applications.\u00a0  When a developer creates a Stream in Pravega, s/he gives the Stream a meaningful\nname such as \"IoTSensorData\" or \"WebApplicationLog20170330\". \u00a0The\nStream's name helps other developers understand the kind of data that is stored\nin the Stream. \u00a0It is also worth noting that Pravega Stream names are organized\nwithin a Scope. \u00a0A Scope is a string and should convey some sort of meaning to\ndevelopers such as \"FactoryMachines\" or \"HRWebsitelogs\". \u00a0A Scope acts as a\nnamespace for Stream names\u00a0\u2013 all Stream names are unique within a Scope.\n\u00a0Therefore a Stream is uniquely identified by the combination of its Stream name\nand Scope. \u00a0Scope can be used to segregate names by tenant (in a multi tenant\nenvironment), by department in an organization, by geographic location or any\nother categorization the developer chooses.  A Stream is unbounded in size\u00a0\u2013 Pravega itself does not impose any limits on how\nmany Events can be in a Stream or how many total bytes are stored in a Stream.\n\u00a0Pravega is built as a data storage primitive first and foremost. \u00a0Pravega is\ncarefully designed to take advantage of software defined storage so that the\namount of data stored in Pravega is limited only by the total storage capacity\nof your data center. \u00a0And like you would expect from a storage primitive, once\ndata is written to Pravega it is durably stored. \u00a0Short of a disaster that\npermanently destroys a large portion of a data center, data stored in Pravega is never\nlost.  To deal with a potentially large amount of data within a Stream, Pravega Streams\nare divided into Stream Segments. \u00a0A Stream Segment is a shard, or partition of\nthe data within a Stream. \u00a0We go into a lot more detail on Stream Segments a bit\nlater in this document. \u00a0Stream Segments are an important concept, but we need\nto introduce a few more things before we can dive into Stream Segments.  Applications, such as a Java program reading from an IoT sensor, write data to\nthe tail (front) of the Stream. \u00a0Applications, such as a  Flink  analytics job,\ncan read from any point in the Stream. Lots of applications can read and write\nthe same Stream in parallel. \u00a0Elastic, scalable support for a large volume of\nStreams, data and applications is at the heart of Pravega's design. \u00a0We will\nget into\u00a0more\u00a0details about how applications read and write Streams a bit later\nin this document when we detail Readers and Writers.",
            "title": "Streams"
        },
        {
            "location": "/pravega-concepts/#events",
            "text": "Pravega's client API allows applications to read and write data in Pravega in terms of an Event. \u00a0An Event is a\nset of bytes within a Stream. \u00a0An Event could be as simple as a small\nnumber of bytes containing a temperature reading from an IoT sensor composed of\na timestamp, a metric identifier and a value. \u00a0An Event could be web log data\nassociated with a user click on a website. \u00a0Events can be anything you can\nrepresent as a set of bytes. \u00a0Applications make sense of Events using\nstandard Java serializers and deserializers, allowing them to read and write\nobjects in Pravega using similar techniques to reading and writing objects from\nfiles.  Every Event has a Routing Key. \u00a0A Routing Key allows Pravega\nand application developers to reason about which Events are related. \u00a0\u00a0A Routing\nKey is\u00a0just\u00a0a string that developers use to group similar Events together.\u00a0A\nRouting Key is often derived from data naturally occurring in the Event,\nsomething like \"customer-id\" or \"machine-id\", but it could also be some\nartificial String. A\nRouting Key could be something\u00a0like a\u00a0date (to group Events together by\ntime) or perhaps a Routing Key could be a IoT sensor id (to group Events by\nmachine). \u00a0A Routing Key is important to defining the precise read and write\nsemantics that Pravega guarantees, we will get into that detail a bit later,\nafter we have introduced a few more key concepts.",
            "title": "Events"
        },
        {
            "location": "/pravega-concepts/#writers-readers-readergroups",
            "text": "Pravega provides a client library, written in Java, that implements a convenient\nAPI for Writer and Reader applications to use.\u00a0 The Pravega Java Client Library\nencapsulates the wire protocol used to communicate between Pravega clients and\nPravega.  A Writer is an application that creates Events and writes them into a Stream.\nAll data is written by appending to the tail (front) of a Stream.  A Reader is an application that reads Events from a Stream. \u00a0Readers can read\nfrom any point in the Stream.\u00a0 Many Readers will be reading Events from the tail\nof the Stream.\u00a0 These Events will be delivered to Readers as quickly as possible.\u00a0 Some Readers will read\nfrom earlier parts of the Stream (called catch-up reads).\u00a0 The application\ndeveloper has control over where in the Stream the Reader starts reading.\n\u00a0Pravega has the concept of a Position, that represents where in a Stream a\nReader is currently located. \u00a0The Position object can be used as a recovery\nmechanism\u00a0\u2013 applications that persist the last Position a Reader has\nsuccessfully processed can use that information to initialize a replacement\nReader to pickup where a failed Reader left off. \u00a0Using this pattern of\npersisting Position objects, applications can be built that guarantee exactly\nonce Event processing in the face of Reader failure.  Readers are organized into ReaderGroups. \u00a0A ReaderGroup is a named collection of\nReaders that together, in parallel, read Events from a given Stream.\u00a0When a\nReader is created through the Pravega data plane API, the developer includes the\nname of the ReaderGroup it is part of. \u00a0We guarantee that each Event published\nto a Stream is sent to exactly one Reader within the ReaderGroup. \u00a0There could\nbe 1 Reader in the ReaderGroup, there could be many. \u00a0There could be many\ndifferent ReaderGroups simultaneously reading from any given Stream.  You can think of a ReaderGroup as a \"composite Reader\" or a \"distributed\nReader\", that allows a distributed application to read and process Stream data\nin parallel, such that a massive amount of Stream data can be consumed by a\ncoordinated fleet of Readers in a ReaderGroup. \u00a0A collection of Flink tasks\nprocessing Stream data in parallel is a good example use of a ReaderGroup.  For more details on the basics of working with Pravega Readers and Writers,\nsee\u00a0 Working with Pravega: Basic Reader and\nWriter .  We need to talk in more detail about the relationship between Readers,\nReaderGroups and Streams and the ordering guarantees provided by Pravega. \u00a0But\nfirst, we need to describe what a Stream Segment is.",
            "title": "Writers, Readers, ReaderGroups"
        },
        {
            "location": "/pravega-concepts/#stream-segments",
            "text": "A Stream is decomposed into a set of Stream Segments; a Stream Segment is a\nshard or partition of a Stream.",
            "title": "Stream Segments"
        },
        {
            "location": "/pravega-concepts/#an-event-is-stored-within-a-stream-segment",
            "text": "The Stream Segment is the container for Events within the Stream. \u00a0 When an\nEvent is written into a Stream, it is stored in one of the Stream Segments based\non the Event's Routing Key. \u00a0Pravega uses consistent hashing to assign Events to\nStream Segments.\u00a0Event Routing Keys are hashed to form a \"key space\" . \u00a0The key\nspace is then divided into a number of partitions, corresponding to the number\nof Stream Segments. Consistent hashing determines which Segment an Event is\nassigned to.",
            "title": "An Event is Stored within a Stream Segment"
        },
        {
            "location": "/pravega-concepts/#autoscaling-the-number-of-stream-segments-can-vary-over-time",
            "text": "The number of Stream Segments in a Stream\u00a0can grow\u00a0 and shrink \u00a0over time as I/O\nload on the Stream increases and decreases. \u00a0 We refer to this feature as\nAutoScaling.  Consider the following figure that shows the relationship between Routing Keys\nand time.  \u00a0  A Stream starts out at time t0 with a configurable number of Segments.\u00a0 If the\nrate of data written to the Stream is constant, the number of Segments won\u2019t\nchange.\u00a0 However at time t1, the system noted an increase in the ingestion rate\nand chose to split Segment 1 into two parts. \u00a0We call this a Scale-up event.\n\u00a0Before t1, Events with a Routing Key that hashes to the upper part of the key\nspace (values 200-399) would be placed in Segment 1 and those that hash into the\nlower part of the key space (values 0-199) would be placed in Segment 0. After\nt1, Segment 1 is split into Segment 2 and Segment 3. \u00a0Segment 1 is sealed and it\nno longer accepts writes. \u00a0At this point in time, Events with Routing Key\n300\u00a0and\u00a0above are written to Segment 3 and those between 200 and 299 would be\nwritten into Segment 2. \u00a0Segment 0 still keeps accepting\u00a0the\u00a0same range of\nEvents as before t1. \u00a0  We also see another Scale-up event at time t2, as Segment 0\u2019s range of Routing\nKey is split into Segment 5 & Segment 4. \u00a0Also at this time, Segment 0 is sealed\noff so that it accepts no further writes.  Segments covering a contiguous range of the key space can also be merged.\u00a0 At\ntime t3, Segment 2\u2019s range and Segment 5\u2019s range are merged into Segment 6 to\naccommodate a decrease in load on the Stream.  When a Stream is created, it is configured with an Scaling Policy that\ndetermines how a Stream reacts to changes in its load. \u00a0Currently there are\nthree kinds of Scaling Policy:    Fixed. \u00a0The number of Stream Segments does not vary with load    Size-based. \u00a0As the number of bytes of data per second written to the Stream\n    increases past a certain target rate, the number of Stream Segments is\n    increased. \u00a0If it falls below a certain level, decrease the number of Stream\n    Segments.    Event-based. \u00a0Similar to Size-based Scaling Policy, except it uses the\n    number of Events instead of the number of bytes.",
            "title": "AutoScaling:\u00a0The\u00a0number of Stream Segments can vary over time"
        },
        {
            "location": "/pravega-concepts/#events-stream-segments-and-autoscaling",
            "text": "We mentioned previously that an Event is written into one of the Stream's\nSegments. \u00a0Taking into account AutoScaling, you should think of Stream Segments\nas a bucketing of Events based on Routing Key and time. \u00a0At any given time,\nEvents published to a Stream within a given value of Routing Key will all appear\nin the same Stream Segment.  \u00a0  It is also worth emphasizing that Events are written into only the active Stream\nSegments. \u00a0Segments that are sealed do not accept writes. \u00a0In the figure above,\nat time \"now\", only Stream Segments 3, 6 and 4 are active and between those\nthree Stream Segments the entire key space is covered.",
            "title": "Events, Stream Segments and AutoScaling"
        },
        {
            "location": "/pravega-concepts/#stream-segments-and-readergroups",
            "text": "Stream Segments are important to understanding the way Reader Groups work.  \u00a0  Pravega assigns each Reader in a ReaderGroup zero or more Stream Segments to\nread from. \u00a0Pravega tries to balance out the number of Stream Segments each\nReader is assigned. \u00a0In the figure above, Reader B1 reads from 2 Stream Segments\nwhile each of the other Readers in the Reader Group have only 1 Stream Segment\nto read from. \u00a0Pravega makes sure that each Stream Segment is read by exactly\none Reader in any ReaderGroup configured to read from that Stream. As Readers\nare added to the ReaderGroup, or Readers crash and are removed from the\nReaderGroup, Pravega reassigns Stream Segments so that Stream Segments are\nbalanced amongst the Readers.  The number of Stream Segments in a Stream determines the upper bound of\nparallelism of readers within a ReaderGroup \u2013\u00a0the more Stream Segments, the more\nseparate, parallel sets of Readers we can have consuming the Stream.\u00a0In the\nabove figure, Stream1 has 4 Stream Segments. \u00a0That means that the largest\neffective Reader Group would contain 4 Readers. \u00a0Reader Group named \"B\" in the\nabove figure is not quite optimal. \u00a0If one more Reader was added to the\nReaderGroup, each Reader would have 1 Stream Segment to process, maximizing read\nparallelism. \u00a0However, the number of Readers in the ReaderGroup increases beyond\n4, at least one of the Readers will not be assigned a Stream Segment.  If Stream1 in the figure above experienced a Scale-Down event, reducing the\nnumber of Stream Segments to 3, then Reader Group B as depicted would have an\nideal number of Readers.  With the AutoScaling feature, Pravega developers don't have to configure their\nStreams with a fixed, pre-determined number of Stream Segments\u00a0\u2013 Pravega can\ndynamically determine the right number. \u00a0With this feature, Pravega Streams can\ngrow and shrink to match the behavior of the data input. \u00a0The size of any Stream\nis limited only by the total storage capacity made available to the Pravega\ncluster; if you need bigger streams, simply add more storage to your cluster.  Applications can react to changes\nin the number of Segments in a Stream, adjusting the number of Readers within a\nReaderGroup, to maintain optimal read parallelism if resources allow. \u00a0This is\nuseful, for example in a Flink application, to allow Flink to increase or\ndecrease the number of task instances that are processing a Stream in parallel,\nas scale events occur over time.",
            "title": "Stream Segments and ReaderGroups"
        },
        {
            "location": "/pravega-concepts/#ordering-guarantees",
            "text": "A stream comprises a set of segments that can change over time. Segments that overlap in their area of keyspace have a defined order.  An event written to a stream is written to a single segment and it is totally ordered with respect to the events of that segment. The existance and position of an event within a segment is strongly consistent.  Readers can be assigned multiple parallel segments (from different parts of keyspace). A reader reading from multiple segments will interleave the events of the segments, but the order of events per segment respects the one of the segment. Specifically, if s is a segment, events e~1 and e~2 of s are such that e~1 precedes e~2, and a reader reads both e~1 and e~2, then the reader will read e~1 before e~2.  This results in the following ordering guarantees:    Events with the same Routing Key are consumed in the order they were written.    Events with different Routing Keys sent to a specific segment will always be\n    seen in the same order even if the Reader backs up and re-reads them.    If an event has been acked to its writer or has been read by a reader it is guaranteed that it will continue to exist in the same place for all subsequent reads until it is deleted.    If there are multiple Readers reading a Stream and they all back up to any given point, they will never see any reordering with respect to that point. (It will never be the case that an event that they read before the chosen point now comes after or vice versa.)",
            "title": "Ordering Guarantees"
        },
        {
            "location": "/pravega-concepts/#readergroup-checkpoints",
            "text": "Pravega provides the ability for an application to initiate a Checkpoint on a\nReaderGroup. \u00a0The idea with a Checkpoint is to create a consistent \"point in\ntime\" persistence of the state of each Reader in the ReaderGroup, by using a\nspecialized Event (a Checkpoint Event) to signal each Reader to preserve its\nstate. \u00a0Once a Checkpoint has been completed, the application can use the\nCheckpoint to reset all the Readers in the ReaderGroup to the known consistent\nstate represented by the Checkpoint.  For more details on working with ReaderGroups, see\u00a0 Working with Pravega: Reader\nGroups .",
            "title": "ReaderGroup Checkpoints"
        },
        {
            "location": "/pravega-concepts/#transactions",
            "text": "Pravega supports Transactions. \u00a0The idea of a Transaction is that a Writer can\n\"batch\" up a bunch of Events and commit them as a unit into a Stream. \u00a0This is\nuseful, for example, with Flink jobs, using Pravega as a sink. \u00a0The Flink job\ncan continuously produce results of some data processing and use the Transaction\nto durably accumulate the results of the processing. \u00a0At the end of some sort of\ntime window (for example) the Flink job can commit the Transaction and therefore\nmake the results of the processing available for downstream processing, or in\nthe case of an error, abort the Transaction and the results disappear.  A key difference between Pravega's Transactions and similar approaches (such as\nKafka's producer-side batching) is related to durability. \u00a0Events added to a\nTransaction are durable when the Event is ack'd back to the Writer. \u00a0However,\nthe Events in the Transaction are NOT visible to readers until the Transaction\nis committed by the Writer. \u00a0A Transaction is a lot like a Stream; a Transaction\nis associated with multiple Stream Segments. \u00a0When an Event is published into a\nTransaction, the Event itself is appended to a Stream Segment of the\nTransaction. \u00a0Say a Stream had 5 Segments, when a Transaction is created on that\nStream, conceptually that Transaction also has 5 Segments. \u00a0When an Event is\npublished into the Transaction, it is routed to the same numbered Segment as if\nit were published to the Stream itself (if the Event would have been placed in\nSegment 3 in the \"real\" Stream, then it will appear in Segment 3 of the\nTransaction). \u00a0When the Transaction is committed, each of the Transaction's\nSegments is automatically appended to the corresponding Segment in the real\nStream. \u00a0If the Stream is aborted, the Transaction, all its Segments and all the\nEvents published into the Transaction are removed from Pravega.  \u00a0  Events published into a Transaction are never visible to the Reader until that\nTransaction is committed.  For more details on working with Transactions, see\u00a0 Working with Pravega:\nTransactions .",
            "title": "Transactions"
        },
        {
            "location": "/pravega-concepts/#state-synchronizers",
            "text": "Pravega is a streaming storage primitive; it can also be thought of as a\nmechanism to coordinate processes in a distributed computing environment. \u00a0The\nState Synchronizer feature of Pravega falls into the latter category.  A State Synchronizer uses a Pravega Stream\u00a0to provide a synchronization\nmechanism for state shared between multiple processes running in a cluster,\nmaking it easier to build distributed applications. \u00a0With State Synchronizer, an\napp developer can use Pravega to read and make changes to shared state with\nconsistency and optimistic locking.\u00a0  \u00a0  State Synchronizer could be used to maintain a single, shared copy of an\napplication's configuration property across all instances of that\u00a0application\u00a0in\na cloud. \u00a0State Synchronizer could also be used to store one piece of data or a\nmap with thousands of different key value pairs. \u00a0In fact, Pravega itself uses\nState Synchronizer internally, to manage the state of ReaderGroups and Readers\ndistributed throughout the network.  An app developer creates a State Synchronizer on a Stream in a fashion similar\nto how s/he creates a Writer. \u00a0The State Synchronizer keeps a local copy\nof the shared state to make access to the data really fast for the application.\n\u00a0Any changes to shared state are written through the StateSynchronizer to the\nStream keeping\u00a0track\u00a0of all changes to the shared state.\n\u00a0Each\u00a0application\u00a0instance uses the State Synchronizer to stay up to date with\nchanges by pulling updates to shared state and modifying the local copy of the\ndata. \u00a0Consistency is maintained through a conditional append style of updates\nto the shared state through the State Synchronizer, making sure that updates are\nmade only to the most recent version of the shared state.  The State Synchronizer can occasionally be \"compacted\", compressing and removing\nold state updates so that only the most recent version of the state is kept in\nthe backing stream. \u00a0This feature helps app developers make sure that shared\nstate does not grow unchecked.  State Synchronizer works best when most updates to shared state are small in\ncomparison to the total data size being stored, allowing them to be written as\nsmall deltas. As with any optimistic concurrency system, State Synchronizer is\nnot at its best when many processes are all attempting to simultaneously update\nthe same piece of data.  For more details on working with State Synchronizers, see\u00a0 Working with Pravega:\nState Synchronizer .",
            "title": "State Synchronizers"
        },
        {
            "location": "/pravega-concepts/#architecture",
            "text": "The following figure depicts the components deployed by Pravega:   Pravega is deployed as a distributed system \u2013 a cluster of servers and storage\ncoordinated to run Pravega called a \"Pravega cluster\". \u00a0  Pravega itself is composed of two kinds of workload: Controller instances and\nPravega Servers. The set of Pravega Servers is known collectively as the Segment Store.\u00a0  The set of Controller instances make up the control plane of Pravega, providing\nfunctionality to create, update and delete Streams, retrieve\u00a0information about\nStreams,\u00a0monitor the health of the Pravega cluster, gather metrics etc. \u00a0There\nare usually multiple (recommended at least 3) Controller instances running in a\ncluster for high availability. \u00a0  The  Segment Store  implements the Pravega data plane.\n\u00a0Pravega Servers provide the API to read and write data in Streams. \u00a0Data\nstorage is comprised of two tiers: Tier 1 Storage,\u00a0which provides short term,\nlow-latency, data storage, \u00a0guaranteeing the durability of data written to\nStreams and Tier 2 Storage providing longer term storage of Stream data.\n\u00a0Pravega uses\u00a0 Apache Bookkeeper \u00a0to implement\nTier 1 Storage and uses HDFS, Dell EMC's Isilon or Dell EMC's Elastic Cloud\nStorage (ECS) to implement Tier 2 Storage. \u00a0Tier 1 Storage typically runs within\nthe Pravega cluster. \u00a0Tier 2 Storage is normally deployed outside the Pravega\ncluster.  Tiering storage is important to deliver the combination of fast access to Stream\ndata but also allow Streams to store a vast amount of data. \u00a0Tier 1\u00a0storage\npersists\u00a0the most recently written Stream data. \u00a0As data in Tier 1 Storage\u00a0ages,\nit is moved\u00a0into\u00a0Tier 2 Storage.  Pravega uses  Apache Zookeeper  as the\ncoordination mechanism for the components in the Pravega cluster. \u00a0  Pravega provides a client library, written in Java, for building client-side\napplications such as analytics applications using Flink.\u00a0The Pravega Java Client\nLibrary manages the interaction between application code and Pravega via a\ncustom TCP wire protocol.",
            "title": "Architecture"
        },
        {
            "location": "/pravega-concepts/#putting-the-concepts-together",
            "text": "The concepts in Pravega are summarized in the following figure:  \u00a0    Pravega clients are Writers and Readers.\u00a0 Writers write Events into a\n    Stream.\u00a0 Readers read Events from a Stream.\u00a0 Readers are grouped into\n    ReaderGroups to read from a Stream in parallel.    The Controller is a server-side component that manages the control plane of\n    Pravega.\u00a0 Streams are created, updated and listed using the Controller API.    The Pravega Server is a server-side component that implements read, write\n    and other data plane operations.    Streams are the fundamental storage primitive in Pravega. \u00a0Streams contain a\n    set of data elements called Events.\u00a0 Events are appended to the \u201ctail\u201d of\n    the Stream by Writers.\u00a0 Readers can read Events from anywhere in the Stream.    A Stream is\u00a0partitioned into a set of Stream Segments. The number of Stream\n    Segments in a Stream can change over time. \u00a0Events are written into exactly\n    one of the Stream Segments based on Routing Key. \u00a0For any ReaderGroup\n    reading a Stream, each Stream Segment is assigned to one Reader in that\n    ReaderGroup.\u00a0    Each Stream Segment is stored in a combination of Tier1 and Tier2 storage.\u00a0\n    The tail of the Segment is stored in Tier1 providing low latency reads and\n    writes.\u00a0 The rest of the Segment is stored in Tier2, providing high\n    throughput read access with horizontal scalibility and low cost.",
            "title": "Putting the Concepts Together"
        },
        {
            "location": "/pravega-concepts/#a-note-on-tiered-storage",
            "text": "To deliver an efficient implementation of Streams, Pravega is based on a tiered\nstorage model.\u00a0 Events are persisted in low latency/high IOPS storage\u00a0 (Tier 1\nStorage) and higher throughput storage (Tier 2 Storage). Writers and Readers are\noblivious to the tiered storage model from an API perspective.\u00a0  Pravega is based on an append-only Log data structure.\u00a0 As Leigh Stewart observed ,\nthere are really three data access mechanisms in a Log:  \u00a0  All of the write activity, and much of the read activity happens at the tail of\nthe log.\u00a0 Writes are appended to the log and many clients want to read data as\nfast as it arrives in the log.\u00a0 These two data access mechanisms are dominated\nby the need for low-latency \u2013 low latency writes by Writers and near real time\naccess to the published data by Readers.\u00a0  Not all Readers read from the tail of the log; some Readers want to read\nstarting at some arbitrary position in the log.\u00a0 These reads are known as catch-up reads .\u00a0 Access to historical data traditionally was done by batch\nanalytics jobs, often using HDFS and Map/Reduce.\u00a0 However with new streaming\napplications, you can access historical data as well as current data by just\naccessing the log. \u00a0One approach would be to store all the historical data in\nSSDs like we do with the tail data, but that can get very expensive and force\ncustomers to economize by deleting historical data. \u00a0Pravega offers a mechanism\nthat allows customers to use cost-effective, highly-scalable, high-throughput\nstorage for the historical part of the log, that way they won\u2019t have to decide\nwhen to delete historical data.\u00a0 Basically, if storage is cheap enough, why not\nkeep all of the history?  Tier 1 Storage is used to make writing to Streams fast and durable and to make\nsure reading from the tail of a Stream is as fast as possible.\u00a0 Tier 1 Storage\nis based on the open source Apache BookKeeper Project. Though not essential, we\npresume that the Tier 1 Storage will be typically implemented on faster SSDs or\neven non-volatile RAM.  Tier 2 Storage provides a highly-scalable, high-throughput cost-effective\nstorage. We expect this tier to be typically deployed on spinning disks. Pravega\nasynchronously migrates Events from Tier 1 to Tier 2 to reflect the different\naccess patterns to Stream data.\u00a0 Tier 2 Storage is based on an HDFS model.",
            "title": "A Note on Tiered Storage"
        },
        {
            "location": "/terminology/",
            "text": "Terminology\n\n\nHere is a glossary of terms related to Pravega:\n\n\n\n\n\n\n\n\nTerm\n\n\nDefinition\n\n\n\n\n\n\n\n\n\n\nPravega\n\n\nPravega is an open source storage primitive implementing \nStreams\n for continuous and unbounded data.\n\n\n\n\n\n\nStream\n\n\nA durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency.\n\n\n\n\n\n\n\n\nA \nStream\n is identified by a name and a Scope.\n\n\n\n\n\n\n\n\nA \nStream\n is comprised of one or more \nStream Segments.\n\n\n\n\n\n\nStream Segment\n\n\nA shard of a \nStream\n.\n\n\n\n\n\n\n\n\nThe number of \nStream Segments\n in a \nStream\n might vary over time according to load and \nScaling Policy\n.\n\n\n\n\n\n\n\n\nIn the absence of a \nScale Event\n, \nEvents\n written to a \nStream\n with the same \nRouting Key\n are stored in the same \nStream Segment\n and are totally ordered.\n\n\n\n\n\n\n\n\nWhen a \nScale Event\n occurs, the set of \nStream Segments\n of a \nStream\n changes and \nEvents\n written with a given \nRouting Key\n \nK\n before the \nScaling Event\n are stored in a different \nStream Segment\n compared to \nEvents\n written with the same \nRouting Key\n \nK\n after the event.\n\n\n\n\n\n\n\n\nIn conjunction with \nReader Groups\n, the number of \nStream Segments\n is the maximum amount of read parallelism of a \nStream\n.\n\n\n\n\n\n\nScope\n\n\nA namespace for \nStream\n names.\n\n\n\n\n\n\n\n\nA \nStream\n name must be unique within a \nScope\n.\n\n\n\n\n\n\nEvent\n\n\nA collection of bytes within a \nStream.\n\n\n\n\n\n\n\n\nAn \nEvent\n is associated with a \nRouting Key.\n\n\n\n\n\n\nRouting Key\n\n\nA property of an \nEvent\n used to route messages to \nReaders.\n\n\n\n\n\n\n\n\nTwo \nEvents\n with the same \nRouting Key\n will be read by \nReaders\n in exactly the same order they were written.\n\n\n\n\n\n\nReader\n\n\nA software application that reads data from one or more \nStreams\n.\n\n\n\n\n\n\nWriter\n\n\nA software application that writes data to one or more \nStreams.\n\n\n\n\n\n\nPravega Java Client Library\n\n\nA Java library that applications use to interface with \nPravega\n\n\n\n\n\n\nReader Group\n\n\nA named collection of one or more \nReaders\n that read from a \nStream\n in parallel.\n\n\n\n\n\n\n\n\nPravega\n assigns \nStream Segments\n to the \nReaders\n making sure that ll \nStream Segments\n are assigned to at least one \nReader\n and that hey are balanced across the \nReaders\n.\n\n\n\n\n\n\nPosition\n\n\nAn offset within a \nStream\n, representing a type of recovery point for a \nReader\n.\n\n\n\n\n\n\n\n\nIf a \nReader\n crashes, a \nPosition\n can be used to initialize the failed \nReader\n's replacement so that the replacement resumes processing the \nStream\n from where the failed \nReader\n* left off.\n\n\n\n\n\n\nTier 1 Storage\n\n\nShort term, low-latency, data storage that guarantees the durability of data written to \nStreams\n.\n\n\n\n\n\n\n\n\nThe current implementation of \nTier 1\n uses\u00a0\u00a0\nApache Bookkeeper\n.\n\n\n\n\n\n\n\n\nTier 1\n storage keeps the most recent appends to streams in \nPravega.\n\n\n\n\n\n\n\n\nAs data in \nTier 1\n ages, it is moved out of \nTier 1\n into \nTier 2.\n\n\n\n\n\n\nTier 2 Storage\n\n\nA portion of \nPravega\n storage based on cheap and deep persistent storage technology such as HDFS, DellEMC's Isilon or DellEMC's Elastic Cloud Storage.\n\n\n\n\n\n\nPravega Server\n\n\nA component of \nPravega\n that implements the \nPravega\n data plane API for operations such as reading from and writing to \nPravega Streams\n.\n\n\n\n\n\n\n\n\nThe data plane of \nPravega,\n also called the \nSegment Store,\n is composed of 1 or more \nPravega Server\n instances.\n\n\n\n\n\n\nSegment Store\n\n\nA collection of Pravega Servers that in aggregate form the data plane of a Pravega cluster.\n\n\n\n\n\n\nController\n\n\nA component of Pravega that implements the \nPravega\n control plane API for operations such as creating and retrieving information about \nStreams\n.\n\n\n\n\n\n\n\n\nThe control plane of \nPravega\n is composed of 1 or more \nController\n instances coordinated by Zookeeper.\n\n\n\n\n\n\nAuto Scaling\n\n\nA Pravega concept that allows the number of \nStream Segments\n in a \nStream\n to change over time, based on \nScaling Policy.\n\n\n\n\n\n\nScaling Policy\n\n\nA configuration item of a \nStream\n that determines how the number of \nStream Segments\n in the \nStream\n should change over time.\n\n\n\n\n\n\n\n\nThere are three kinds of \nScaling Policy\n, a \nStream\n has exactly one of these at any given time.\n\n\n\n\n\n\n\n\n- Fixed number of \nStream Segments\n\n\n\n\n\n\n\n\n- Change the number of \nStream Segments\n based on the number of bytes per second written to the \nStream\n\n\n\n\n\n\n\n\n- Change the number of \nStream Segments\n based on the number of \nEvents\n er second written to the \nStream\n\n\n\n\n\n\nScale Event\n\n\nThere are two types of Scale Event: Scale-Up Event and Scale-Down Event. A \nScale Event\n triggers \nAuto Scaling\n.\n\n\n\n\n\n\n\n\nA Scale-Up Event is a situation where an increase in load causes one or more \nStream Segments\n to be split, increasing the number of \nStream Segments\n in the \nStream\n.\n\n\n\n\n\n\n\n\nA Scale-Down Event is a situation where a decrease in load causes one or more \nStream Segments\n to be merged, reducing the number of \nStream Segments\n in the \nStream\n.\n\n\n\n\n\n\nTransaction\n\n\nA collection of \nStream\n write operations that are applied atomically to the \nStream\n.\n\n\n\n\n\n\n\n\nEither all of the bytes in a \nTransaction\n are written to the \nStream\n or none of them are.\n\n\n\n\n\n\nState Synchronizer\n\n\nAn abstraction built on top of Pravega to enable the implementation of replicated state using a Pravega segment to back up the state transformations.\n\n\n\n\n\n\n\n\nA \nState Synchronizer\n allows a piece of data to be shared between multiple processes with strong consistency and optimistic concurrency.\n\n\n\n\n\n\nCheckpoint\n\n\nA kind of \nEvent\n that signals all \nReaders\n within a \nReader Group\n to persist their state.",
            "title": "Terminology"
        },
        {
            "location": "/terminology/#terminology",
            "text": "Here is a glossary of terms related to Pravega:     Term  Definition      Pravega  Pravega is an open source storage primitive implementing  Streams  for continuous and unbounded data.    Stream  A durable, elastic, append-only, unbounded sequence of bytes that has good performance and strong consistency.     A  Stream  is identified by a name and a Scope.     A  Stream  is comprised of one or more  Stream Segments.    Stream Segment  A shard of a  Stream .     The number of  Stream Segments  in a  Stream  might vary over time according to load and  Scaling Policy .     In the absence of a  Scale Event ,  Events  written to a  Stream  with the same  Routing Key  are stored in the same  Stream Segment  and are totally ordered.     When a  Scale Event  occurs, the set of  Stream Segments  of a  Stream  changes and  Events  written with a given  Routing Key   K  before the  Scaling Event  are stored in a different  Stream Segment  compared to  Events  written with the same  Routing Key   K  after the event.     In conjunction with  Reader Groups , the number of  Stream Segments  is the maximum amount of read parallelism of a  Stream .    Scope  A namespace for  Stream  names.     A  Stream  name must be unique within a  Scope .    Event  A collection of bytes within a  Stream.     An  Event  is associated with a  Routing Key.    Routing Key  A property of an  Event  used to route messages to  Readers.     Two  Events  with the same  Routing Key  will be read by  Readers  in exactly the same order they were written.    Reader  A software application that reads data from one or more  Streams .    Writer  A software application that writes data to one or more  Streams.    Pravega Java Client Library  A Java library that applications use to interface with  Pravega    Reader Group  A named collection of one or more  Readers  that read from a  Stream  in parallel.     Pravega  assigns  Stream Segments  to the  Readers  making sure that ll  Stream Segments  are assigned to at least one  Reader  and that hey are balanced across the  Readers .    Position  An offset within a  Stream , representing a type of recovery point for a  Reader .     If a  Reader  crashes, a  Position  can be used to initialize the failed  Reader 's replacement so that the replacement resumes processing the  Stream  from where the failed  Reader * left off.    Tier 1 Storage  Short term, low-latency, data storage that guarantees the durability of data written to  Streams .     The current implementation of  Tier 1  uses\u00a0\u00a0 Apache Bookkeeper .     Tier 1  storage keeps the most recent appends to streams in  Pravega.     As data in  Tier 1  ages, it is moved out of  Tier 1  into  Tier 2.    Tier 2 Storage  A portion of  Pravega  storage based on cheap and deep persistent storage technology such as HDFS, DellEMC's Isilon or DellEMC's Elastic Cloud Storage.    Pravega Server  A component of  Pravega  that implements the  Pravega  data plane API for operations such as reading from and writing to  Pravega Streams .     The data plane of  Pravega,  also called the  Segment Store,  is composed of 1 or more  Pravega Server  instances.    Segment Store  A collection of Pravega Servers that in aggregate form the data plane of a Pravega cluster.    Controller  A component of Pravega that implements the  Pravega  control plane API for operations such as creating and retrieving information about  Streams .     The control plane of  Pravega  is composed of 1 or more  Controller  instances coordinated by Zookeeper.    Auto Scaling  A Pravega concept that allows the number of  Stream Segments  in a  Stream  to change over time, based on  Scaling Policy.    Scaling Policy  A configuration item of a  Stream  that determines how the number of  Stream Segments  in the  Stream  should change over time.     There are three kinds of  Scaling Policy , a  Stream  has exactly one of these at any given time.     - Fixed number of  Stream Segments     - Change the number of  Stream Segments  based on the number of bytes per second written to the  Stream     - Change the number of  Stream Segments  based on the number of  Events  er second written to the  Stream    Scale Event  There are two types of Scale Event: Scale-Up Event and Scale-Down Event. A  Scale Event  triggers  Auto Scaling .     A Scale-Up Event is a situation where an increase in load causes one or more  Stream Segments  to be split, increasing the number of  Stream Segments  in the  Stream .     A Scale-Down Event is a situation where a decrease in load causes one or more  Stream Segments  to be merged, reducing the number of  Stream Segments  in the  Stream .    Transaction  A collection of  Stream  write operations that are applied atomically to the  Stream .     Either all of the bytes in a  Transaction  are written to the  Stream  or none of them are.    State Synchronizer  An abstraction built on top of Pravega to enable the implementation of replicated state using a Pravega segment to back up the state transformations.     A  State Synchronizer  allows a piece of data to be shared between multiple processes with strong consistency and optimistic concurrency.    Checkpoint  A kind of  Event  that signals all  Readers  within a  Reader Group  to persist their state.",
            "title": "Terminology"
        },
        {
            "location": "/key-features/",
            "text": "Pravega Key Features\n\n\nThis document explains some of the key features of Pravega. \u00a0It may be\nadvantageous if you are already familiar with the core \nconcepts of\nPravega\n.\n\n\nPravega Design Principles\n\n\nPravega was designed to support the new generation of streaming applications:\napplications that deal with a large amount of data arriving continuously that\nalso need to generate an accurate analysis of that data in the face of late\narriving data, data arriving out of order and failure conditions. There are\nseveral open source tools to enable developers to build such applications,\nincluding Apache Flink, Apache Beam, Spark Streaming and others.\nTo date, these applications used systems such as Apache Kafka, Apache ActiveMQ,\nRabbitMQ, Apache Cassandra, and Apache HDFS to ingest and store data. We envision\ninstead a unification of the two concepts and our work focuses on both ingesting\nand storing stream data.\n\n\nPravega approaches streaming applications from a storage perspective. It enables\napplications to ingest stream data continuously and storing it permanently. Such\nstream data can be accessed with low latency (order of milliseconds), but also\nmonths, years ahead as part of analyzing historical data.\n\n\nThe design of Pravega incorporates lessons learned from using the Lambda architecture\nto build streaming applications and the challenges to deploy streaming applications\nat scale that consistently deliver accurate results in a fault tolerant manner.\nThe Pravega architecture provides strong durability and consistency guarantees,\ndelivering a rock solid foundation to build streaming applications upon.\n\n\nWith the Lambda architecture, the developer uses a complex combination of middleware\ntools that include batch style middleware mainly influenced by Hadoop and\ncontinuous processing tools like Storm, Samza, Kafka and others.\n\n\n\n\nIn this architecture, batch processing is used to deliver accurate, but potentially\nout of date analysis of data. The second path processes data as it is ingested, and\nin principle the results are innacurate, which justifies the first\nbatch path. With this approach, there are two copies of the application logic because\nthe programming models of the speed layer are different than those used in the\nbatch layer. \u00a0An implementation of the Lambda architecture can be difficult to maintain\nand manage in production.\u00a0This style of big data application design consequently has\nbeen losing traction. A different kind of architecture has been gaining traction recently\nthat does not rely on a batch processing data path. This architecture is called Kappa.\n\n\nThe Kappa architecture style is a reaction to the complexity of the Lambda architecture\nand relies on components that are designed for streaming, supporting stronger\nsemantics and delivering both fast and accurate data analysis. The Kappa\narchitecture is a simpler approach:\n\n\n\n\nThere is only one data path to execute, and one implementation of the application logic\nto maintain, not two. \u00a0With the right tools, built for the demands of processing\nstreaming data in a fast and accurate fashion, it becomes simpler to design\nand run applications in the space of IoT, connected cars, finance, risk management, online\nservices, etc. With the right tooling, it is possible to build such pipelines and serve\napplications that present high volume and demand low latency.\n\n\nApplications often require more than one stage of processing. Any practical system for stream\nanalytics must be able to accomodate the composition of stages in the form of data pipelines:\n\n\n\n\nWith data pipelines,\u00a0it is important to think of guarantees end-to-end rather than on a\nper componenent basis. For example, it is not sufficient that one stage guarantees exactly-once\nsemantics while at least one other does not make such a guarantee. Our goal in Pravega is enable\nthe design and implementation of data pipelines with strong guarantees end-to-end.\n\n\nPravega - Storage Reimagined for a Streaming World\n\n\nPravega introduces a new storage primitive, a stream, that matches\ncontinuous processing of unbounded data. \u00a0In Pravega, a stream is a named,\ndurable, append-only and unbounded sequence of bytes. \u00a0With this primitive, and\nthe key features discussed in this document, Pravega is an ideal component to\ncombine with stream processing engines such as Flink to build streaming\napplications. \u00a0Because of Pravega's key features, we imagine that it will be the\nfundamental storage primitive for a new generation of streaming-oriented\nmiddleware.\n\n\nLet's examine the key features of Pravega.\n\n\nExactly Once Semantics\n\n\nBy exactly once semantics we mean that Pravega ensures that data is not duplicated\nand no event is missed despite failures. Of course, this statement comes with a\nnumber of caveats, like any other system that promises exactly-once semantics, but\nlet's not dive into the gory details here. An important consideration is that\nexactly-once semantics is a natural part of Pravega and has been a goal and part\nof the design from day zero.\n\n\nTo achieve exactly once semantics, Pravega\u00a0Streams are durable, ordered,\nconsistent and transactional. \u00a0We discuss durable and transactional in separate\nsections below.\n\n\nBy ordering, we mean that data is observed by readers in the order it is written.\nIn Pravega, data is written along with an application-defined routing key. \u00a0\nPravega makes ordering guarantees in terms of routing keys. \u00a0Two pieces of data\nwith the same routing key will always be read by a Reader in the order they were\nwritten. Pravega's\u00a0ordering guarantees allow data reads to be replayed (e.g.\nwhen applications crash) and the results of replaying the reads will be the\nsame.\n\n\nBy consistency, we mean all Readers see the same ordered view of data for a\ngiven routing key, even in the face of failure. Systems that are \"mostly\nconsistent\" are not sufficient for building accurate data processing.\n\n\nSystems that provide \"at least once\" semantics might present duplication. In\nsuch systems, a data producer might write the same data twice in some scenarios.\nIn Pravega, writes are idempotent, rewrites done as a result of reconnection\ndon't result in data duplication. Note that we make no guarantee when the data\ncoming from the source already contains duplicates. Written data is opaque to\nPravega and it makes no attempt to remove existing duplicates.\n\n\nWe have not limited our focus to exactly-once semantics for writing, however.\nWe also provide, and are actively working on extending the features, that enable\nexactly-once end-to-end for a data pipeline. The strong consistency guarantees\nthat the Pravega store provides along with the semantics of a data analytics \nengine like Flink enables such end-to-end guarantees. \n\n\nAuto Scaling\n\n\nUnlike systems with static partitioning, Pravega can automatically scale\nindividual data streams to accommodate changes in data ingestion rate.\n\n\nImagine an IoT application with millions of devices feeding thousands of data\nstreams with information about those devices. \u00a0Imagine a pipeline\u00a0of Flink jobs\nthat process those streams to derive business value from all that raw IoT data:\npredicting device failures, optimizing service delivery\u00a0through those devices,\nor tailoring a customer's experience when interacting with those devices.\n\u00a0Building such an application at\u00a0scale is difficult without having the\ncomponents be able to scale automatically as the rate of data increases and\ndecreases.\n\n\nWith Pravega, it is easy to\u00a0elastically and independently scale data ingestion,\nstorage and processing \u2013 orchestrating the scaling of every component in a data\npipeline.\n\n\nPravega's support of auto scaling starts with the idea that Streams are\npartitioned into Stream Segments. \u00a0A Stream may have 1 or more Stream Segments; recall\nthat a Stream Segment is a partition of the Stream associated with\na range of routing keys.\n\nAny data written into the Stream is written to the Stream Segment associated with the\ndata's routing key. \u00a0Writers use application-meaningful routing keys like\ncustomer-id, timestamp, machine-id, etc to make sure like data is grouped\ntogether. \u00a0\n\n\nA Stream Segment is the fundamental unit of parallelism in Pravega Streams. \u00a0A\nStream with multiple Stream Segments can support more parallelism of data\nwrites; multiple Writers writing data into the different Stream Segments\npotentially involving all the Pravega Servers in the cluster. \u00a0On the Reader\nside, the number of Stream Segments represents the maximum degree of read\nparallelism possible. \u00a0If a Stream has N Stream Segments, then a ReaderGroup\nwith N Readers can consume from the Stream in parallel. \u00a0Increase the number of\nStream Segments, you can increase the number of Readers in the ReaderGroup to\nincrease the scale of processing the data from that Stream. \u00a0And of course if\nthe number of Stream Segments decreases, it would be a good idea to reduce the\nnumber of Readers.\n\n\nA Stream can be configured to grow the number of Stream Segments as more data is\nwritten to the Stream, and to shrink when data volume drops off. \u00a0We refer to\nthis configuration as the Stream's Service Level Objective or SLO. \u00a0Pravega\nmonitors the rate of data input to the Stream and uses the SLO to add or remove\nStream Segments from a Stream. \u00a0Segments are added by splitting a Segment.\n\u00a0Segments are removed by merging two Segments. \u00a0See\u00a0\nAutoScaling:\u00a0The\u00a0number of\nStream Segments can vary over\ntime\n,\nfor more detail on how\u00a0Pravega manages Stream Segments.\n\n\nIt is possible to coordinate the auto scaling of Streams in Pravega with\napplication scale out (in the works). \u00a0Using metadata available from Pravega,\napplications can configure\u00a0the\u00a0scaling of their application components; for\nexample, to drive the number of instances of a Flink job. Alternatively, you\ncould use software such as Cloud Foundry, Mesos/Marathon, Kubernetes or the\nDocker stack to deploy new instances of an application to react to increased\nparallelism at the Pravega level, or to terminate instances as Pravega scales\ndown in response to reduced rate of data ingestion.\n\n\nDistributed Computing Primitive\n\n\nPravega is great for distributed applications, such as micro-services; it can be\nused as a data storage mechanism, for messaging between micro-services and for\nother distributed computing services such as leader election.\u00a0\n\n\nState Synchronizer, a part of the Pravega API, is the basis of sharing state\nacross a cluster with consistency and optimistic concurrency. \u00a0State\nSynchronizer is based on a fundamental conditional write operation in Pravega,\nso that data is written only if it would appear at a given position in the\nStream. \u00a0If a conditional write operation cannot meet the condition, it fails.\n\n\nState Synchronizer is therefore a strong synchronization primitive that can be\nused for shared state in a cluster, membership management, leader election and\nother distributed computing scenarios.\n\n\nYou can learn more about the State Synchronizer\n\nhere\n.\n\n\nWrite Efficiency\n\n\nPravega write latency is of the order of milliseconds, and seamlessly scales\nto handle high throughput reads and writes from thousands of concurrent\nclients, making it ideal for IoT and other time sensitive applications.\n\n\nStreams are light weight, Pravega can support millions of Streams, this frees\nthe application from worrying about statically configuring streams and preallocating \na small fixed number of streams and husbanding or limiting stream resource.\n\n\nWrite operations in Pravega are low latency, under 10 ms to return an\nacknowledgment is returned to a Writer. Furthermore, writes are optimized so\nthat I/O throughput is limited by network bandwidth; \u00a0the persistence mechanism\nis not the bottleneck. \u00a0Pravega uses Apache BookKeeper to persist all write\noperations. \u00a0BookKeeper persists and protects the data very efficiently.\n\u00a0Because data is protected before the write operation is acknowledged to the\nWriter, data is always durable. \u00a0As we discuss below, data durability is a\nfundamental characteristic of a storage primitive, \u00a0To add further efficiency,\nwrites to BookKeeper often involve data from multiple Stream Segments, so the\ncost of persisting data to disk can be amortized over several write operations.\n\n\nThere is no durability-performance trade-off with Pravega. \n\n\nReads are efficient too. \u00a0A Reader can read from a Stream either at the tail of\nthe Stream or at any part of the Stream's history. \u00a0Unlike some log-based\nsystems that use the same kind of storage for tail reads and writes as well as\nreads to historical data, Pravega uses two types of storage. \u00a0The tail of the\nStream is in so-called Tier-1 storage. \u00a0Writes are implemented by Apache\nBookKeeper as mentioned above. \u00a0Tail reads are served out of a Pravega-managed\nmemory cache. \u00a0In fact, BookKeeper serves reads only in failure recovery\nscenarios, where a Pravega Server has\u00a0crashed\u00a0and it is being recovered. \u00a0This\nuse of BookKeeper is\u00a0exactly\u00a0what it was designed for: fast\nwrites,\u00a0occasional\u00a0reads. \u00a0 The historical part of the Stream is in so-called\nTier 2 storage that is optimized for low-cost storage with high-throughput.\n\u00a0Pravega uses efficient in-memory read ahead cache, taking advantage of the fact\nthat Streams are usually read in large contiguous chunks and that HDFS is well\nsuited for\u00a0those\u00a0sort of large, high-throughput reads. \u00a0It is also worth noting\nthat tail reads do not impact the performance of writes.\n\n\nUnlimited Retention\n\n\nData in Streams can be retained for as long as the application needs it, \nconstrained to the amount of data available, which is unbounded given the use\nof cloud storage in Tier 2. Pravega provides one convenient API to\naccess both real-time and historical data. \u00a0With Pravega, batch and real-time\napplications can both be handled efficiently; yet another reason why Pravega is\na great storage primitive for Kappa architectures.\n\n\nIf there is a value to keeping old data, why not keep it around? \u00a0For example,\nin a machine learning example, you may want to periodically change the model and\ntrain the new version of the model against as much historical data as possible\nto yield more accurate predictive power of the model. \u00a0With Pravega\nauto-tiering, keeping lots of historical data does not affect the performance of\ntail reads and writes.\u00a0\n\n\nSize of a stream is not limited by the storage capacity of a single server, but\nrather, it is limited only by the storage capacity of your storage cluster or cloud\nprovider. \u00a0As cost of storage decreases, the economic incentive to delete data\ngoes away\n\n\nStorage Efficiency\n\n\nUse Pravega to build pipelines of data processing, combining batch, real-time\nand other applications without duplicating data for every step of the pipeline.\n\n\nConsider the following data processing environment that combines real time\nprocessing using Spark, Flink, and or Storm; Haddoop for batch; some kind of\nLucene-based Search mechanism like Elastic Search for full text search; and\nmaybe one (or several) NoSQL databases to support micro-services apps.\n\n\n\n\nUsing traditional approaches, one set of source data, for example, sensor data\nfrom an IoT app, would be ingested and replicated separately by each system.\n\u00a0You would end up with 3 replicas of the data protected in the pub/sub system, 3\ncopies in HDFS, 3 copies in Lucene, 3 copies in the NoSQL database. \u00a0When we\nconsider the source data is measured in terabytes, the cost of data replication\nseparated by middleware category becomes prohibitively expensive.\n\n\nConsider the same pipeline using Pravega and middleware adapted to use Pravega\nfor its storage:\n\n\n\n\nWith Pravega, the data is ingested and protected in one place; Pravega provides\nthe single source of truth for the entire pipeline. \u00a0Furthermore, with the bulk\nof the data being stored in Tier-2 enabled with erasure coding to efficiently\nprotect the data, the storage cost of the data is substantially reduced.\n\n\nDurability\n\n\nWith Pravega, you don't \u00a0face a compromise between performance, durability and\nconsistency. Pravega\u00a0provides durable storage of streaming data with strong\nconsistency, ordering guarantees and great performance.\n\n\nDurability is a fundamental storage primitive requirement. \u00a0Storage that could\nlose data is not reliable storage. \u00a0Systems based on such storage are not\nproduction quality.\n\n\nOnce a write operation is acknowledged, the data will never be lost, even when\nfailures occur. \u00a0This is because Pravega always saves data in protected,\npersistent storage before the write operation returns to the Writer.\n\n\nWith Pravega, data in the Stream is protected. \u00a0A Stream can be treated as a\nsystem of record, just as you would treat data stored in databases or files.\n\n\nTransaction Support\n\n\nA developer uses a Pravega Transaction to ensure that a set of events are\nwritten to a stream atomically.\n\n\nA Pravega Transaction is part of Pravega's Writer API. \u00a0Data can be written to a\nStream directly through the API, or an application can write data through a\nTransaction. \u00a0\u00a0With Transactions, a Writer can persist data now, and later\ndecide whether the data should be appended to a Stream or abandoned.\n\n\nUsing a Transaction, data is written to the Stream only when the Transaction is\ncommitted. \u00a0When the Transaction is committed, all data written to the\nTransaction is atomically appended to the Stream. \u00a0Because Transactions are\nimplemented in the same way as Stream Segments, data written to a Transaction is\njust as durable as data written directly to a Stream. \u00a0If a Transaction is\nabandoned (e.g. if the Writer crashes) the Transaction is aborted and all data\nis discarded. \u00a0Of course, an application can choose to abort the Transaction\nthrough the API if a condition occurs that suggests the Writer should discard\nthe data.\u00a0\n\n\nTransactions are key to chaining Flink jobs together. \u00a0When a Flink job uses\nPravega as a sink, it\u00a0can begin a Transaction, and if it successfully finishes\nprocessing, commit the Transaction, writing the data into its Pravega-based\nsink. \u00a0If the job fails for some reason, the Transaction times out and data is\nnot written. \u00a0When the job is restarted, there is no \"partial result\" in the\nsink that would need to be managed or cleaned up.\n\n\nCombining Transactions and other key features of Pravega, it is possible to\nchain Flink jobs together, having one job's Pravega-based sink be the source for\na downstream Flink job. \u00a0This provides the ability for an entire pipeline of\nFlink jobs to have end-end exactly once, guaranteed ordering of data processing.\n\n\nOf course, it is possible for Transactions across multiple Streams be\ncoordinated with Transactions, so that a Flink job can use 2 or more\nPravega-based sinks to provide source input to downstream Flink jobs. \u00a0In\naddition, it is possible for application logic to coordinate Pravega\nTransactions with external databases such as Flink's checkpoint store.\n\n\nLearn more about Transactions\n\nhere\n.",
            "title": "Key Features"
        },
        {
            "location": "/key-features/#pravega-key-features",
            "text": "This document explains some of the key features of Pravega. \u00a0It may be\nadvantageous if you are already familiar with the core  concepts of\nPravega .",
            "title": "Pravega Key Features"
        },
        {
            "location": "/key-features/#pravega-design-principles",
            "text": "Pravega was designed to support the new generation of streaming applications:\napplications that deal with a large amount of data arriving continuously that\nalso need to generate an accurate analysis of that data in the face of late\narriving data, data arriving out of order and failure conditions. There are\nseveral open source tools to enable developers to build such applications,\nincluding Apache Flink, Apache Beam, Spark Streaming and others.\nTo date, these applications used systems such as Apache Kafka, Apache ActiveMQ,\nRabbitMQ, Apache Cassandra, and Apache HDFS to ingest and store data. We envision\ninstead a unification of the two concepts and our work focuses on both ingesting\nand storing stream data.  Pravega approaches streaming applications from a storage perspective. It enables\napplications to ingest stream data continuously and storing it permanently. Such\nstream data can be accessed with low latency (order of milliseconds), but also\nmonths, years ahead as part of analyzing historical data.  The design of Pravega incorporates lessons learned from using the Lambda architecture\nto build streaming applications and the challenges to deploy streaming applications\nat scale that consistently deliver accurate results in a fault tolerant manner.\nThe Pravega architecture provides strong durability and consistency guarantees,\ndelivering a rock solid foundation to build streaming applications upon.  With the Lambda architecture, the developer uses a complex combination of middleware\ntools that include batch style middleware mainly influenced by Hadoop and\ncontinuous processing tools like Storm, Samza, Kafka and others.   In this architecture, batch processing is used to deliver accurate, but potentially\nout of date analysis of data. The second path processes data as it is ingested, and\nin principle the results are innacurate, which justifies the first\nbatch path. With this approach, there are two copies of the application logic because\nthe programming models of the speed layer are different than those used in the\nbatch layer. \u00a0An implementation of the Lambda architecture can be difficult to maintain\nand manage in production.\u00a0This style of big data application design consequently has\nbeen losing traction. A different kind of architecture has been gaining traction recently\nthat does not rely on a batch processing data path. This architecture is called Kappa.  The Kappa architecture style is a reaction to the complexity of the Lambda architecture\nand relies on components that are designed for streaming, supporting stronger\nsemantics and delivering both fast and accurate data analysis. The Kappa\narchitecture is a simpler approach:   There is only one data path to execute, and one implementation of the application logic\nto maintain, not two. \u00a0With the right tools, built for the demands of processing\nstreaming data in a fast and accurate fashion, it becomes simpler to design\nand run applications in the space of IoT, connected cars, finance, risk management, online\nservices, etc. With the right tooling, it is possible to build such pipelines and serve\napplications that present high volume and demand low latency.  Applications often require more than one stage of processing. Any practical system for stream\nanalytics must be able to accomodate the composition of stages in the form of data pipelines:   With data pipelines,\u00a0it is important to think of guarantees end-to-end rather than on a\nper componenent basis. For example, it is not sufficient that one stage guarantees exactly-once\nsemantics while at least one other does not make such a guarantee. Our goal in Pravega is enable\nthe design and implementation of data pipelines with strong guarantees end-to-end.",
            "title": "Pravega Design Principles"
        },
        {
            "location": "/key-features/#pravega-storage-reimagined-for-a-streaming-world",
            "text": "Pravega introduces a new storage primitive, a stream, that matches\ncontinuous processing of unbounded data. \u00a0In Pravega, a stream is a named,\ndurable, append-only and unbounded sequence of bytes. \u00a0With this primitive, and\nthe key features discussed in this document, Pravega is an ideal component to\ncombine with stream processing engines such as Flink to build streaming\napplications. \u00a0Because of Pravega's key features, we imagine that it will be the\nfundamental storage primitive for a new generation of streaming-oriented\nmiddleware.  Let's examine the key features of Pravega.",
            "title": "Pravega - Storage Reimagined for a Streaming World"
        },
        {
            "location": "/key-features/#exactly-once-semantics",
            "text": "By exactly once semantics we mean that Pravega ensures that data is not duplicated\nand no event is missed despite failures. Of course, this statement comes with a\nnumber of caveats, like any other system that promises exactly-once semantics, but\nlet's not dive into the gory details here. An important consideration is that\nexactly-once semantics is a natural part of Pravega and has been a goal and part\nof the design from day zero.  To achieve exactly once semantics, Pravega\u00a0Streams are durable, ordered,\nconsistent and transactional. \u00a0We discuss durable and transactional in separate\nsections below.  By ordering, we mean that data is observed by readers in the order it is written.\nIn Pravega, data is written along with an application-defined routing key. \u00a0\nPravega makes ordering guarantees in terms of routing keys. \u00a0Two pieces of data\nwith the same routing key will always be read by a Reader in the order they were\nwritten. Pravega's\u00a0ordering guarantees allow data reads to be replayed (e.g.\nwhen applications crash) and the results of replaying the reads will be the\nsame.  By consistency, we mean all Readers see the same ordered view of data for a\ngiven routing key, even in the face of failure. Systems that are \"mostly\nconsistent\" are not sufficient for building accurate data processing.  Systems that provide \"at least once\" semantics might present duplication. In\nsuch systems, a data producer might write the same data twice in some scenarios.\nIn Pravega, writes are idempotent, rewrites done as a result of reconnection\ndon't result in data duplication. Note that we make no guarantee when the data\ncoming from the source already contains duplicates. Written data is opaque to\nPravega and it makes no attempt to remove existing duplicates.  We have not limited our focus to exactly-once semantics for writing, however.\nWe also provide, and are actively working on extending the features, that enable\nexactly-once end-to-end for a data pipeline. The strong consistency guarantees\nthat the Pravega store provides along with the semantics of a data analytics \nengine like Flink enables such end-to-end guarantees.",
            "title": "Exactly Once Semantics"
        },
        {
            "location": "/key-features/#auto-scaling",
            "text": "Unlike systems with static partitioning, Pravega can automatically scale\nindividual data streams to accommodate changes in data ingestion rate.  Imagine an IoT application with millions of devices feeding thousands of data\nstreams with information about those devices. \u00a0Imagine a pipeline\u00a0of Flink jobs\nthat process those streams to derive business value from all that raw IoT data:\npredicting device failures, optimizing service delivery\u00a0through those devices,\nor tailoring a customer's experience when interacting with those devices.\n\u00a0Building such an application at\u00a0scale is difficult without having the\ncomponents be able to scale automatically as the rate of data increases and\ndecreases.  With Pravega, it is easy to\u00a0elastically and independently scale data ingestion,\nstorage and processing \u2013 orchestrating the scaling of every component in a data\npipeline.  Pravega's support of auto scaling starts with the idea that Streams are\npartitioned into Stream Segments. \u00a0A Stream may have 1 or more Stream Segments; recall\nthat a Stream Segment is a partition of the Stream associated with\na range of routing keys. \nAny data written into the Stream is written to the Stream Segment associated with the\ndata's routing key. \u00a0Writers use application-meaningful routing keys like\ncustomer-id, timestamp, machine-id, etc to make sure like data is grouped\ntogether. \u00a0  A Stream Segment is the fundamental unit of parallelism in Pravega Streams. \u00a0A\nStream with multiple Stream Segments can support more parallelism of data\nwrites; multiple Writers writing data into the different Stream Segments\npotentially involving all the Pravega Servers in the cluster. \u00a0On the Reader\nside, the number of Stream Segments represents the maximum degree of read\nparallelism possible. \u00a0If a Stream has N Stream Segments, then a ReaderGroup\nwith N Readers can consume from the Stream in parallel. \u00a0Increase the number of\nStream Segments, you can increase the number of Readers in the ReaderGroup to\nincrease the scale of processing the data from that Stream. \u00a0And of course if\nthe number of Stream Segments decreases, it would be a good idea to reduce the\nnumber of Readers.  A Stream can be configured to grow the number of Stream Segments as more data is\nwritten to the Stream, and to shrink when data volume drops off. \u00a0We refer to\nthis configuration as the Stream's Service Level Objective or SLO. \u00a0Pravega\nmonitors the rate of data input to the Stream and uses the SLO to add or remove\nStream Segments from a Stream. \u00a0Segments are added by splitting a Segment.\n\u00a0Segments are removed by merging two Segments. \u00a0See\u00a0 AutoScaling:\u00a0The\u00a0number of\nStream Segments can vary over\ntime ,\nfor more detail on how\u00a0Pravega manages Stream Segments.  It is possible to coordinate the auto scaling of Streams in Pravega with\napplication scale out (in the works). \u00a0Using metadata available from Pravega,\napplications can configure\u00a0the\u00a0scaling of their application components; for\nexample, to drive the number of instances of a Flink job. Alternatively, you\ncould use software such as Cloud Foundry, Mesos/Marathon, Kubernetes or the\nDocker stack to deploy new instances of an application to react to increased\nparallelism at the Pravega level, or to terminate instances as Pravega scales\ndown in response to reduced rate of data ingestion.",
            "title": "Auto Scaling"
        },
        {
            "location": "/key-features/#distributed-computing-primitive",
            "text": "Pravega is great for distributed applications, such as micro-services; it can be\nused as a data storage mechanism, for messaging between micro-services and for\nother distributed computing services such as leader election.\u00a0  State Synchronizer, a part of the Pravega API, is the basis of sharing state\nacross a cluster with consistency and optimistic concurrency. \u00a0State\nSynchronizer is based on a fundamental conditional write operation in Pravega,\nso that data is written only if it would appear at a given position in the\nStream. \u00a0If a conditional write operation cannot meet the condition, it fails.  State Synchronizer is therefore a strong synchronization primitive that can be\nused for shared state in a cluster, membership management, leader election and\nother distributed computing scenarios.  You can learn more about the State Synchronizer here .",
            "title": "Distributed Computing Primitive"
        },
        {
            "location": "/key-features/#write-efficiency",
            "text": "Pravega write latency is of the order of milliseconds, and seamlessly scales\nto handle high throughput reads and writes from thousands of concurrent\nclients, making it ideal for IoT and other time sensitive applications.  Streams are light weight, Pravega can support millions of Streams, this frees\nthe application from worrying about statically configuring streams and preallocating \na small fixed number of streams and husbanding or limiting stream resource.  Write operations in Pravega are low latency, under 10 ms to return an\nacknowledgment is returned to a Writer. Furthermore, writes are optimized so\nthat I/O throughput is limited by network bandwidth; \u00a0the persistence mechanism\nis not the bottleneck. \u00a0Pravega uses Apache BookKeeper to persist all write\noperations. \u00a0BookKeeper persists and protects the data very efficiently.\n\u00a0Because data is protected before the write operation is acknowledged to the\nWriter, data is always durable. \u00a0As we discuss below, data durability is a\nfundamental characteristic of a storage primitive, \u00a0To add further efficiency,\nwrites to BookKeeper often involve data from multiple Stream Segments, so the\ncost of persisting data to disk can be amortized over several write operations.  There is no durability-performance trade-off with Pravega.   Reads are efficient too. \u00a0A Reader can read from a Stream either at the tail of\nthe Stream or at any part of the Stream's history. \u00a0Unlike some log-based\nsystems that use the same kind of storage for tail reads and writes as well as\nreads to historical data, Pravega uses two types of storage. \u00a0The tail of the\nStream is in so-called Tier-1 storage. \u00a0Writes are implemented by Apache\nBookKeeper as mentioned above. \u00a0Tail reads are served out of a Pravega-managed\nmemory cache. \u00a0In fact, BookKeeper serves reads only in failure recovery\nscenarios, where a Pravega Server has\u00a0crashed\u00a0and it is being recovered. \u00a0This\nuse of BookKeeper is\u00a0exactly\u00a0what it was designed for: fast\nwrites,\u00a0occasional\u00a0reads. \u00a0 The historical part of the Stream is in so-called\nTier 2 storage that is optimized for low-cost storage with high-throughput.\n\u00a0Pravega uses efficient in-memory read ahead cache, taking advantage of the fact\nthat Streams are usually read in large contiguous chunks and that HDFS is well\nsuited for\u00a0those\u00a0sort of large, high-throughput reads. \u00a0It is also worth noting\nthat tail reads do not impact the performance of writes.",
            "title": "Write Efficiency"
        },
        {
            "location": "/key-features/#unlimited-retention",
            "text": "Data in Streams can be retained for as long as the application needs it, \nconstrained to the amount of data available, which is unbounded given the use\nof cloud storage in Tier 2. Pravega provides one convenient API to\naccess both real-time and historical data. \u00a0With Pravega, batch and real-time\napplications can both be handled efficiently; yet another reason why Pravega is\na great storage primitive for Kappa architectures.  If there is a value to keeping old data, why not keep it around? \u00a0For example,\nin a machine learning example, you may want to periodically change the model and\ntrain the new version of the model against as much historical data as possible\nto yield more accurate predictive power of the model. \u00a0With Pravega\nauto-tiering, keeping lots of historical data does not affect the performance of\ntail reads and writes.\u00a0  Size of a stream is not limited by the storage capacity of a single server, but\nrather, it is limited only by the storage capacity of your storage cluster or cloud\nprovider. \u00a0As cost of storage decreases, the economic incentive to delete data\ngoes away",
            "title": "Unlimited Retention"
        },
        {
            "location": "/key-features/#storage-efficiency",
            "text": "Use Pravega to build pipelines of data processing, combining batch, real-time\nand other applications without duplicating data for every step of the pipeline.  Consider the following data processing environment that combines real time\nprocessing using Spark, Flink, and or Storm; Haddoop for batch; some kind of\nLucene-based Search mechanism like Elastic Search for full text search; and\nmaybe one (or several) NoSQL databases to support micro-services apps.   Using traditional approaches, one set of source data, for example, sensor data\nfrom an IoT app, would be ingested and replicated separately by each system.\n\u00a0You would end up with 3 replicas of the data protected in the pub/sub system, 3\ncopies in HDFS, 3 copies in Lucene, 3 copies in the NoSQL database. \u00a0When we\nconsider the source data is measured in terabytes, the cost of data replication\nseparated by middleware category becomes prohibitively expensive.  Consider the same pipeline using Pravega and middleware adapted to use Pravega\nfor its storage:   With Pravega, the data is ingested and protected in one place; Pravega provides\nthe single source of truth for the entire pipeline. \u00a0Furthermore, with the bulk\nof the data being stored in Tier-2 enabled with erasure coding to efficiently\nprotect the data, the storage cost of the data is substantially reduced.",
            "title": "Storage Efficiency"
        },
        {
            "location": "/key-features/#durability",
            "text": "With Pravega, you don't \u00a0face a compromise between performance, durability and\nconsistency. Pravega\u00a0provides durable storage of streaming data with strong\nconsistency, ordering guarantees and great performance.  Durability is a fundamental storage primitive requirement. \u00a0Storage that could\nlose data is not reliable storage. \u00a0Systems based on such storage are not\nproduction quality.  Once a write operation is acknowledged, the data will never be lost, even when\nfailures occur. \u00a0This is because Pravega always saves data in protected,\npersistent storage before the write operation returns to the Writer.  With Pravega, data in the Stream is protected. \u00a0A Stream can be treated as a\nsystem of record, just as you would treat data stored in databases or files.",
            "title": "Durability"
        },
        {
            "location": "/key-features/#transaction-support",
            "text": "A developer uses a Pravega Transaction to ensure that a set of events are\nwritten to a stream atomically.  A Pravega Transaction is part of Pravega's Writer API. \u00a0Data can be written to a\nStream directly through the API, or an application can write data through a\nTransaction. \u00a0\u00a0With Transactions, a Writer can persist data now, and later\ndecide whether the data should be appended to a Stream or abandoned.  Using a Transaction, data is written to the Stream only when the Transaction is\ncommitted. \u00a0When the Transaction is committed, all data written to the\nTransaction is atomically appended to the Stream. \u00a0Because Transactions are\nimplemented in the same way as Stream Segments, data written to a Transaction is\njust as durable as data written directly to a Stream. \u00a0If a Transaction is\nabandoned (e.g. if the Writer crashes) the Transaction is aborted and all data\nis discarded. \u00a0Of course, an application can choose to abort the Transaction\nthrough the API if a condition occurs that suggests the Writer should discard\nthe data.\u00a0  Transactions are key to chaining Flink jobs together. \u00a0When a Flink job uses\nPravega as a sink, it\u00a0can begin a Transaction, and if it successfully finishes\nprocessing, commit the Transaction, writing the data into its Pravega-based\nsink. \u00a0If the job fails for some reason, the Transaction times out and data is\nnot written. \u00a0When the job is restarted, there is no \"partial result\" in the\nsink that would need to be managed or cleaned up.  Combining Transactions and other key features of Pravega, it is possible to\nchain Flink jobs together, having one job's Pravega-based sink be the source for\na downstream Flink job. \u00a0This provides the ability for an entire pipeline of\nFlink jobs to have end-end exactly once, guaranteed ordering of data processing.  Of course, it is possible for Transactions across multiple Streams be\ncoordinated with Transactions, so that a Flink job can use 2 or more\nPravega-based sinks to provide source input to downstream Flink jobs. \u00a0In\naddition, it is possible for application logic to coordinate Pravega\nTransactions with external databases such as Flink's checkpoint store.  Learn more about Transactions here .",
            "title": "Transaction Support"
        },
        {
            "location": "/faq/",
            "text": "Frequently Asked Questions\n\n\nWhat is Pravega?\n\nPravega is an open source storage primitive implementing \nStreams\n for continuous and unbounded data.  See \nhere\n for more definitions of terms related to Pravega.\n\n\nWhat does \"Pravega\" mean?\n\n\"Pravega\" is a word from Sanskrit referring to \"good speed\".\n\n\nIs Pravega similiar to systems such as Kafka and Kinesis?\n\nPravega is built from the ground up as an enterprise grade storage system to support features such as exactly once, durability etc.  Pravega is an ideal store for streaming data, data from real-time applications and IoT data.\n\n\nHow can I participate in open source?\n\nDisruptive innovation is accelerated by open source.  When Pravega was created, there was no question it made sense to make it open source.\nWe welcome contributions from experienced and new developers alike.  Check out the code in \nGithub\n .  More detail about how to get involved can be found \nhere\n.\n\n\nHow do I get started?\n\nRead the \nGetting Started\n guide for more information, and also visit \nsample-apps\n repo for some sample applications.   \n\n\nI am stuck. Where can I get help?\n\nDon\u2019t hesitate to ask! Contact the developers and community on the mailing lists\nif you need any help.  See \nJoin the Community\n for more details.\n\n\nDoes Pravega support exactly once semantics?\n\nAbsolutely.  See \nKey Features\n for a discussion on how Pravega supports exactly once semantics.\n\n\nHow does Pravega work with stream processors such as Apache Flink?\n\nSo many features of Pravega make it ideal for stream processors.  First, Pravega comes out of the box with a Flink connector.  Critically, Pravega provides exactly once semantics, making it much easier to develop accurate stream processing applications.  The combination of exactly once semantics, durable storage and transactions makes Pravega an ideal way to chain Flink jobs together, providing end-end consistency and exactly once semantics.  See \nhere\n for a list of key features of Pravega.\n\n\nHow does auto scaling work between stream processors and Flink\n\nAuto scaling is a feature of Pravega where the number of segments in a stream changes based on the ingestion rate of data.  If data arrives at a faster rate, Pravega increases the capacity of a stream by adding segments.  When the data rate falls, Pravega can reduce capacity of a stream. As Pravega scales up and down the capacity of a stream, applications, such as a Flink job can observe this change and respond by adding or reducing the number of job instances consuming the stream. See the \"Auto Scaling\" section in \nKey Features\n for more discussion of auto scaling.\n\n\nWhat consistency guarantees does Pravega provide?\n\nPravega makes several guarantees. Durability - once data is acknowledged to a client, Pravega guarantees it is protected.  Ordering - events with the same routing key will always be read in the order they were written. Exactly once - data written to Pravega will not be duplicated.\n\n\nWhy is supporting consistency and durability so important for storage systems such as Pravega?\n\nPrimarily because it makes building applications easier. Consistency and durability are key for supporting exactly once semantics. Without exactly once semantics, it is difficult to build fault tolerant applications that consistency produce accurate results.  See \nKey Features\n for a discussion on consistency and durability guarantees play a role in Pravega's support of exactly once semantics.\n\n\nDoes Pravega support transactions?\n\nYes.  The Pravega API allows an application to create a transaction on a stream and write data to the transaction.  The data is durably stored, just like any other data written to Pravega.  When the application chooses, it can commit or abort the transaction.  When a transaction is committed, the data in the transaction is atomically appended to the stream.  See \nhere\n for more details on Pravega's transaction support.\n\n\nDoes Pravega support transactions across different routing keys?\n\nYes.  A transaction in Pravega is itself a stream; it can have 1 or more segments and data written to the transaction is placed into the segment associated with the data's routing key.  When the transaction is committed, the transaction data is appended to the appropriate segment in the stream.\n\n\nDo I need HDFS installed in order to use Pravega?\n\nYes. Normally, you would deploy an HDFS for Pravega to use as its Tier 2 storage.  However, for simple test/dev environments, the so-called standAlone version of Pravega provides its own simulated HDFS.  See the \nRunning Pravega\n guide for more details.\n\n\nWhich Tier 2 storage systems does Pravega support?\n\nPravega is designed to support various types of Tier 2 storage systems.  Currently we have implemented HDFS as the first embodiment of Tier 2 storage.\n\n\nWhat distributed computing primitives does Pravega provide?\n\nPravega provides an API construct called StateSynchronizer.  Using the StateSynchronizer, a developer can use Pravega to build synchronized shared state between multiple processes.  This primitive can be used to build all sorts of distributed computing solutions such as shared configuration, leader election, etc.  See the \"Distributed Computing Primitive\" section in \nKey Features\n for more details.\n\n\nWhat hardware do you recommend for Pravega?\n\nThe Segment Store requires faster access to storage and more memory for its cache.  It can run on 1 GB memory and 2 core CPU.  10 GB is a good start for storage.  The Controller is less resource intensive, 1 CPU and 0.5 GB memory is a good start.",
            "title": "Pravega FAQ"
        },
        {
            "location": "/faq/#frequently-asked-questions",
            "text": "What is Pravega? \nPravega is an open source storage primitive implementing  Streams  for continuous and unbounded data.  See  here  for more definitions of terms related to Pravega.  What does \"Pravega\" mean? \n\"Pravega\" is a word from Sanskrit referring to \"good speed\".  Is Pravega similiar to systems such as Kafka and Kinesis? \nPravega is built from the ground up as an enterprise grade storage system to support features such as exactly once, durability etc.  Pravega is an ideal store for streaming data, data from real-time applications and IoT data.  How can I participate in open source? \nDisruptive innovation is accelerated by open source.  When Pravega was created, there was no question it made sense to make it open source.\nWe welcome contributions from experienced and new developers alike.  Check out the code in  Github  .  More detail about how to get involved can be found  here .  How do I get started? \nRead the  Getting Started  guide for more information, and also visit  sample-apps  repo for some sample applications.     I am stuck. Where can I get help? \nDon\u2019t hesitate to ask! Contact the developers and community on the mailing lists\nif you need any help.  See  Join the Community  for more details.  Does Pravega support exactly once semantics? \nAbsolutely.  See  Key Features  for a discussion on how Pravega supports exactly once semantics.  How does Pravega work with stream processors such as Apache Flink? \nSo many features of Pravega make it ideal for stream processors.  First, Pravega comes out of the box with a Flink connector.  Critically, Pravega provides exactly once semantics, making it much easier to develop accurate stream processing applications.  The combination of exactly once semantics, durable storage and transactions makes Pravega an ideal way to chain Flink jobs together, providing end-end consistency and exactly once semantics.  See  here  for a list of key features of Pravega.  How does auto scaling work between stream processors and Flink \nAuto scaling is a feature of Pravega where the number of segments in a stream changes based on the ingestion rate of data.  If data arrives at a faster rate, Pravega increases the capacity of a stream by adding segments.  When the data rate falls, Pravega can reduce capacity of a stream. As Pravega scales up and down the capacity of a stream, applications, such as a Flink job can observe this change and respond by adding or reducing the number of job instances consuming the stream. See the \"Auto Scaling\" section in  Key Features  for more discussion of auto scaling.  What consistency guarantees does Pravega provide? \nPravega makes several guarantees. Durability - once data is acknowledged to a client, Pravega guarantees it is protected.  Ordering - events with the same routing key will always be read in the order they were written. Exactly once - data written to Pravega will not be duplicated.  Why is supporting consistency and durability so important for storage systems such as Pravega? \nPrimarily because it makes building applications easier. Consistency and durability are key for supporting exactly once semantics. Without exactly once semantics, it is difficult to build fault tolerant applications that consistency produce accurate results.  See  Key Features  for a discussion on consistency and durability guarantees play a role in Pravega's support of exactly once semantics.  Does Pravega support transactions? \nYes.  The Pravega API allows an application to create a transaction on a stream and write data to the transaction.  The data is durably stored, just like any other data written to Pravega.  When the application chooses, it can commit or abort the transaction.  When a transaction is committed, the data in the transaction is atomically appended to the stream.  See  here  for more details on Pravega's transaction support.  Does Pravega support transactions across different routing keys? \nYes.  A transaction in Pravega is itself a stream; it can have 1 or more segments and data written to the transaction is placed into the segment associated with the data's routing key.  When the transaction is committed, the transaction data is appended to the appropriate segment in the stream.  Do I need HDFS installed in order to use Pravega? \nYes. Normally, you would deploy an HDFS for Pravega to use as its Tier 2 storage.  However, for simple test/dev environments, the so-called standAlone version of Pravega provides its own simulated HDFS.  See the  Running Pravega  guide for more details.  Which Tier 2 storage systems does Pravega support? \nPravega is designed to support various types of Tier 2 storage systems.  Currently we have implemented HDFS as the first embodiment of Tier 2 storage.  What distributed computing primitives does Pravega provide? \nPravega provides an API construct called StateSynchronizer.  Using the StateSynchronizer, a developer can use Pravega to build synchronized shared state between multiple processes.  This primitive can be used to build all sorts of distributed computing solutions such as shared configuration, leader election, etc.  See the \"Distributed Computing Primitive\" section in  Key Features  for more details.  What hardware do you recommend for Pravega? \nThe Segment Store requires faster access to storage and more memory for its cache.  It can run on 1 GB memory and 2 core CPU.  10 GB is a good start for storage.  The Controller is less resource intensive, 1 CPU and 0.5 GB memory is a good start.",
            "title": "Frequently Asked Questions"
        },
        {
            "location": "/segment-store-service/",
            "text": "Pravega Segment Store Service\n\n\nThe Pravega Segment Store Service is a subsystem that lies at the heart of the entire Pravega deployment. It is the main access point for managing Stream Segments, providing the ability to create, delete and modify/access their contents. The Pravega Client communicates with the Pravega Stream Controller to figure out which Segments need to be used (for a Stream), and both the Stream Controller and the Client deal with the Segment Store Service to actually operate on them.\n\n\nThe basic idea behind the Segment Store Service is that it buffers incoming data in a very fast and durable append-only medium (Tier 1), and syncs it to a high-throughput (but not necessarily low latency) system (Tier 2) in the background, while aggregating multiple (smaller) operations to a Segment into a fewer (but larger) ones.\n\n\nThe Pravega Segment Store Service can provide the following guarantees:\n- Stream Segments that are unlimited in length, with append-only semantics, yet supporting arbitrary-offset reads.\n- No throughput degradation when performing small appends, regardless of the performance of the underlying Tier-2 storage system.\n- Multiple concurrent writers to the same Segment.\n - Order is guaranteed within the context of a single writer, but appends from multiple concurrent writers will be added in the order in which they were received (appends are atomic without interleaving their contents).\n- Writing to and reading from a Segment concurrently with relatively low latency between writing and reading.\n\n\nTerminology\n\n\nThroughout the rest of this document, we will use the following terminology:\n- \nStream Segment\n or \nSegment\n: A contiguous sequence of bytes. Similar to a file with no size limit. This is a part of a Stream, limited both temporally and laterally (by key). The scope of Streams and mapping Stream Segments to such Streams is beyond the purpose of this document.\n- \nTier-2 storage\n or \nPermanent Storage\n: The final resting place of the data.\n- \nTier-1 storage\n: Fast append storage, used for durably buffering incoming appends before distribution to Tier-2 Storage.\n- \nCache\n: A key-value local cache with no expectation of durability.\n- \nPravega Segment Store Service\n or \nSegment Store\n: The Service that this document describes. \n- \nTransaction\n: A sequence of appends that are related to a Segment, which, if persisted, would make up a contiguous range of bytes within it. This is used for ingesting very large records or for accumulating data that may or may not be persisted into the Segment (but its fate cannot be determined until later in the future).\n    - Note that at the Pravega level, a Transaction applies to an entire Stream. In this document, a Transaction applies to a single Segment.\n\n\nArchitecture\n\n\nThe \nSegment Store\n is made up of the following components:\n- \nPravega Node\n: a host running a Pravega Process.\n- \nStream Segment Container\n (or \nSegment Container\n): A logical grouping of Stream Segments. The mapping of Segments to Containers is deterministic and does not require any persistent store; Segments are mapped to Containers via a hash function (based on the Segment's name).\n- \nDurable Data Log Adapter\n (or \nDurableDataLog\n): an abstraction layer for Tier-1 Storage.\n- \nStorage Adapter\n: an abstraction layer for Tier-2 Storage.\n- \nCache\n: an abstraction layer for append data caching.\n- \nStreaming Client\n: an API that can be used to communicate with the Pravega Segment Store.\n- \nSegment Container Manager\n: a component that can be used to determine the lifecycle of Segment Containers on a Pravega Node. This is used to start or stop Segment Containers based on an external coordination service (such as the Pravega Controller).\n\n\nThe Segment Store handles writes by first writing them to a log (Durable Data Log) on a fast storage (SSDs preferably) and immediately acking back to the client after they have been persisted there. Subsequently, those writes are then aggregated into larger chunks\u00a0and written in the background to Tier-2 storage. Data for appends that have been acknowledged (and are in Tier-1) but not yet in Tier-2 is stored in the Cache (in addition to Tier-1). Once such data has been written to Tier-2 Storage, it may or may not be kept in the Cache, depending on a number of factors, such as Cache utilization/pressure and access patterns.\n\n\nMore details about each component described above can be found in the \nComponents\n section (below).\n\n\nSystem Diagram\n\n\n\n\nIn this image, we show the major components of the Segment Store (for simplicity, only one Segment Container is depicted). All Container components and major links between them (how they interact with each other) are shown. The \nContainer Metadata\n component is not shown, because every other component communicates with it in one form or another, and adding it would only clutter the diagram.\n\n\nMore detailed diagrams can be found under the \nData Flow\n section (below).\n\n\nComponents\n\n\nSegment Containers\n\n\nSegment Containers are a logical grouping of Segments, and are responsible for all operations on those Segments within their span. A Segment Container is made of multiple sub-components:\n- \nSegment Container Metadata\n: A collection of Segment-specific metadata that describe the current state of each Segment (how much data in Tier-2, how much in Tier-1, whether it is sealed, etc.), as well as other misc info about each Container.\n- \nDurable Log\n: The Container writes every operation it receives to this log and acks back only when the log says it has been accepted and durably persisted..\n- \nRead Index\n:\u00a0An in-memory index of where data can be read from. The Container delegates all read requests to it, and it is responsible for fetching the data from wherever it is currently located (Cache, Tier-1 Storage or Tier-2 Storage).\n- \nCache\n:\u00a0Used to store data for appends that exist in Tier-1 only (not yet in Tier-2), as well as blocks of data that support reads.\n- \nStorage Writer\n:\u00a0Processes the durable log operations and applies them to Tier-2 storage (in the order in which they were received). This component is also the one that coalesces multiple operations together, for better back-end throughput.\n\n\nSegment Container Metadata\n\n\nThe Segment Container Metadata is critical to the good functioning and synchronization of its components. This metadata is shared across all components and it comes at two levels: Container-wide Metadata and per-Segment Metadata. Each serves a different purpose and is described below.\n\n\nContainer Metadata\n\n\nEach \nSegment Container\n needs to keep some general-purpose metadata that affects all operations inside the container:\n\n\n\n\nOperation Sequence Number\n:\u00a0The largest sequence number assigned by the \nDurable Log\n. Every time a new Operation is received and successfully processed by the \nDurable Log\n, this number is incremented (its value never decreases or otherwise roll back, even if an operation failed to be persisted).\n\n\nThe Operation Sequence Number is guaranteed to be strict-monotonic increasing (no two Operations have the same value, and an Operation will always have a larger Sequence Number than all Operations before it).\n\n\n\n\n\n\nEpoch\n: A number that is incremented every time a successful recovery (Container Start) happens. This value is durably incremented and stored as part of recovery and can be used for a number of cases (a good use is Tier-2 fencing for HDFS, which doesn't provide a good, native mechanism for that).\n\n\nActive Segment Metadata\n: Information about each active Segment (see next section below). A Segment is active if it has had activity (read or write) recently and is currently loaded in memory. If a Segment is not used for a while, or if there are many Segments currently active, a Segment becomes inactive by having its outstanding metadata flushed to Tier-2 Storage and evicted from memory.\n\n\nTier-1 Metadata\n: Various pieces of information that can be used to accurately truncate the Tier-1 Storage Log once all operations prior to that point have been durably stored to Tier-2.\n\n\nCheckpoints\n: Container Metadata is periodically checkpointed by having its entire snapshot (including Active Segments) serialized to Tier-1. A Checkpoint serves as a Truncation Point for Tier-1, meaning it contains all the updates that have been made to the Container via all the processed operations before it so we no longer need those operations in order to reconstruct the Metadata. If we truncate Tier-1 on a Checkpoint, then we can use information from Tier-2 and this Checkpoint to reconstruct what is in the Metadata previously, without relying on any operation prior to it in Tier-1. \n\n\n\n\nSegment Metadata\n\n\nEach Segment Container needs to keep per-segment metadata, which it uses to keep track of the state of each segment as it processes operations for it. The metadata can be volatile (it can be fully rebuilt upon recovery) and contains the following properties for each segment that is currently in use:\n\n\n\n\nName\n the name of the Segment.\n\n\nId\n: Internally assigned unique Segment Id. This is used to refer to Segments, which is preferred to the Name. This Id is sticky for the lifetime of the Segment, which means that even if the Segment becomes inactive, a future reactivation will have it mapped to the same Id.\n\n\nStartOffset\n (also known as \nTruncationOffset\n): the lowest offset of the data that is available for reading. A non-truncated segment will have Start Offset equal to 0, while subsequent Truncate operations will increase (but never decrease) this number. \n\n\nStorageLength\n: the highest offset of the data that exists in Tier-2 Storage.\n\n\nLength\n: the highest offset of the committed data in\u00a0Tier-1 Storage.\n\n\nLastModified\n: the timestamp of the last processed (and acknowledged) append.\n\n\nIsSealed\n: whether the Segment is closed for appends (this value may not have been applied to Tier-2 Storage yet).\n\n\nIsSealedInStorage\n: whether the Segment is closed for appends (and this has been persisted in Tier-2 Storage).\n\n\nIsMerged\n: whether this Segment has been merged into another one (but this has not yet been persisted in Tier-2 Storage). This only applies for Transactions. Once the merger is persisted into Tier-2, the Transaction Segment does not exist anymore (so \nIsDeleted\n will become true).\n\n\nIsDeleted\n: whether the Segment is deleted or has recently been merged into another Segment. This only applies for recently deleted Segments, and not for Segments that never existed.\n\n\n\n\nThe following are\u00a0\nalways\n\u00a0true for any Segment:\n\n\n\n\nStorageLength\n\u00a0<=\u00a0\nLength\n\n\nStartOffset\n\u00a0<=\u00a0\nLength\n\n\n\n\nLog Operations\n\n\nA \nLog Operation\n is the basic unit that is enqueued in the Durable Log. It does not represent an action, per se, but is the base for several serializable operations (we can serialize multiple types of operations, not just Appends). Each Operation is the result of an external action (which denote the alteration of a Segment), or an internal trigger, such as Metadata maintenance operations.\n\n\nEvery Log Operation has the following elements:\n- \nSequenceNumber\n: the unique sequence number assigned to this entry (see more under \nContainer Metadata\n).\n\n\nThese are the various types of Log Operations: \n- \nStorage Operations\n represent operations that need to be applied to the underlying Tier-2 Storage:\n    - \nStreamSegmentAppendOperation\n: Represents an Append to a particular Segment.\n    - \nCachedStreamSegmentAppendOperation\n: Same as \nStreamSegmentAppendOperation\n, but this is for internal use (instead of having an actual data payload, it points to a location in the cache from where the data can be retrieved).\n    - \nStreamSegmentSealOperation\n:\u00a0When processed, it sets a flag in the in-memory metadata that no more appends can be received. When the\u00a0Storage Writer\u00a0processes it, it marks the Segment as read-only in Tier-2 Storage.\n    - \nStreamSegmentTruncateOperation\n: Truncates a Segment at a particular offset. This causes the Segment's \nStartOffset\n to change.\n    - \nMergeTransactionOperation\n: Indicates that a Transaction is to be merged into its parent Segment.\n- \nMetadata Operations\n are auxiliary operations that indicate a change to the Container Metadata. They can be the result of an external operation (we received an request for a Segment we never knew about before, so we must assign a unique Id to it) or to snapshot the entire Metadata (which helps with recovery and cleaning up Tier-1 Storage). The purpose of the \nMetadata Operations\n is to reduce the amount of time needed for failover recovery (when needed):\n    - \nStreamSegmentMapOperation\n:\u00a0maps an Id to a Segment Name.\n    - \nTransactionMapOperation\n:\u00a0maps an Id to a Transaction and to its Parent Segment.\n    - \nUpdateAttributesOperation\n: Updates any attributes on a Segment.\n    - \nMetadataCheckpoint\n includes an entire snapshot of the Metadata. This can be useful when during recovery - this contains all metadata up to this point, which is a sufficient base for all operations after it.\n\n\nDurable Log\n\n\nThe \nDurable Log\n is the central component that handles all Log Operations. All Operations (which are created by the Container) are added to the Durable Log, which processes them in the order in which they were received. It is made up of a few other components, all of which are working towards a single goal of processing all incoming operations as quickly as possible, without compromising data integrity.\n\n\nInformation Flow in the Durable Log\n\n\n\n\nAll received operations are added to an \nOperation Queue\n  (the caller receives a Future which will be completed when the operation is durably persisted)\n\n\nThe \nOperation Processor\n picks all operations currently available in the queue (if the queue is empty, it will wait until at least one Operation is added).\n\n\nThe \nOperation Processor\n runs as a continuous loop (in a background thread), and has four main stages.\n\n\nDequeue all outstanding Operations from the Operation Queue (described above)\n\n\nPre-process the Operations (validate that they are correct and would not cause undesired behavior, assign offsets (where needed), assign Sequence Numbers, etc.)\n\n\nWrite the operations to a \nData Frame Builder\n, which serializes and packs the operations in \nData Frames\n. Once a Data Frame is complete (full or no more operations to add), the Data Frame Builder sends the Data Frame to the _Durable Data Log. Note that an Operation may span multiple DataFrames - the goal is to make best use of the Durable Data Log throughput capacity by making writes as large as possible (but also taking into account that there may be a maximum size limit per write).\n\n\n\n\n\n\nWhen a DataFrame has been durably persisted in the Durable Data Log, the Operation Processor post-processes all operations that were fully written so far (adds them to in-memory structures, updates indices, etc.) and completes the Futures associated with them.\n\n\nThe \nOperation Processor\n works asynchronously, in that it does not wait for a particular Data Frame to be written before starting another one and sending it to the Durable Data Log. As such, multiple DataFrames may be in flight (but in a specific order), and the Operation Processor relies on certain ordering guarantees from the Durable Data Log (if a particular DataFrame was acked, then all DataFrames prior to that were also committed successfully, in the right order).  \n\n\nThe Operation Processor does not do any write throttling (it leaves that to the Durable Data Log implementation), but it control the size of the Data Frames that get sent to it.\n\n\n\n\n\n\n\n\nTruncation\n\n\nBased on supplied configuration, the Durable Log auto-adds a special kind of operation, named \nMetadataCheckpointOperation\n. This operation, when processed by the Operation Processor, collects a snapshot of the entire Container Metadata and serializes it to the Durable Data Log. This special Operation marks a \nTruncation Point\n - a place in the stream of Log Operations where we can issue Truncate operations. It is very important that after every truncation, the first operation to be found in the log is a \nMetadataCheckpointOperation\n, because without the prior operations to reconstruct metadata, this is the only way to be able to process subsequent operations.\n\n\nNote: Durable Data Log (Tier-1) truncation should not be confused with Segment Truncation. They serve different purposes and are applied to different targets.\n\n\nOperation Processor\n\n\nThe \nOperation Processor\n is a sub-component of the Durable Log that deals with incoming Log Operations. Its purpose is to validate, persist, and update Metadata and other internal structures based on the contents of each operation.\n\n\nOperation Metadata Updater\n\n\nThe \nOperation Metadata Updater\n is a sub-component of the Durable Log that is responsible with validating operations based on the current state of the metadata, as well as update the Metadata after a successful committal of an operation. Internally it has various mechanisms to cope with failures, and it can rollback certain changes in failure situations.\n\n\nDurable Data Log\n\n\nThe _Durable Data Log is an abstraction layer to an external component that provides append-only semantics. It is supposed to be a system which provides very fast appends to a log, that guarantees the durability and consistency of the written data. The read performance is not so much a factor, because we do not read directly from this component - we only read from it when we need to recover the contents of the Durable Log.\n\n\nAs explained above, Log Operations are serialized into Data Frames (with a single Operation able to span multiple such Frames if needed), and these Data Frames are then serialized as entries into this Durable Data Log. This is used only as a fail-safe, and we only need to read these Frames back if we need to perform recovery (in which case we need to deserialize all Log Operations contained in them, in the same order in which they were received).\n\n\nIn-Memory Operation Log\n\n\nThe \nIn-Memory Operation Log\n contains committed (and replicated) Log Operations\u00a0in the exact same order as they were added to the Durable Data Log. While the Durable Data Log contains a sequence of Data Frames (which contain serializations of Operations), the Memory Log contains the actual Operations, which can be used throughout the Durable Log (and the Storage Writer).\n\n\nThe Memory Log is essentially a chain of Log Operations\u00a0ordered by the time when the Operation\u00a0was received. We always add at one end, and we remove\u00a0from the other. When we truncate the Durable Data Log the Memory Log is also truncated at the same location.\n\n\nRead Index\n\n\nThe \nRead Index\n helps the Segment Container perform reads from streams at arbitrary offsets. While the Durable Log records (and can only replay) data in the order in which it is received, the Read Index can access the data in a random fashion. The Read Index is made of multiple \nSegment Read Indices\n (one per live segment).\n\n\nThe\u00a0\nSegment Read Index\n\u00a0is a data structure that is used to serve reads from memory, as well as pull data from Tier-2 Storage and provide \nFuture Reads\n (tail reads) when data is not yet available. When a read request is received, the Segment Read Index returns a read iterator that will return data as long as the read request parameters have not yet been satisfied. The iterator will either fetch data that is immediately available in memory, or request data from Tier-2 storage (and bring it to memory) or, if it reached the current end of the Segment, return a Future that will be completed when new data is added (thus providing tailing or future reads).\n\n\nAt the heart of the \nSegment Read Index\n lies a sorted index of entries (indexed by their start offsets) which is used to locate the requested data when needed. The index itself is implemented by a custom balanced binary search tree (AVL Tree to be more precise) with a goal of minimizing memory usage while not sacrificing insert or access performance. The entries themselves do not contain data, rather some small amount of metadata that is used to locate the data in the \nCache\n and to determine usage patterns (good for cache evictions).\n\n\nCache\n\n\nThe \nCache\n is a component where all data (whether from new appends or that was pulled from Tier-2 storage) is stored. It is a key-value store entirely managed by the Read Index. \n\n\nThe Cache is defined as an abstraction layer, and there are two implementations of it:\n- In-memory implementation (via a \nHashMap\n). This is currently only used for unit tests.\n- Memory-disk hybrid, powered by \nRocksDB\n. This is the preferred (and default) implementation, since it is not limited by available heap space or machine RAM. Performance is comparable to the in-memory implementation.\n\n\nStorage Writer\n\n\nPravega is by no means the final resting place of the data, nor is it meant to be a storage service. The Tier-2 Storage is where we want data to be in the long term and Pravega is only used to store a very short tail-end of it (using Tier-1 Storage), enough to make appends fast and aggregate them into bigger chunks for committal to Tier-2 Storage. In order to perform this, it needs another component (Storage Writer) that reads data from the Durable Log in the order in which it was received, aggregates it, and sends it to Tier-2 Storage.\n\n\nJust like the Durable Log, there is one Storage Writer\u00a0per Segment Container. Each Writer\u00a0reads Log\u00a0Operations from the In-Memory Operation Log (exposed via the \nread()\n method in the Durable Log) in the order they were processed. It keeps track of the last read item by means of its Sequence Number. This state is not persisted, and upon recovery, it can just start from the beginning of the available Durable Log.\n\n\nThe Storage Writer can process any Storage Operation (Append, Seal, Merge), and since Pravega is the sole actor modifying such data in Tier-2, it applies them without restraint. It has several mechanisms to detect and recover from possible data loss or external actors modifying data concurrently.\n\n\nIntegration with Controller\n\n\nActual methods for how Segment Containers are mapped to hosts and what rules are used for moving from from one to another are beyond the scope of this document. Here, we just describe how the Segment Store Service interacts with the \nController\n and how it manages the lifecycle of Segment Containers based on external events.\n\n\nSegment Container Manager\n\n\nEach instance of a Segment Store Service needs a \nSegment Container Manager\n. The role of this component is to manage the lifecycle of the Segment Containers that are assigned to that node (service). It performs the following duties:\n- Connects to the Controller Service-Side Client (i.e., a client that deals only with Segment Container events, and not with the management of Streams and listens to all changes that pertain to Containers that pertain to its own instance.\n- When it receives a notification that it needs to start a Segment Container for a particular Container Id, it initiates the process of bootstrapping such an object. It does not notify the requesting client of success until the operation completes without error.\n- When it receives a notification that it needs to stop a Segment Container for a particular Container Id, it initiates the process of shutting it down. It does not notify the requesting client of success until the operation completes without error.\n- If the Segment Container shuts down unexpectedly (whether during Start, or during its normal operation), it will not attempt to restart it locally; instead it will notify the Controller of the fact.\n\n\nStorage Abstractions\n\n\nThe Segment Store was not designed with particular implementations for Tier-1 or Tier-2 in mind. Instead, all these components have been abstracted out in simple, well-defined interfaces, which can be implemented against any standard file system (Tier-2) or append-only log system (Tier-1). \n\n\nPossible candidates for Tier-1 storage:\n- \nApache BookKeeper\n (preferred, adapter is fully implemented as part of Pravega)\n- Non-durable, non-replicated solutions:\n    - In-Memory (Single node deployment only - Pravega becomes a volatile buffer for Tier-2 storage; data loss is unavoidable and unrecoverable from in the case of process crash or system restart).\n        - This is used for unit test only.\n    - Local File System (Single node deployment only - Pravega becomes a semi-durable buffer for Tier-2 storage; data loss is unavoidable and unrecoverable from in the case of complete node failure)\n\n\nPossible candidates for Tier-2 storage:\n- \nHDFS\n (implementation available)\n- \nExtended S3\n (implementation available)\n- \nNFS\n (general \nFileSystem\n) (implementation available)\n- In-Memory (Single node deployment only - limited usefulness; data loss is unavoidable and unrecoverable from in the case of process crash or system restart) \n    - This is used for unit test only.\n\n\nA note about \nTier-2 Truncation\n:\n- The Segment Store supports Segment truncation at a particular offset, which means that, once that request is complete, no offset below that one will be available for reading.\n- This above is only a metadata update operation, however this also needs to be supported by Tier-2 so that the truncated data is physically deleted from it.\n- If a Tier-2 implementation does not natively support truncation from the beginning of a file with offset preservation (i.e., a Segment of length 100 is truncated at offset 50, then offsets 0..49 are deleted, but offsets 50-99 are available and are not shifted down), then the \nSegment Store\n provides a wrapper on top of a generic Tier-2 implementation that can do that\n- The \nRollingStorage\n Tier-2 wrapper splits a Segment into multiple \nSegment Chunks\n and exposes them as a single Segment to the upper layers. \nSegment Chunks\n that have been truncated out are deleted automatically. This is not a very precise application (since it relies heavily on a rollover policy dictating granularity), but it is a practical solution for those cases when the real Tier-2 implementation does not provide the features that we need.  \n\n\nData Flow\n\n\nHere are a few examples on how data flows inside the Pravega Segment Store Service.\n\n\nAppends\n\n\n\n\nThe diagram above depicts these steps (note the step numbers may not match, but the order is the same):\n\n\n\n\nSegment Store\n receives append request with params: Segment Name, Payload and AttributeUpdates.\n\n\nSegment Store\n determines the ContainerId for the given Segment and verifies that the \nSegment Container\n is registered locally. If not, it returns an appropriate error code.\n\n\nSegment Store\n delegates request to the appropriate \nSegment Container\n instance.\n\n\nSegment Container\n verifies that the Segment belongs to the Segment Container and that the Segment actually exists. If not, it returns an appropriate error code. \n\n\nDuring this process, it also get an existing Segment Id or assigns a new one (by using the \nSegment Mapper\n component).\n\n\n\n\n\n\nSegment Container creates a \nStreamSegmentAppendOperation\n with the input data and sends it to the \nDurable Log\n.\n\n\n\n\n\n\nDurable Log\n takes the Append Operation and processes it according to the algorithm described in the\u00a0\nDurable Log\n section.  \n\n\nPuts it in its Operation Queue.\n\n\nOperation Processor pulls all operations off the Queue.\n\n\nOperation Processor uses the Data Frame Builder to construct Data Frames with the operations it has.\n\n\nData Frame Builder writes the Data Frame to the \nDurable Data Log\n\n\nMetadata is updated and the Operation is added to the \nMemory Operation Log\n.\n\n\nThe above process is asynchronous, which means the Operation Processor will have multiple Data Frames in flight (not illustrated). \n\n\n\n\n\n\nUpon completion:\n\n\nSuccess\n: Append Operation is added to the \nRead Index\n and any Future Reads for the Segment are activated, if they meet the trigger criteria.\n\n\nFailure\n: The call is failed with an appropriate error code.\n\n\n\n\n\n\n\n\nThis process applies for every single operation that the \nSegment Store\n supports. All \nmodify\n operations go through the Operation Processor and have a similar path.\n\n\nReads\n\n\n\n\nThe diagram above depicts these steps (note the step numbers may not match, but the order is the same):\n0. \nSegment Store\n receives read request with params: Segment Name, Read Offset, Max-Length.\n    1. \nSegment Store\n determines the ContainerId for the given Segment and verifies if it is Leader for given \nSegment Container\n. If not, it returns an appropriate error code.\n    2. \nSegment Store\n delegates request to the \nSegment Container\n instance.\n1. \nSegment Container\n verifies that the Segment belongs to that Container and that it actually exists. If not, it returns an appropriate error code to the client. \n    - During this process, it also get an existing Segment Id or assigns a new one (by using the \nSegment Mapper\n component).\n2. \nSegment Container\n delegates the request to its \nRead Index\n, which processes the read as described in the \nRead Index\n section, by issuing Reads from \nStorage\n (for data that is not in the \nCache\n), and querying/updating the \nCache\n as needed.\n\n\nSynchronization with Tier-2 (Storage Writer)\n\n\n\n\nThe diagram above depicts these steps (note the step numbers may not match, but the order is the same):\n\n\n\n\nThe \nStorage Writer\n's main loop is the sub-component that triggers all these operations\n\n\nRead next Operation from the \nDurable Log\n (in between each loop, the Writer remembers what the Sequence Number of the last processed Operation was)\n\n\nAll operations are processed, and added to the internal \nSegment Aggregators\n (one Aggregator per Segment)\n\n\nEligible Segment Aggregators are flushed to \nStorage\n (eligibility depends on the amount of data collected in each aggregator, and whether there are any Seal, Merge or Truncate operations queued up)\n\n\nEach time an Append operation is encountered, a trip to the \nRead Index\n may be required in order to get the contents of the append\n\n\n\n\n\n\nAfter every successful modification (write/seal/concat/truncate) to \nStorage\n, the \nContainer Metadata\n is updated to reflect the changes.\n\n\nThe \nDurable Log\n is truncated (if eligible).\n\n\n\n\nContainer Startup (Normal/Recovery)\n\n\n\n\nThe diagram above depicts these steps (note the step numbers may not match, but the order is the same):\n\n\n\n\nThe \nContainer Manager\n receives a request to start a Container in this instance of the \nSegment Store Service\n.\n\n\nIt creates, registers, and starts the Container.\n\n\n\n\n\n\nThe \nContainer\n starts the \nDurable Log\n component.\n\n\nDurable Log\n initiates the recovery process (coordinated by the \nRecovery Executor\n).\n\n\nRecovery Executor\n reads all Data Frames from \nDurable Data Log\n.\n\n\nDeserialized Operations from the read Data Frames are added to the \nMemory Operation Log\n.\n\n\nThe \nContainer Metadata\n is updated by means of the \nOperation Metadata Updater\n (same as the one used inside Operation Processor).\n\n\nThe \nRead Index\n is populated with the contents of those Operations that apply to it.\n\n\nThe \nContainer\n Starts the \nStorage Writer\n.\n\n\nThe \nStorage Writer\n's Main Loop starts processing operations from the \nDurable Log\n, and upon first encountering a new Segment, it reconciles its content (and metadata) with the reality that exists in \nStorage\n.\n\n\n\n\n\n\nAfter both the \nDurable Log\n and the \nStorage Writer\n have started, the \nContainer\n is ready to start accepting new external requests.",
            "title": "Segment Store Service"
        },
        {
            "location": "/segment-store-service/#pravega-segment-store-service",
            "text": "The Pravega Segment Store Service is a subsystem that lies at the heart of the entire Pravega deployment. It is the main access point for managing Stream Segments, providing the ability to create, delete and modify/access their contents. The Pravega Client communicates with the Pravega Stream Controller to figure out which Segments need to be used (for a Stream), and both the Stream Controller and the Client deal with the Segment Store Service to actually operate on them.  The basic idea behind the Segment Store Service is that it buffers incoming data in a very fast and durable append-only medium (Tier 1), and syncs it to a high-throughput (but not necessarily low latency) system (Tier 2) in the background, while aggregating multiple (smaller) operations to a Segment into a fewer (but larger) ones.  The Pravega Segment Store Service can provide the following guarantees:\n- Stream Segments that are unlimited in length, with append-only semantics, yet supporting arbitrary-offset reads.\n- No throughput degradation when performing small appends, regardless of the performance of the underlying Tier-2 storage system.\n- Multiple concurrent writers to the same Segment.\n - Order is guaranteed within the context of a single writer, but appends from multiple concurrent writers will be added in the order in which they were received (appends are atomic without interleaving their contents).\n- Writing to and reading from a Segment concurrently with relatively low latency between writing and reading.",
            "title": "Pravega Segment Store Service"
        },
        {
            "location": "/segment-store-service/#terminology",
            "text": "Throughout the rest of this document, we will use the following terminology:\n-  Stream Segment  or  Segment : A contiguous sequence of bytes. Similar to a file with no size limit. This is a part of a Stream, limited both temporally and laterally (by key). The scope of Streams and mapping Stream Segments to such Streams is beyond the purpose of this document.\n-  Tier-2 storage  or  Permanent Storage : The final resting place of the data.\n-  Tier-1 storage : Fast append storage, used for durably buffering incoming appends before distribution to Tier-2 Storage.\n-  Cache : A key-value local cache with no expectation of durability.\n-  Pravega Segment Store Service  or  Segment Store : The Service that this document describes. \n-  Transaction : A sequence of appends that are related to a Segment, which, if persisted, would make up a contiguous range of bytes within it. This is used for ingesting very large records or for accumulating data that may or may not be persisted into the Segment (but its fate cannot be determined until later in the future).\n    - Note that at the Pravega level, a Transaction applies to an entire Stream. In this document, a Transaction applies to a single Segment.",
            "title": "Terminology"
        },
        {
            "location": "/segment-store-service/#architecture",
            "text": "The  Segment Store  is made up of the following components:\n-  Pravega Node : a host running a Pravega Process.\n-  Stream Segment Container  (or  Segment Container ): A logical grouping of Stream Segments. The mapping of Segments to Containers is deterministic and does not require any persistent store; Segments are mapped to Containers via a hash function (based on the Segment's name).\n-  Durable Data Log Adapter  (or  DurableDataLog ): an abstraction layer for Tier-1 Storage.\n-  Storage Adapter : an abstraction layer for Tier-2 Storage.\n-  Cache : an abstraction layer for append data caching.\n-  Streaming Client : an API that can be used to communicate with the Pravega Segment Store.\n-  Segment Container Manager : a component that can be used to determine the lifecycle of Segment Containers on a Pravega Node. This is used to start or stop Segment Containers based on an external coordination service (such as the Pravega Controller).  The Segment Store handles writes by first writing them to a log (Durable Data Log) on a fast storage (SSDs preferably) and immediately acking back to the client after they have been persisted there. Subsequently, those writes are then aggregated into larger chunks\u00a0and written in the background to Tier-2 storage. Data for appends that have been acknowledged (and are in Tier-1) but not yet in Tier-2 is stored in the Cache (in addition to Tier-1). Once such data has been written to Tier-2 Storage, it may or may not be kept in the Cache, depending on a number of factors, such as Cache utilization/pressure and access patterns.  More details about each component described above can be found in the  Components  section (below).",
            "title": "Architecture"
        },
        {
            "location": "/segment-store-service/#system-diagram",
            "text": "In this image, we show the major components of the Segment Store (for simplicity, only one Segment Container is depicted). All Container components and major links between them (how they interact with each other) are shown. The  Container Metadata  component is not shown, because every other component communicates with it in one form or another, and adding it would only clutter the diagram.  More detailed diagrams can be found under the  Data Flow  section (below).",
            "title": "System Diagram"
        },
        {
            "location": "/segment-store-service/#components",
            "text": "",
            "title": "Components"
        },
        {
            "location": "/segment-store-service/#segment-containers",
            "text": "Segment Containers are a logical grouping of Segments, and are responsible for all operations on those Segments within their span. A Segment Container is made of multiple sub-components:\n-  Segment Container Metadata : A collection of Segment-specific metadata that describe the current state of each Segment (how much data in Tier-2, how much in Tier-1, whether it is sealed, etc.), as well as other misc info about each Container.\n-  Durable Log : The Container writes every operation it receives to this log and acks back only when the log says it has been accepted and durably persisted..\n-  Read Index :\u00a0An in-memory index of where data can be read from. The Container delegates all read requests to it, and it is responsible for fetching the data from wherever it is currently located (Cache, Tier-1 Storage or Tier-2 Storage).\n-  Cache :\u00a0Used to store data for appends that exist in Tier-1 only (not yet in Tier-2), as well as blocks of data that support reads.\n-  Storage Writer :\u00a0Processes the durable log operations and applies them to Tier-2 storage (in the order in which they were received). This component is also the one that coalesces multiple operations together, for better back-end throughput.",
            "title": "Segment Containers"
        },
        {
            "location": "/segment-store-service/#segment-container-metadata",
            "text": "The Segment Container Metadata is critical to the good functioning and synchronization of its components. This metadata is shared across all components and it comes at two levels: Container-wide Metadata and per-Segment Metadata. Each serves a different purpose and is described below.",
            "title": "Segment Container Metadata"
        },
        {
            "location": "/segment-store-service/#container-metadata",
            "text": "Each  Segment Container  needs to keep some general-purpose metadata that affects all operations inside the container:   Operation Sequence Number :\u00a0The largest sequence number assigned by the  Durable Log . Every time a new Operation is received and successfully processed by the  Durable Log , this number is incremented (its value never decreases or otherwise roll back, even if an operation failed to be persisted).  The Operation Sequence Number is guaranteed to be strict-monotonic increasing (no two Operations have the same value, and an Operation will always have a larger Sequence Number than all Operations before it).    Epoch : A number that is incremented every time a successful recovery (Container Start) happens. This value is durably incremented and stored as part of recovery and can be used for a number of cases (a good use is Tier-2 fencing for HDFS, which doesn't provide a good, native mechanism for that).  Active Segment Metadata : Information about each active Segment (see next section below). A Segment is active if it has had activity (read or write) recently and is currently loaded in memory. If a Segment is not used for a while, or if there are many Segments currently active, a Segment becomes inactive by having its outstanding metadata flushed to Tier-2 Storage and evicted from memory.  Tier-1 Metadata : Various pieces of information that can be used to accurately truncate the Tier-1 Storage Log once all operations prior to that point have been durably stored to Tier-2.  Checkpoints : Container Metadata is periodically checkpointed by having its entire snapshot (including Active Segments) serialized to Tier-1. A Checkpoint serves as a Truncation Point for Tier-1, meaning it contains all the updates that have been made to the Container via all the processed operations before it so we no longer need those operations in order to reconstruct the Metadata. If we truncate Tier-1 on a Checkpoint, then we can use information from Tier-2 and this Checkpoint to reconstruct what is in the Metadata previously, without relying on any operation prior to it in Tier-1.",
            "title": "Container Metadata"
        },
        {
            "location": "/segment-store-service/#segment-metadata",
            "text": "Each Segment Container needs to keep per-segment metadata, which it uses to keep track of the state of each segment as it processes operations for it. The metadata can be volatile (it can be fully rebuilt upon recovery) and contains the following properties for each segment that is currently in use:   Name  the name of the Segment.  Id : Internally assigned unique Segment Id. This is used to refer to Segments, which is preferred to the Name. This Id is sticky for the lifetime of the Segment, which means that even if the Segment becomes inactive, a future reactivation will have it mapped to the same Id.  StartOffset  (also known as  TruncationOffset ): the lowest offset of the data that is available for reading. A non-truncated segment will have Start Offset equal to 0, while subsequent Truncate operations will increase (but never decrease) this number.   StorageLength : the highest offset of the data that exists in Tier-2 Storage.  Length : the highest offset of the committed data in\u00a0Tier-1 Storage.  LastModified : the timestamp of the last processed (and acknowledged) append.  IsSealed : whether the Segment is closed for appends (this value may not have been applied to Tier-2 Storage yet).  IsSealedInStorage : whether the Segment is closed for appends (and this has been persisted in Tier-2 Storage).  IsMerged : whether this Segment has been merged into another one (but this has not yet been persisted in Tier-2 Storage). This only applies for Transactions. Once the merger is persisted into Tier-2, the Transaction Segment does not exist anymore (so  IsDeleted  will become true).  IsDeleted : whether the Segment is deleted or has recently been merged into another Segment. This only applies for recently deleted Segments, and not for Segments that never existed.   The following are\u00a0 always \u00a0true for any Segment:   StorageLength \u00a0<=\u00a0 Length  StartOffset \u00a0<=\u00a0 Length",
            "title": "Segment Metadata"
        },
        {
            "location": "/segment-store-service/#log-operations",
            "text": "A  Log Operation  is the basic unit that is enqueued in the Durable Log. It does not represent an action, per se, but is the base for several serializable operations (we can serialize multiple types of operations, not just Appends). Each Operation is the result of an external action (which denote the alteration of a Segment), or an internal trigger, such as Metadata maintenance operations.  Every Log Operation has the following elements:\n-  SequenceNumber : the unique sequence number assigned to this entry (see more under  Container Metadata ).  These are the various types of Log Operations: \n-  Storage Operations  represent operations that need to be applied to the underlying Tier-2 Storage:\n    -  StreamSegmentAppendOperation : Represents an Append to a particular Segment.\n    -  CachedStreamSegmentAppendOperation : Same as  StreamSegmentAppendOperation , but this is for internal use (instead of having an actual data payload, it points to a location in the cache from where the data can be retrieved).\n    -  StreamSegmentSealOperation :\u00a0When processed, it sets a flag in the in-memory metadata that no more appends can be received. When the\u00a0Storage Writer\u00a0processes it, it marks the Segment as read-only in Tier-2 Storage.\n    -  StreamSegmentTruncateOperation : Truncates a Segment at a particular offset. This causes the Segment's  StartOffset  to change.\n    -  MergeTransactionOperation : Indicates that a Transaction is to be merged into its parent Segment.\n-  Metadata Operations  are auxiliary operations that indicate a change to the Container Metadata. They can be the result of an external operation (we received an request for a Segment we never knew about before, so we must assign a unique Id to it) or to snapshot the entire Metadata (which helps with recovery and cleaning up Tier-1 Storage). The purpose of the  Metadata Operations  is to reduce the amount of time needed for failover recovery (when needed):\n    -  StreamSegmentMapOperation :\u00a0maps an Id to a Segment Name.\n    -  TransactionMapOperation :\u00a0maps an Id to a Transaction and to its Parent Segment.\n    -  UpdateAttributesOperation : Updates any attributes on a Segment.\n    -  MetadataCheckpoint  includes an entire snapshot of the Metadata. This can be useful when during recovery - this contains all metadata up to this point, which is a sufficient base for all operations after it.",
            "title": "Log Operations"
        },
        {
            "location": "/segment-store-service/#durable-log",
            "text": "The  Durable Log  is the central component that handles all Log Operations. All Operations (which are created by the Container) are added to the Durable Log, which processes them in the order in which they were received. It is made up of a few other components, all of which are working towards a single goal of processing all incoming operations as quickly as possible, without compromising data integrity.",
            "title": "Durable Log"
        },
        {
            "location": "/segment-store-service/#information-flow-in-the-durable-log",
            "text": "All received operations are added to an  Operation Queue   (the caller receives a Future which will be completed when the operation is durably persisted)  The  Operation Processor  picks all operations currently available in the queue (if the queue is empty, it will wait until at least one Operation is added).  The  Operation Processor  runs as a continuous loop (in a background thread), and has four main stages.  Dequeue all outstanding Operations from the Operation Queue (described above)  Pre-process the Operations (validate that they are correct and would not cause undesired behavior, assign offsets (where needed), assign Sequence Numbers, etc.)  Write the operations to a  Data Frame Builder , which serializes and packs the operations in  Data Frames . Once a Data Frame is complete (full or no more operations to add), the Data Frame Builder sends the Data Frame to the _Durable Data Log. Note that an Operation may span multiple DataFrames - the goal is to make best use of the Durable Data Log throughput capacity by making writes as large as possible (but also taking into account that there may be a maximum size limit per write).    When a DataFrame has been durably persisted in the Durable Data Log, the Operation Processor post-processes all operations that were fully written so far (adds them to in-memory structures, updates indices, etc.) and completes the Futures associated with them.  The  Operation Processor  works asynchronously, in that it does not wait for a particular Data Frame to be written before starting another one and sending it to the Durable Data Log. As such, multiple DataFrames may be in flight (but in a specific order), and the Operation Processor relies on certain ordering guarantees from the Durable Data Log (if a particular DataFrame was acked, then all DataFrames prior to that were also committed successfully, in the right order).    The Operation Processor does not do any write throttling (it leaves that to the Durable Data Log implementation), but it control the size of the Data Frames that get sent to it.",
            "title": "Information Flow in the Durable Log"
        },
        {
            "location": "/segment-store-service/#truncation",
            "text": "Based on supplied configuration, the Durable Log auto-adds a special kind of operation, named  MetadataCheckpointOperation . This operation, when processed by the Operation Processor, collects a snapshot of the entire Container Metadata and serializes it to the Durable Data Log. This special Operation marks a  Truncation Point  - a place in the stream of Log Operations where we can issue Truncate operations. It is very important that after every truncation, the first operation to be found in the log is a  MetadataCheckpointOperation , because without the prior operations to reconstruct metadata, this is the only way to be able to process subsequent operations.  Note: Durable Data Log (Tier-1) truncation should not be confused with Segment Truncation. They serve different purposes and are applied to different targets.",
            "title": "Truncation"
        },
        {
            "location": "/segment-store-service/#operation-processor",
            "text": "The  Operation Processor  is a sub-component of the Durable Log that deals with incoming Log Operations. Its purpose is to validate, persist, and update Metadata and other internal structures based on the contents of each operation.",
            "title": "Operation Processor"
        },
        {
            "location": "/segment-store-service/#operation-metadata-updater",
            "text": "The  Operation Metadata Updater  is a sub-component of the Durable Log that is responsible with validating operations based on the current state of the metadata, as well as update the Metadata after a successful committal of an operation. Internally it has various mechanisms to cope with failures, and it can rollback certain changes in failure situations.",
            "title": "Operation Metadata Updater"
        },
        {
            "location": "/segment-store-service/#durable-data-log",
            "text": "The _Durable Data Log is an abstraction layer to an external component that provides append-only semantics. It is supposed to be a system which provides very fast appends to a log, that guarantees the durability and consistency of the written data. The read performance is not so much a factor, because we do not read directly from this component - we only read from it when we need to recover the contents of the Durable Log.  As explained above, Log Operations are serialized into Data Frames (with a single Operation able to span multiple such Frames if needed), and these Data Frames are then serialized as entries into this Durable Data Log. This is used only as a fail-safe, and we only need to read these Frames back if we need to perform recovery (in which case we need to deserialize all Log Operations contained in them, in the same order in which they were received).",
            "title": "Durable Data Log"
        },
        {
            "location": "/segment-store-service/#in-memory-operation-log",
            "text": "The  In-Memory Operation Log  contains committed (and replicated) Log Operations\u00a0in the exact same order as they were added to the Durable Data Log. While the Durable Data Log contains a sequence of Data Frames (which contain serializations of Operations), the Memory Log contains the actual Operations, which can be used throughout the Durable Log (and the Storage Writer).  The Memory Log is essentially a chain of Log Operations\u00a0ordered by the time when the Operation\u00a0was received. We always add at one end, and we remove\u00a0from the other. When we truncate the Durable Data Log the Memory Log is also truncated at the same location.",
            "title": "In-Memory Operation Log"
        },
        {
            "location": "/segment-store-service/#read-index",
            "text": "The  Read Index  helps the Segment Container perform reads from streams at arbitrary offsets. While the Durable Log records (and can only replay) data in the order in which it is received, the Read Index can access the data in a random fashion. The Read Index is made of multiple  Segment Read Indices  (one per live segment).  The\u00a0 Segment Read Index \u00a0is a data structure that is used to serve reads from memory, as well as pull data from Tier-2 Storage and provide  Future Reads  (tail reads) when data is not yet available. When a read request is received, the Segment Read Index returns a read iterator that will return data as long as the read request parameters have not yet been satisfied. The iterator will either fetch data that is immediately available in memory, or request data from Tier-2 storage (and bring it to memory) or, if it reached the current end of the Segment, return a Future that will be completed when new data is added (thus providing tailing or future reads).  At the heart of the  Segment Read Index  lies a sorted index of entries (indexed by their start offsets) which is used to locate the requested data when needed. The index itself is implemented by a custom balanced binary search tree (AVL Tree to be more precise) with a goal of minimizing memory usage while not sacrificing insert or access performance. The entries themselves do not contain data, rather some small amount of metadata that is used to locate the data in the  Cache  and to determine usage patterns (good for cache evictions).",
            "title": "Read Index"
        },
        {
            "location": "/segment-store-service/#cache",
            "text": "The  Cache  is a component where all data (whether from new appends or that was pulled from Tier-2 storage) is stored. It is a key-value store entirely managed by the Read Index.   The Cache is defined as an abstraction layer, and there are two implementations of it:\n- In-memory implementation (via a  HashMap ). This is currently only used for unit tests.\n- Memory-disk hybrid, powered by  RocksDB . This is the preferred (and default) implementation, since it is not limited by available heap space or machine RAM. Performance is comparable to the in-memory implementation.",
            "title": "Cache"
        },
        {
            "location": "/segment-store-service/#storage-writer",
            "text": "Pravega is by no means the final resting place of the data, nor is it meant to be a storage service. The Tier-2 Storage is where we want data to be in the long term and Pravega is only used to store a very short tail-end of it (using Tier-1 Storage), enough to make appends fast and aggregate them into bigger chunks for committal to Tier-2 Storage. In order to perform this, it needs another component (Storage Writer) that reads data from the Durable Log in the order in which it was received, aggregates it, and sends it to Tier-2 Storage.  Just like the Durable Log, there is one Storage Writer\u00a0per Segment Container. Each Writer\u00a0reads Log\u00a0Operations from the In-Memory Operation Log (exposed via the  read()  method in the Durable Log) in the order they were processed. It keeps track of the last read item by means of its Sequence Number. This state is not persisted, and upon recovery, it can just start from the beginning of the available Durable Log.  The Storage Writer can process any Storage Operation (Append, Seal, Merge), and since Pravega is the sole actor modifying such data in Tier-2, it applies them without restraint. It has several mechanisms to detect and recover from possible data loss or external actors modifying data concurrently.",
            "title": "Storage Writer"
        },
        {
            "location": "/segment-store-service/#integration-with-controller",
            "text": "Actual methods for how Segment Containers are mapped to hosts and what rules are used for moving from from one to another are beyond the scope of this document. Here, we just describe how the Segment Store Service interacts with the  Controller  and how it manages the lifecycle of Segment Containers based on external events.",
            "title": "Integration with Controller"
        },
        {
            "location": "/segment-store-service/#segment-container-manager",
            "text": "Each instance of a Segment Store Service needs a  Segment Container Manager . The role of this component is to manage the lifecycle of the Segment Containers that are assigned to that node (service). It performs the following duties:\n- Connects to the Controller Service-Side Client (i.e., a client that deals only with Segment Container events, and not with the management of Streams and listens to all changes that pertain to Containers that pertain to its own instance.\n- When it receives a notification that it needs to start a Segment Container for a particular Container Id, it initiates the process of bootstrapping such an object. It does not notify the requesting client of success until the operation completes without error.\n- When it receives a notification that it needs to stop a Segment Container for a particular Container Id, it initiates the process of shutting it down. It does not notify the requesting client of success until the operation completes without error.\n- If the Segment Container shuts down unexpectedly (whether during Start, or during its normal operation), it will not attempt to restart it locally; instead it will notify the Controller of the fact.",
            "title": "Segment Container Manager"
        },
        {
            "location": "/segment-store-service/#storage-abstractions",
            "text": "The Segment Store was not designed with particular implementations for Tier-1 or Tier-2 in mind. Instead, all these components have been abstracted out in simple, well-defined interfaces, which can be implemented against any standard file system (Tier-2) or append-only log system (Tier-1).   Possible candidates for Tier-1 storage:\n-  Apache BookKeeper  (preferred, adapter is fully implemented as part of Pravega)\n- Non-durable, non-replicated solutions:\n    - In-Memory (Single node deployment only - Pravega becomes a volatile buffer for Tier-2 storage; data loss is unavoidable and unrecoverable from in the case of process crash or system restart).\n        - This is used for unit test only.\n    - Local File System (Single node deployment only - Pravega becomes a semi-durable buffer for Tier-2 storage; data loss is unavoidable and unrecoverable from in the case of complete node failure)  Possible candidates for Tier-2 storage:\n-  HDFS  (implementation available)\n-  Extended S3  (implementation available)\n-  NFS  (general  FileSystem ) (implementation available)\n- In-Memory (Single node deployment only - limited usefulness; data loss is unavoidable and unrecoverable from in the case of process crash or system restart) \n    - This is used for unit test only.  A note about  Tier-2 Truncation :\n- The Segment Store supports Segment truncation at a particular offset, which means that, once that request is complete, no offset below that one will be available for reading.\n- This above is only a metadata update operation, however this also needs to be supported by Tier-2 so that the truncated data is physically deleted from it.\n- If a Tier-2 implementation does not natively support truncation from the beginning of a file with offset preservation (i.e., a Segment of length 100 is truncated at offset 50, then offsets 0..49 are deleted, but offsets 50-99 are available and are not shifted down), then the  Segment Store  provides a wrapper on top of a generic Tier-2 implementation that can do that\n- The  RollingStorage  Tier-2 wrapper splits a Segment into multiple  Segment Chunks  and exposes them as a single Segment to the upper layers.  Segment Chunks  that have been truncated out are deleted automatically. This is not a very precise application (since it relies heavily on a rollover policy dictating granularity), but it is a practical solution for those cases when the real Tier-2 implementation does not provide the features that we need.",
            "title": "Storage Abstractions"
        },
        {
            "location": "/segment-store-service/#data-flow",
            "text": "Here are a few examples on how data flows inside the Pravega Segment Store Service.",
            "title": "Data Flow"
        },
        {
            "location": "/segment-store-service/#appends",
            "text": "The diagram above depicts these steps (note the step numbers may not match, but the order is the same):   Segment Store  receives append request with params: Segment Name, Payload and AttributeUpdates.  Segment Store  determines the ContainerId for the given Segment and verifies that the  Segment Container  is registered locally. If not, it returns an appropriate error code.  Segment Store  delegates request to the appropriate  Segment Container  instance.  Segment Container  verifies that the Segment belongs to the Segment Container and that the Segment actually exists. If not, it returns an appropriate error code.   During this process, it also get an existing Segment Id or assigns a new one (by using the  Segment Mapper  component).    Segment Container creates a  StreamSegmentAppendOperation  with the input data and sends it to the  Durable Log .    Durable Log  takes the Append Operation and processes it according to the algorithm described in the\u00a0 Durable Log  section.    Puts it in its Operation Queue.  Operation Processor pulls all operations off the Queue.  Operation Processor uses the Data Frame Builder to construct Data Frames with the operations it has.  Data Frame Builder writes the Data Frame to the  Durable Data Log  Metadata is updated and the Operation is added to the  Memory Operation Log .  The above process is asynchronous, which means the Operation Processor will have multiple Data Frames in flight (not illustrated).     Upon completion:  Success : Append Operation is added to the  Read Index  and any Future Reads for the Segment are activated, if they meet the trigger criteria.  Failure : The call is failed with an appropriate error code.     This process applies for every single operation that the  Segment Store  supports. All  modify  operations go through the Operation Processor and have a similar path.",
            "title": "Appends"
        },
        {
            "location": "/segment-store-service/#reads",
            "text": "The diagram above depicts these steps (note the step numbers may not match, but the order is the same):\n0.  Segment Store  receives read request with params: Segment Name, Read Offset, Max-Length.\n    1.  Segment Store  determines the ContainerId for the given Segment and verifies if it is Leader for given  Segment Container . If not, it returns an appropriate error code.\n    2.  Segment Store  delegates request to the  Segment Container  instance.\n1.  Segment Container  verifies that the Segment belongs to that Container and that it actually exists. If not, it returns an appropriate error code to the client. \n    - During this process, it also get an existing Segment Id or assigns a new one (by using the  Segment Mapper  component).\n2.  Segment Container  delegates the request to its  Read Index , which processes the read as described in the  Read Index  section, by issuing Reads from  Storage  (for data that is not in the  Cache ), and querying/updating the  Cache  as needed.",
            "title": "Reads"
        },
        {
            "location": "/segment-store-service/#synchronization-with-tier-2-storage-writer",
            "text": "The diagram above depicts these steps (note the step numbers may not match, but the order is the same):   The  Storage Writer 's main loop is the sub-component that triggers all these operations  Read next Operation from the  Durable Log  (in between each loop, the Writer remembers what the Sequence Number of the last processed Operation was)  All operations are processed, and added to the internal  Segment Aggregators  (one Aggregator per Segment)  Eligible Segment Aggregators are flushed to  Storage  (eligibility depends on the amount of data collected in each aggregator, and whether there are any Seal, Merge or Truncate operations queued up)  Each time an Append operation is encountered, a trip to the  Read Index  may be required in order to get the contents of the append    After every successful modification (write/seal/concat/truncate) to  Storage , the  Container Metadata  is updated to reflect the changes.  The  Durable Log  is truncated (if eligible).",
            "title": "Synchronization with Tier-2 (Storage Writer)"
        },
        {
            "location": "/segment-store-service/#container-startup-normalrecovery",
            "text": "The diagram above depicts these steps (note the step numbers may not match, but the order is the same):   The  Container Manager  receives a request to start a Container in this instance of the  Segment Store Service .  It creates, registers, and starts the Container.    The  Container  starts the  Durable Log  component.  Durable Log  initiates the recovery process (coordinated by the  Recovery Executor ).  Recovery Executor  reads all Data Frames from  Durable Data Log .  Deserialized Operations from the read Data Frames are added to the  Memory Operation Log .  The  Container Metadata  is updated by means of the  Operation Metadata Updater  (same as the one used inside Operation Processor).  The  Read Index  is populated with the contents of those Operations that apply to it.  The  Container  Starts the  Storage Writer .  The  Storage Writer 's Main Loop starts processing operations from the  Durable Log , and upon first encountering a new Segment, it reconciles its content (and metadata) with the reality that exists in  Storage .    After both the  Durable Log  and the  Storage Writer  have started, the  Container  is ready to start accepting new external requests.",
            "title": "Container Startup (Normal/Recovery)"
        },
        {
            "location": "/javadoc/",
            "text": "Java API Reference\n\n\nClients\n\n\nA Writer is a client that creates Events and publishes them into Streams.\nA Reader is a client that Consumes events from Streams.\nWe provide a Java library, which implements a convenient API for Writer and Reader applications to use.  The client library encapsulates the wire protocol that is used to convey requests and responses between Pravega Clients and the Pravega service.\n\n\nWriter and Reader API",
            "title": "Java API Reference"
        },
        {
            "location": "/javadoc/#java-api-reference",
            "text": "",
            "title": "Java API Reference"
        },
        {
            "location": "/javadoc/#clients",
            "text": "A Writer is a client that creates Events and publishes them into Streams.\nA Reader is a client that Consumes events from Streams.\nWe provide a Java library, which implements a convenient API for Writer and Reader applications to use.  The client library encapsulates the wire protocol that is used to convey requests and responses between Pravega Clients and the Pravega service.  Writer and Reader API",
            "title": "Clients"
        },
        {
            "location": "/rest/restapis/",
            "text": "Pravega Controller APIs\n\n\n\n\nOverview\n\n\nList of admin REST APIs for the pravega controller service.\n\n\nVersion information\n\n\nVersion\n : 0.0.1\n\n\nLicense information\n\n\nLicense\n : Apache 2.0\n\n\nLicense URL\n : http://www.apache.org/licenses/LICENSE-2.0\n\n\nTerms of service\n : null\n\n\nURI scheme\n\n\nBasePath\n : /v1\n\n\nSchemes\n : HTTP\n\n\nTags\n\n\n\n\nReaderGroups : Reader group related APIs\n\n\nScopes : Scope related APIs\n\n\nStreams : Stream related APIs\n\n\n\n\n\n\nPaths\n\n\n\n\nPOST /scopes\n\n\nDescription\n\n\nCreate a new scope\n\n\nParameters\n\n\n\n\n\n\n\n\nType\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nBody\n\n\nCreateScopeRequest\n  \nrequired\n\n\nThe scope configuration\n\n\nCreateScopeRequest\n\n\n\n\n\n\n\n\n\n\nCreateScopeRequest\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nscopeName\n  \noptional\n\n\nExample\n : \n\"string\"\n\n\nstring\n\n\n\n\n\n\n\n\nResponses\n\n\n\n\n\n\n\n\nHTTP Code\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\n201\n\n\nSuccessfully created the scope\n\n\nScopeProperty\n\n\n\n\n\n\n409\n\n\nScope with the given name already exists\n\n\nNo Content\n\n\n\n\n\n\n500\n\n\nInternal server error while creating a scope\n\n\nNo Content\n\n\n\n\n\n\n\n\nConsumes\n\n\n\n\napplication/json\n\n\n\n\nProduces\n\n\n\n\napplication/json\n\n\n\n\nTags\n\n\n\n\nScopes\n\n\n\n\nExample HTTP request\n\n\nRequest path\n\n\n/scopes\n\n\n\n\nRequest body\n\n\njson :\n{\n  \"scopeName\" : \"string\"\n}\n\n\n\n\nExample HTTP response\n\n\nResponse 201\n\n\njson :\n{\n  \"scopeName\" : \"string\"\n}\n\n\n\n\n\n\nGET /scopes\n\n\nDescription\n\n\nList all available scopes in pravega\n\n\nResponses\n\n\n\n\n\n\n\n\nHTTP Code\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\n200\n\n\nList of currently available scopes\n\n\nScopesList\n\n\n\n\n\n\n500\n\n\nInternal server error while fetching list of scopes\n\n\nNo Content\n\n\n\n\n\n\n\n\nProduces\n\n\n\n\napplication/json\n\n\n\n\nTags\n\n\n\n\nScopes\n\n\n\n\nExample HTTP request\n\n\nRequest path\n\n\n/scopes\n\n\n\n\nExample HTTP response\n\n\nResponse 200\n\n\njson :\n{\n  \"scopes\" : [ {\n    \"scopeName\" : \"string\"\n  } ]\n}\n\n\n\n\n\n\nGET /scopes/{scopeName}\n\n\nDescription\n\n\nRetrieve details of an existing scope\n\n\nParameters\n\n\n\n\n\n\n\n\nType\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nPath\n\n\nscopeName\n  \nrequired\n\n\nScope name\n\n\nstring\n\n\n\n\n\n\n\n\nResponses\n\n\n\n\n\n\n\n\nHTTP Code\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\n200\n\n\nSuccessfully retrieved the scope details\n\n\nScopeProperty\n\n\n\n\n\n\n404\n\n\nScope with the given name not found\n\n\nNo Content\n\n\n\n\n\n\n500\n\n\nInternal server error while fetching scope details\n\n\nNo Content\n\n\n\n\n\n\n\n\nProduces\n\n\n\n\napplication/json\n\n\n\n\nTags\n\n\n\n\nScopes\n\n\n\n\nExample HTTP request\n\n\nRequest path\n\n\n/scopes/string\n\n\n\n\nExample HTTP response\n\n\nResponse 200\n\n\njson :\n{\n  \"scopeName\" : \"string\"\n}\n\n\n\n\n\n\nDELETE /scopes/{scopeName}\n\n\nDescription\n\n\nDelete a scope\n\n\nParameters\n\n\n\n\n\n\n\n\nType\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nPath\n\n\nscopeName\n  \nrequired\n\n\nScope name\n\n\nstring\n\n\n\n\n\n\n\n\nResponses\n\n\n\n\n\n\n\n\nHTTP Code\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\n204\n\n\nSuccessfully deleted the scope\n\n\nNo Content\n\n\n\n\n\n\n404\n\n\nScope not found\n\n\nNo Content\n\n\n\n\n\n\n412\n\n\nCannot delete scope since it has non-empty list of streams\n\n\nNo Content\n\n\n\n\n\n\n500\n\n\nInternal server error while deleting a scope\n\n\nNo Content\n\n\n\n\n\n\n\n\nTags\n\n\n\n\nScopes\n\n\n\n\nExample HTTP request\n\n\nRequest path\n\n\n/scopes/string\n\n\n\n\n\n\nGET /scopes/{scopeName}/readergroups\n\n\nDescription\n\n\nList reader groups within the given scope\n\n\nParameters\n\n\n\n\n\n\n\n\nType\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nPath\n\n\nscopeName\n  \nrequired\n\n\nScope name\n\n\nstring\n\n\n\n\n\n\n\n\nResponses\n\n\n\n\n\n\n\n\nHTTP Code\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\n200\n\n\nList of all reader groups configured for the given scope\n\n\nReaderGroupsList\n\n\n\n\n\n\n404\n\n\nScope not found\n\n\nNo Content\n\n\n\n\n\n\n500\n\n\nInternal server error while fetching the list of reader groups for the given scope\n\n\nNo Content\n\n\n\n\n\n\n\n\nProduces\n\n\n\n\napplication/json\n\n\n\n\nTags\n\n\n\n\nReaderGroups\n\n\n\n\nExample HTTP request\n\n\nRequest path\n\n\n/scopes/string/readergroups\n\n\n\n\nExample HTTP response\n\n\nResponse 200\n\n\njson :\n{\n  \"readerGroups\" : [ \"object\" ]\n}\n\n\n\n\n\n\nGET /scopes/{scopeName}/readergroups/{readerGroupName}\n\n\nDescription\n\n\nFetch the properties of an existing reader group\n\n\nParameters\n\n\n\n\n\n\n\n\nType\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nPath\n\n\nreaderGroupName\n  \nrequired\n\n\nReader group name\n\n\nstring\n\n\n\n\n\n\nPath\n\n\nscopeName\n  \nrequired\n\n\nScope name\n\n\nstring\n\n\n\n\n\n\n\n\nResponses\n\n\n\n\n\n\n\n\nHTTP Code\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\n200\n\n\nFound reader group properties\n\n\nReaderGroupProperty\n\n\n\n\n\n\n404\n\n\nScope or reader group with given name not found\n\n\nNo Content\n\n\n\n\n\n\n500\n\n\nInternal server error while fetching reader group details\n\n\nNo Content\n\n\n\n\n\n\n\n\nProduces\n\n\n\n\napplication/json\n\n\n\n\nTags\n\n\n\n\nReaderGroups\n\n\n\n\nExample HTTP request\n\n\nRequest path\n\n\n/scopes/string/readergroups/string\n\n\n\n\nExample HTTP response\n\n\nResponse 200\n\n\njson :\n{\n  \"scopeName\" : \"string\",\n  \"readerGroupName\" : \"string\",\n  \"streamList\" : [ \"string\" ],\n  \"onlineReaderIds\" : [ \"string\" ]\n}\n\n\n\n\n\n\nPOST /scopes/{scopeName}/streams\n\n\nDescription\n\n\nCreate a new stream\n\n\nParameters\n\n\n\n\n\n\n\n\nType\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nPath\n\n\nscopeName\n  \nrequired\n\n\nScope name\n\n\nstring\n\n\n\n\n\n\nBody\n\n\nCreateStreamRequest\n  \nrequired\n\n\nThe stream configuration\n\n\nCreateStreamRequest\n\n\n\n\n\n\n\n\n\n\nCreateStreamRequest\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nretentionPolicy\n  \noptional\n\n\nExample\n : \n\"[retentionconfig](#retentionconfig)\"\n\n\nRetentionConfig\n\n\n\n\n\n\nscalingPolicy\n  \noptional\n\n\nExample\n : \n\"[scalingconfig](#scalingconfig)\"\n\n\nScalingConfig\n\n\n\n\n\n\nstreamName\n  \noptional\n\n\nExample\n : \n\"string\"\n\n\nstring\n\n\n\n\n\n\n\n\nResponses\n\n\n\n\n\n\n\n\nHTTP Code\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\n201\n\n\nSuccessfully created the stream with the given configuration\n\n\nStreamProperty\n\n\n\n\n\n\n404\n\n\nScope not found\n\n\nNo Content\n\n\n\n\n\n\n409\n\n\nStream with given name already exists\n\n\nNo Content\n\n\n\n\n\n\n500\n\n\nInternal server error while creating a stream\n\n\nNo Content\n\n\n\n\n\n\n\n\nConsumes\n\n\n\n\napplication/json\n\n\n\n\nProduces\n\n\n\n\napplication/json\n\n\n\n\nTags\n\n\n\n\nStreams\n\n\n\n\nExample HTTP request\n\n\nRequest path\n\n\n/scopes/string/streams\n\n\n\n\nRequest body\n\n\njson :\n{\n  \"streamName\" : \"string\",\n  \"scalingPolicy\" : {\n    \"type\" : \"string\",\n    \"targetRate\" : 0,\n    \"scaleFactor\" : 0,\n    \"minSegments\" : 0\n  },\n  \"retentionPolicy\" : {\n    \"type\" : \"string\",\n    \"value\" : 0\n  }\n}\n\n\n\n\nExample HTTP response\n\n\nResponse 201\n\n\njson :\n{\n  \"scopeName\" : \"string\",\n  \"streamName\" : \"string\",\n  \"scalingPolicy\" : {\n    \"type\" : \"string\",\n    \"targetRate\" : 0,\n    \"scaleFactor\" : 0,\n    \"minSegments\" : 0\n  },\n  \"retentionPolicy\" : {\n    \"type\" : \"string\",\n    \"value\" : 0\n  }\n}\n\n\n\n\n\n\nGET /scopes/{scopeName}/streams\n\n\nDescription\n\n\nList streams within the given scope\n\n\nParameters\n\n\n\n\n\n\n\n\nType\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nPath\n\n\nscopeName\n  \nrequired\n\n\nScope name\n\n\nstring\n\n\n\n\n\n\nQuery\n\n\nshowInternalStreams\n  \noptional\n\n\nOptional flag whether to display system created streams. If not specified only user created streams will be returned\n\n\nstring\n\n\n\n\n\n\n\n\nResponses\n\n\n\n\n\n\n\n\nHTTP Code\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\n200\n\n\nList of all streams configured for the given scope\n\n\nStreamsList\n\n\n\n\n\n\n404\n\n\nScope not found\n\n\nNo Content\n\n\n\n\n\n\n500\n\n\nInternal server error while fetching the list of streams for the given scope\n\n\nNo Content\n\n\n\n\n\n\n\n\nProduces\n\n\n\n\napplication/json\n\n\n\n\nTags\n\n\n\n\nStreams\n\n\n\n\nExample HTTP request\n\n\nRequest path\n\n\n/scopes/string/streams\n\n\n\n\nRequest query\n\n\njson :\n{\n  \"showInternalStreams\" : \"string\"\n}\n\n\n\n\nExample HTTP response\n\n\nResponse 200\n\n\njson :\n{\n  \"streams\" : [ {\n    \"scopeName\" : \"string\",\n    \"streamName\" : \"string\",\n    \"scalingPolicy\" : {\n      \"type\" : \"string\",\n      \"targetRate\" : 0,\n      \"scaleFactor\" : 0,\n      \"minSegments\" : 0\n    },\n    \"retentionPolicy\" : {\n      \"type\" : \"string\",\n      \"value\" : 0\n    }\n  } ]\n}\n\n\n\n\n\n\nGET /scopes/{scopeName}/streams/{streamName}\n\n\nDescription\n\n\nFetch the properties of an existing stream\n\n\nParameters\n\n\n\n\n\n\n\n\nType\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nPath\n\n\nscopeName\n  \nrequired\n\n\nScope name\n\n\nstring\n\n\n\n\n\n\nPath\n\n\nstreamName\n  \nrequired\n\n\nStream name\n\n\nstring\n\n\n\n\n\n\n\n\nResponses\n\n\n\n\n\n\n\n\nHTTP Code\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\n200\n\n\nFound stream properties\n\n\nStreamProperty\n\n\n\n\n\n\n404\n\n\nScope or stream with given name not found\n\n\nNo Content\n\n\n\n\n\n\n500\n\n\nInternal server error while fetching stream details\n\n\nNo Content\n\n\n\n\n\n\n\n\nProduces\n\n\n\n\napplication/json\n\n\n\n\nTags\n\n\n\n\nStreams\n\n\n\n\nExample HTTP request\n\n\nRequest path\n\n\n/scopes/string/streams/string\n\n\n\n\nExample HTTP response\n\n\nResponse 200\n\n\njson :\n{\n  \"scopeName\" : \"string\",\n  \"streamName\" : \"string\",\n  \"scalingPolicy\" : {\n    \"type\" : \"string\",\n    \"targetRate\" : 0,\n    \"scaleFactor\" : 0,\n    \"minSegments\" : 0\n  },\n  \"retentionPolicy\" : {\n    \"type\" : \"string\",\n    \"value\" : 0\n  }\n}\n\n\n\n\n\n\nPUT /scopes/{scopeName}/streams/{streamName}\n\n\nDescription\n\n\nUpdate configuration of an existing stream\n\n\nParameters\n\n\n\n\n\n\n\n\nType\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nPath\n\n\nscopeName\n  \nrequired\n\n\nScope name\n\n\nstring\n\n\n\n\n\n\nPath\n\n\nstreamName\n  \nrequired\n\n\nStream name\n\n\nstring\n\n\n\n\n\n\nBody\n\n\nUpdateStreamRequest\n  \nrequired\n\n\nThe new stream configuration\n\n\nUpdateStreamRequest\n\n\n\n\n\n\n\n\n\n\nUpdateStreamRequest\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nretentionPolicy\n  \noptional\n\n\nExample\n : \n\"[retentionconfig](#retentionconfig)\"\n\n\nRetentionConfig\n\n\n\n\n\n\nscalingPolicy\n  \noptional\n\n\nExample\n : \n\"[scalingconfig](#scalingconfig)\"\n\n\nScalingConfig\n\n\n\n\n\n\n\n\nResponses\n\n\n\n\n\n\n\n\nHTTP Code\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\n200\n\n\nSuccessfully updated the stream configuration\n\n\nStreamProperty\n\n\n\n\n\n\n404\n\n\nScope or stream with given name not found\n\n\nNo Content\n\n\n\n\n\n\n500\n\n\nInternal server error while updating the stream\n\n\nNo Content\n\n\n\n\n\n\n\n\nConsumes\n\n\n\n\napplication/json\n\n\n\n\nProduces\n\n\n\n\napplication/json\n\n\n\n\nTags\n\n\n\n\nStreams\n\n\n\n\nExample HTTP request\n\n\nRequest path\n\n\n/scopes/string/streams/string\n\n\n\n\nRequest body\n\n\njson :\n{\n  \"scalingPolicy\" : {\n    \"type\" : \"string\",\n    \"targetRate\" : 0,\n    \"scaleFactor\" : 0,\n    \"minSegments\" : 0\n  },\n  \"retentionPolicy\" : {\n    \"type\" : \"string\",\n    \"value\" : 0\n  }\n}\n\n\n\n\nExample HTTP response\n\n\nResponse 200\n\n\njson :\n{\n  \"scopeName\" : \"string\",\n  \"streamName\" : \"string\",\n  \"scalingPolicy\" : {\n    \"type\" : \"string\",\n    \"targetRate\" : 0,\n    \"scaleFactor\" : 0,\n    \"minSegments\" : 0\n  },\n  \"retentionPolicy\" : {\n    \"type\" : \"string\",\n    \"value\" : 0\n  }\n}\n\n\n\n\n\n\nDELETE /scopes/{scopeName}/streams/{streamName}\n\n\nDescription\n\n\nDelete a stream\n\n\nParameters\n\n\n\n\n\n\n\n\nType\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nPath\n\n\nscopeName\n  \nrequired\n\n\nScope name\n\n\nstring\n\n\n\n\n\n\nPath\n\n\nstreamName\n  \nrequired\n\n\nStream name\n\n\nstring\n\n\n\n\n\n\n\n\nResponses\n\n\n\n\n\n\n\n\nHTTP Code\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\n204\n\n\nSuccessfully deleted the stream\n\n\nNo Content\n\n\n\n\n\n\n404\n\n\nStream not found\n\n\nNo Content\n\n\n\n\n\n\n412\n\n\nCannot delete stream since it is not sealed\n\n\nNo Content\n\n\n\n\n\n\n500\n\n\nInternal server error while deleting the stream\n\n\nNo Content\n\n\n\n\n\n\n\n\nTags\n\n\n\n\nStreams\n\n\n\n\nExample HTTP request\n\n\nRequest path\n\n\n/scopes/string/streams/string\n\n\n\n\n\n\nGET /scopes/{scopeName}/streams/{streamName}/scaling-events\n\n\nDescription\n\n\nGet scaling events for a given datetime period.\n\n\nParameters\n\n\n\n\n\n\n\n\nType\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nPath\n\n\nscopeName\n  \nrequired\n\n\nScope name\n\n\nstring\n\n\n\n\n\n\nPath\n\n\nstreamName\n  \nrequired\n\n\nStream name\n\n\nstring\n\n\n\n\n\n\nQuery\n\n\nfrom\n  \nrequired\n\n\nParameter to display scaling events from that particular datetime. Input should be milliseconds from Jan 1 1970.\n\n\ninteger (int64)\n\n\n\n\n\n\nQuery\n\n\nto\n  \nrequired\n\n\nParameter to display scaling events to that particular datetime. Input should be milliseconds from Jan 1 1970.\n\n\ninteger (int64)\n\n\n\n\n\n\n\n\nResponses\n\n\n\n\n\n\n\n\nHTTP Code\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\n200\n\n\nSuccessfully fetched list of scaling events.\n\n\nScalingEventList\n\n\n\n\n\n\n404\n\n\nScope/Stream not found.\n\n\nNo Content\n\n\n\n\n\n\n500\n\n\nInternal Server error while fetching scaling events.\n\n\nNo Content\n\n\n\n\n\n\n\n\nProduces\n\n\n\n\napplication/json\n\n\n\n\nTags\n\n\n\n\nStreams\n\n\n\n\nExample HTTP request\n\n\nRequest path\n\n\n/scopes/string/streams/string/scaling-events\n\n\n\n\nRequest query\n\n\njson :\n{\n  \"from\" : 0,\n  \"to\" : 0\n}\n\n\n\n\nExample HTTP response\n\n\nResponse 200\n\n\njson :\n{\n  \"scalingEvents\" : [ {\n    \"timestamp\" : 0,\n    \"segmentList\" : [ {\n      \"number\" : 0,\n      \"startTime\" : 0,\n      \"keyStart\" : 0,\n      \"keyEnd\" : 0\n    } ]\n  } ]\n}\n\n\n\n\n\n\nPUT /scopes/{scopeName}/streams/{streamName}/state\n\n\nDescription\n\n\nUpdates the current state of the stream\n\n\nParameters\n\n\n\n\n\n\n\n\nType\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nPath\n\n\nscopeName\n  \nrequired\n\n\nScope name\n\n\nstring\n\n\n\n\n\n\nPath\n\n\nstreamName\n  \nrequired\n\n\nStream name\n\n\nstring\n\n\n\n\n\n\nBody\n\n\nUpdateStreamStateRequest\n  \nrequired\n\n\nThe state info to be updated\n\n\nStreamState\n\n\n\n\n\n\n\n\nResponses\n\n\n\n\n\n\n\n\nHTTP Code\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\n200\n\n\nSuccessfully updated the stream state\n\n\nStreamState\n\n\n\n\n\n\n404\n\n\nScope or stream with given name not found\n\n\nNo Content\n\n\n\n\n\n\n500\n\n\nInternal server error while updating the stream state\n\n\nNo Content\n\n\n\n\n\n\n\n\nConsumes\n\n\n\n\napplication/json\n\n\n\n\nProduces\n\n\n\n\napplication/json\n\n\n\n\nTags\n\n\n\n\nStreams\n\n\n\n\nExample HTTP request\n\n\nRequest path\n\n\n/scopes/string/streams/string/state\n\n\n\n\nRequest body\n\n\njson :\n{\n  \"streamState\" : \"string\"\n}\n\n\n\n\nExample HTTP response\n\n\nResponse 200\n\n\njson :\n{\n  \"streamState\" : \"string\"\n}\n\n\n\n\n\n\nDefinitions\n\n\n\n\nReaderGroupProperty\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nonlineReaderIds\n  \noptional\n\n\nExample\n : \n[ \"string\" ]\n\n\n< string > array\n\n\n\n\n\n\nreaderGroupName\n  \noptional\n\n\nExample\n : \n\"string\"\n\n\nstring\n\n\n\n\n\n\nscopeName\n  \noptional\n\n\nExample\n : \n\"string\"\n\n\nstring\n\n\n\n\n\n\nstreamList\n  \noptional\n\n\nExample\n : \n[ \"string\" ]\n\n\n< string > array\n\n\n\n\n\n\n\n\n\n\nReaderGroupsList\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nreaderGroups\n  \noptional\n\n\nExample\n : \n[ \"object\" ]\n\n\n< \nreaderGroups\n > array\n\n\n\n\n\n\n\n\n\n\nreaderGroups\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nreaderGroupName\n  \noptional\n\n\nExample\n : \n\"string\"\n\n\nstring\n\n\n\n\n\n\n\n\n\n\nRetentionConfig\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\ntype\n  \noptional\n\n\nExample\n : \n\"string\"\n\n\nenum (LIMITED_DAYS, LIMITED_SIZE_MB)\n\n\n\n\n\n\nvalue\n  \noptional\n\n\nExample\n : \n0\n\n\ninteger (int64)\n\n\n\n\n\n\n\n\n\n\nScaleMetadata\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nsegmentList\n  \noptional\n\n\nExample\n : \n[ \"[segment](#segment)\" ]\n\n\n< \nSegment\n > array\n\n\n\n\n\n\ntimestamp\n  \noptional\n\n\nExample\n : \n0\n\n\ninteger (int64)\n\n\n\n\n\n\n\n\n\n\nScalingConfig\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nminSegments\n  \noptional\n\n\nExample\n : \n0\n\n\ninteger (int32)\n\n\n\n\n\n\nscaleFactor\n  \noptional\n\n\nExample\n : \n0\n\n\ninteger (int32)\n\n\n\n\n\n\ntargetRate\n  \noptional\n\n\nExample\n : \n0\n\n\ninteger (int32)\n\n\n\n\n\n\ntype\n  \noptional\n\n\nExample\n : \n\"string\"\n\n\nenum (FIXED_NUM_SEGMENTS, BY_RATE_IN_KBYTES_PER_SEC, BY_RATE_IN_EVENTS_PER_SEC)\n\n\n\n\n\n\n\n\n\n\nScalingEventList\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nscalingEvents\n  \noptional\n\n\nExample\n : \n[ \"[scalemetadata](#scalemetadata)\" ]\n\n\n< \nScaleMetadata\n > array\n\n\n\n\n\n\n\n\n\n\nScopeProperty\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nscopeName\n  \noptional\n\n\nExample\n : \n\"string\"\n\n\nstring\n\n\n\n\n\n\n\n\n\n\nScopesList\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nscopes\n  \noptional\n\n\nExample\n : \n[ \"[scopeproperty](#scopeproperty)\" ]\n\n\n< \nScopeProperty\n > array\n\n\n\n\n\n\n\n\n\n\nSegment\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nkeyEnd\n  \noptional\n\n\nExample\n : \n0\n\n\ninteger (double)\n\n\n\n\n\n\nkeyStart\n  \noptional\n\n\nExample\n : \n0\n\n\ninteger (double)\n\n\n\n\n\n\nnumber\n  \noptional\n\n\nExample\n : \n0\n\n\ninteger (int32)\n\n\n\n\n\n\nstartTime\n  \noptional\n\n\nExample\n : \n0\n\n\ninteger (int64)\n\n\n\n\n\n\n\n\n\n\nStreamProperty\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nretentionPolicy\n  \noptional\n\n\nExample\n : \n\"[retentionconfig](#retentionconfig)\"\n\n\nRetentionConfig\n\n\n\n\n\n\nscalingPolicy\n  \noptional\n\n\nExample\n : \n\"[scalingconfig](#scalingconfig)\"\n\n\nScalingConfig\n\n\n\n\n\n\nscopeName\n  \noptional\n\n\nExample\n : \n\"string\"\n\n\nstring\n\n\n\n\n\n\nstreamName\n  \noptional\n\n\nExample\n : \n\"string\"\n\n\nstring\n\n\n\n\n\n\n\n\n\n\nStreamState\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nstreamState\n  \noptional\n\n\nExample\n : \n\"string\"\n\n\nenum (SEALED)\n\n\n\n\n\n\n\n\n\n\nStreamsList\n\n\n\n\n\n\n\n\nName\n\n\nDescription\n\n\nSchema\n\n\n\n\n\n\n\n\n\n\nstreams\n  \noptional\n\n\nExample\n : \n[ \"[streamproperty](#streamproperty)\" ]\n\n\n< \nStreamProperty\n > array",
            "title": "REST API - Controller"
        },
        {
            "location": "/rest/restapis/#pravega-controller-apis",
            "text": "",
            "title": "Pravega Controller APIs"
        },
        {
            "location": "/rest/restapis/#overview",
            "text": "List of admin REST APIs for the pravega controller service.",
            "title": "Overview"
        },
        {
            "location": "/rest/restapis/#version-information",
            "text": "Version  : 0.0.1",
            "title": "Version information"
        },
        {
            "location": "/rest/restapis/#license-information",
            "text": "License  : Apache 2.0  License URL  : http://www.apache.org/licenses/LICENSE-2.0  Terms of service  : null",
            "title": "License information"
        },
        {
            "location": "/rest/restapis/#uri-scheme",
            "text": "BasePath  : /v1  Schemes  : HTTP",
            "title": "URI scheme"
        },
        {
            "location": "/rest/restapis/#tags",
            "text": "ReaderGroups : Reader group related APIs  Scopes : Scope related APIs  Streams : Stream related APIs",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#paths",
            "text": "",
            "title": "Paths"
        },
        {
            "location": "/rest/restapis/#post-scopes",
            "text": "",
            "title": "POST /scopes"
        },
        {
            "location": "/rest/restapis/#description",
            "text": "Create a new scope",
            "title": "Description"
        },
        {
            "location": "/rest/restapis/#parameters",
            "text": "Type  Name  Description  Schema      Body  CreateScopeRequest    required  The scope configuration  CreateScopeRequest      CreateScopeRequest     Name  Description  Schema      scopeName    optional  Example  :  \"string\"  string",
            "title": "Parameters"
        },
        {
            "location": "/rest/restapis/#responses",
            "text": "HTTP Code  Description  Schema      201  Successfully created the scope  ScopeProperty    409  Scope with the given name already exists  No Content    500  Internal server error while creating a scope  No Content",
            "title": "Responses"
        },
        {
            "location": "/rest/restapis/#consumes",
            "text": "application/json",
            "title": "Consumes"
        },
        {
            "location": "/rest/restapis/#produces",
            "text": "application/json",
            "title": "Produces"
        },
        {
            "location": "/rest/restapis/#tags_1",
            "text": "Scopes",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#example-http-request",
            "text": "",
            "title": "Example HTTP request"
        },
        {
            "location": "/rest/restapis/#request-path",
            "text": "/scopes",
            "title": "Request path"
        },
        {
            "location": "/rest/restapis/#request-body",
            "text": "json :\n{\n  \"scopeName\" : \"string\"\n}",
            "title": "Request body"
        },
        {
            "location": "/rest/restapis/#example-http-response",
            "text": "",
            "title": "Example HTTP response"
        },
        {
            "location": "/rest/restapis/#response-201",
            "text": "json :\n{\n  \"scopeName\" : \"string\"\n}",
            "title": "Response 201"
        },
        {
            "location": "/rest/restapis/#get-scopes",
            "text": "",
            "title": "GET /scopes"
        },
        {
            "location": "/rest/restapis/#description_1",
            "text": "List all available scopes in pravega",
            "title": "Description"
        },
        {
            "location": "/rest/restapis/#responses_1",
            "text": "HTTP Code  Description  Schema      200  List of currently available scopes  ScopesList    500  Internal server error while fetching list of scopes  No Content",
            "title": "Responses"
        },
        {
            "location": "/rest/restapis/#produces_1",
            "text": "application/json",
            "title": "Produces"
        },
        {
            "location": "/rest/restapis/#tags_2",
            "text": "Scopes",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#example-http-request_1",
            "text": "",
            "title": "Example HTTP request"
        },
        {
            "location": "/rest/restapis/#request-path_1",
            "text": "/scopes",
            "title": "Request path"
        },
        {
            "location": "/rest/restapis/#example-http-response_1",
            "text": "",
            "title": "Example HTTP response"
        },
        {
            "location": "/rest/restapis/#response-200",
            "text": "json :\n{\n  \"scopes\" : [ {\n    \"scopeName\" : \"string\"\n  } ]\n}",
            "title": "Response 200"
        },
        {
            "location": "/rest/restapis/#get-scopesscopename",
            "text": "",
            "title": "GET /scopes/{scopeName}"
        },
        {
            "location": "/rest/restapis/#description_2",
            "text": "Retrieve details of an existing scope",
            "title": "Description"
        },
        {
            "location": "/rest/restapis/#parameters_1",
            "text": "Type  Name  Description  Schema      Path  scopeName    required  Scope name  string",
            "title": "Parameters"
        },
        {
            "location": "/rest/restapis/#responses_2",
            "text": "HTTP Code  Description  Schema      200  Successfully retrieved the scope details  ScopeProperty    404  Scope with the given name not found  No Content    500  Internal server error while fetching scope details  No Content",
            "title": "Responses"
        },
        {
            "location": "/rest/restapis/#produces_2",
            "text": "application/json",
            "title": "Produces"
        },
        {
            "location": "/rest/restapis/#tags_3",
            "text": "Scopes",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#example-http-request_2",
            "text": "",
            "title": "Example HTTP request"
        },
        {
            "location": "/rest/restapis/#request-path_2",
            "text": "/scopes/string",
            "title": "Request path"
        },
        {
            "location": "/rest/restapis/#example-http-response_2",
            "text": "",
            "title": "Example HTTP response"
        },
        {
            "location": "/rest/restapis/#response-200_1",
            "text": "json :\n{\n  \"scopeName\" : \"string\"\n}",
            "title": "Response 200"
        },
        {
            "location": "/rest/restapis/#delete-scopesscopename",
            "text": "",
            "title": "DELETE /scopes/{scopeName}"
        },
        {
            "location": "/rest/restapis/#description_3",
            "text": "Delete a scope",
            "title": "Description"
        },
        {
            "location": "/rest/restapis/#parameters_2",
            "text": "Type  Name  Description  Schema      Path  scopeName    required  Scope name  string",
            "title": "Parameters"
        },
        {
            "location": "/rest/restapis/#responses_3",
            "text": "HTTP Code  Description  Schema      204  Successfully deleted the scope  No Content    404  Scope not found  No Content    412  Cannot delete scope since it has non-empty list of streams  No Content    500  Internal server error while deleting a scope  No Content",
            "title": "Responses"
        },
        {
            "location": "/rest/restapis/#tags_4",
            "text": "Scopes",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#example-http-request_3",
            "text": "",
            "title": "Example HTTP request"
        },
        {
            "location": "/rest/restapis/#request-path_3",
            "text": "/scopes/string",
            "title": "Request path"
        },
        {
            "location": "/rest/restapis/#get-scopesscopenamereadergroups",
            "text": "",
            "title": "GET /scopes/{scopeName}/readergroups"
        },
        {
            "location": "/rest/restapis/#description_4",
            "text": "List reader groups within the given scope",
            "title": "Description"
        },
        {
            "location": "/rest/restapis/#parameters_3",
            "text": "Type  Name  Description  Schema      Path  scopeName    required  Scope name  string",
            "title": "Parameters"
        },
        {
            "location": "/rest/restapis/#responses_4",
            "text": "HTTP Code  Description  Schema      200  List of all reader groups configured for the given scope  ReaderGroupsList    404  Scope not found  No Content    500  Internal server error while fetching the list of reader groups for the given scope  No Content",
            "title": "Responses"
        },
        {
            "location": "/rest/restapis/#produces_3",
            "text": "application/json",
            "title": "Produces"
        },
        {
            "location": "/rest/restapis/#tags_5",
            "text": "ReaderGroups",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#example-http-request_4",
            "text": "",
            "title": "Example HTTP request"
        },
        {
            "location": "/rest/restapis/#request-path_4",
            "text": "/scopes/string/readergroups",
            "title": "Request path"
        },
        {
            "location": "/rest/restapis/#example-http-response_3",
            "text": "",
            "title": "Example HTTP response"
        },
        {
            "location": "/rest/restapis/#response-200_2",
            "text": "json :\n{\n  \"readerGroups\" : [ \"object\" ]\n}",
            "title": "Response 200"
        },
        {
            "location": "/rest/restapis/#get-scopesscopenamereadergroupsreadergroupname",
            "text": "",
            "title": "GET /scopes/{scopeName}/readergroups/{readerGroupName}"
        },
        {
            "location": "/rest/restapis/#description_5",
            "text": "Fetch the properties of an existing reader group",
            "title": "Description"
        },
        {
            "location": "/rest/restapis/#parameters_4",
            "text": "Type  Name  Description  Schema      Path  readerGroupName    required  Reader group name  string    Path  scopeName    required  Scope name  string",
            "title": "Parameters"
        },
        {
            "location": "/rest/restapis/#responses_5",
            "text": "HTTP Code  Description  Schema      200  Found reader group properties  ReaderGroupProperty    404  Scope or reader group with given name not found  No Content    500  Internal server error while fetching reader group details  No Content",
            "title": "Responses"
        },
        {
            "location": "/rest/restapis/#produces_4",
            "text": "application/json",
            "title": "Produces"
        },
        {
            "location": "/rest/restapis/#tags_6",
            "text": "ReaderGroups",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#example-http-request_5",
            "text": "",
            "title": "Example HTTP request"
        },
        {
            "location": "/rest/restapis/#request-path_5",
            "text": "/scopes/string/readergroups/string",
            "title": "Request path"
        },
        {
            "location": "/rest/restapis/#example-http-response_4",
            "text": "",
            "title": "Example HTTP response"
        },
        {
            "location": "/rest/restapis/#response-200_3",
            "text": "json :\n{\n  \"scopeName\" : \"string\",\n  \"readerGroupName\" : \"string\",\n  \"streamList\" : [ \"string\" ],\n  \"onlineReaderIds\" : [ \"string\" ]\n}",
            "title": "Response 200"
        },
        {
            "location": "/rest/restapis/#post-scopesscopenamestreams",
            "text": "",
            "title": "POST /scopes/{scopeName}/streams"
        },
        {
            "location": "/rest/restapis/#description_6",
            "text": "Create a new stream",
            "title": "Description"
        },
        {
            "location": "/rest/restapis/#parameters_5",
            "text": "Type  Name  Description  Schema      Path  scopeName    required  Scope name  string    Body  CreateStreamRequest    required  The stream configuration  CreateStreamRequest      CreateStreamRequest     Name  Description  Schema      retentionPolicy    optional  Example  :  \"[retentionconfig](#retentionconfig)\"  RetentionConfig    scalingPolicy    optional  Example  :  \"[scalingconfig](#scalingconfig)\"  ScalingConfig    streamName    optional  Example  :  \"string\"  string",
            "title": "Parameters"
        },
        {
            "location": "/rest/restapis/#responses_6",
            "text": "HTTP Code  Description  Schema      201  Successfully created the stream with the given configuration  StreamProperty    404  Scope not found  No Content    409  Stream with given name already exists  No Content    500  Internal server error while creating a stream  No Content",
            "title": "Responses"
        },
        {
            "location": "/rest/restapis/#consumes_1",
            "text": "application/json",
            "title": "Consumes"
        },
        {
            "location": "/rest/restapis/#produces_5",
            "text": "application/json",
            "title": "Produces"
        },
        {
            "location": "/rest/restapis/#tags_7",
            "text": "Streams",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#example-http-request_6",
            "text": "",
            "title": "Example HTTP request"
        },
        {
            "location": "/rest/restapis/#request-path_6",
            "text": "/scopes/string/streams",
            "title": "Request path"
        },
        {
            "location": "/rest/restapis/#request-body_1",
            "text": "json :\n{\n  \"streamName\" : \"string\",\n  \"scalingPolicy\" : {\n    \"type\" : \"string\",\n    \"targetRate\" : 0,\n    \"scaleFactor\" : 0,\n    \"minSegments\" : 0\n  },\n  \"retentionPolicy\" : {\n    \"type\" : \"string\",\n    \"value\" : 0\n  }\n}",
            "title": "Request body"
        },
        {
            "location": "/rest/restapis/#example-http-response_5",
            "text": "",
            "title": "Example HTTP response"
        },
        {
            "location": "/rest/restapis/#response-201_1",
            "text": "json :\n{\n  \"scopeName\" : \"string\",\n  \"streamName\" : \"string\",\n  \"scalingPolicy\" : {\n    \"type\" : \"string\",\n    \"targetRate\" : 0,\n    \"scaleFactor\" : 0,\n    \"minSegments\" : 0\n  },\n  \"retentionPolicy\" : {\n    \"type\" : \"string\",\n    \"value\" : 0\n  }\n}",
            "title": "Response 201"
        },
        {
            "location": "/rest/restapis/#get-scopesscopenamestreams",
            "text": "",
            "title": "GET /scopes/{scopeName}/streams"
        },
        {
            "location": "/rest/restapis/#description_7",
            "text": "List streams within the given scope",
            "title": "Description"
        },
        {
            "location": "/rest/restapis/#parameters_6",
            "text": "Type  Name  Description  Schema      Path  scopeName    required  Scope name  string    Query  showInternalStreams    optional  Optional flag whether to display system created streams. If not specified only user created streams will be returned  string",
            "title": "Parameters"
        },
        {
            "location": "/rest/restapis/#responses_7",
            "text": "HTTP Code  Description  Schema      200  List of all streams configured for the given scope  StreamsList    404  Scope not found  No Content    500  Internal server error while fetching the list of streams for the given scope  No Content",
            "title": "Responses"
        },
        {
            "location": "/rest/restapis/#produces_6",
            "text": "application/json",
            "title": "Produces"
        },
        {
            "location": "/rest/restapis/#tags_8",
            "text": "Streams",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#example-http-request_7",
            "text": "",
            "title": "Example HTTP request"
        },
        {
            "location": "/rest/restapis/#request-path_7",
            "text": "/scopes/string/streams",
            "title": "Request path"
        },
        {
            "location": "/rest/restapis/#request-query",
            "text": "json :\n{\n  \"showInternalStreams\" : \"string\"\n}",
            "title": "Request query"
        },
        {
            "location": "/rest/restapis/#example-http-response_6",
            "text": "",
            "title": "Example HTTP response"
        },
        {
            "location": "/rest/restapis/#response-200_4",
            "text": "json :\n{\n  \"streams\" : [ {\n    \"scopeName\" : \"string\",\n    \"streamName\" : \"string\",\n    \"scalingPolicy\" : {\n      \"type\" : \"string\",\n      \"targetRate\" : 0,\n      \"scaleFactor\" : 0,\n      \"minSegments\" : 0\n    },\n    \"retentionPolicy\" : {\n      \"type\" : \"string\",\n      \"value\" : 0\n    }\n  } ]\n}",
            "title": "Response 200"
        },
        {
            "location": "/rest/restapis/#get-scopesscopenamestreamsstreamname",
            "text": "",
            "title": "GET /scopes/{scopeName}/streams/{streamName}"
        },
        {
            "location": "/rest/restapis/#description_8",
            "text": "Fetch the properties of an existing stream",
            "title": "Description"
        },
        {
            "location": "/rest/restapis/#parameters_7",
            "text": "Type  Name  Description  Schema      Path  scopeName    required  Scope name  string    Path  streamName    required  Stream name  string",
            "title": "Parameters"
        },
        {
            "location": "/rest/restapis/#responses_8",
            "text": "HTTP Code  Description  Schema      200  Found stream properties  StreamProperty    404  Scope or stream with given name not found  No Content    500  Internal server error while fetching stream details  No Content",
            "title": "Responses"
        },
        {
            "location": "/rest/restapis/#produces_7",
            "text": "application/json",
            "title": "Produces"
        },
        {
            "location": "/rest/restapis/#tags_9",
            "text": "Streams",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#example-http-request_8",
            "text": "",
            "title": "Example HTTP request"
        },
        {
            "location": "/rest/restapis/#request-path_8",
            "text": "/scopes/string/streams/string",
            "title": "Request path"
        },
        {
            "location": "/rest/restapis/#example-http-response_7",
            "text": "",
            "title": "Example HTTP response"
        },
        {
            "location": "/rest/restapis/#response-200_5",
            "text": "json :\n{\n  \"scopeName\" : \"string\",\n  \"streamName\" : \"string\",\n  \"scalingPolicy\" : {\n    \"type\" : \"string\",\n    \"targetRate\" : 0,\n    \"scaleFactor\" : 0,\n    \"minSegments\" : 0\n  },\n  \"retentionPolicy\" : {\n    \"type\" : \"string\",\n    \"value\" : 0\n  }\n}",
            "title": "Response 200"
        },
        {
            "location": "/rest/restapis/#put-scopesscopenamestreamsstreamname",
            "text": "",
            "title": "PUT /scopes/{scopeName}/streams/{streamName}"
        },
        {
            "location": "/rest/restapis/#description_9",
            "text": "Update configuration of an existing stream",
            "title": "Description"
        },
        {
            "location": "/rest/restapis/#parameters_8",
            "text": "Type  Name  Description  Schema      Path  scopeName    required  Scope name  string    Path  streamName    required  Stream name  string    Body  UpdateStreamRequest    required  The new stream configuration  UpdateStreamRequest      UpdateStreamRequest     Name  Description  Schema      retentionPolicy    optional  Example  :  \"[retentionconfig](#retentionconfig)\"  RetentionConfig    scalingPolicy    optional  Example  :  \"[scalingconfig](#scalingconfig)\"  ScalingConfig",
            "title": "Parameters"
        },
        {
            "location": "/rest/restapis/#responses_9",
            "text": "HTTP Code  Description  Schema      200  Successfully updated the stream configuration  StreamProperty    404  Scope or stream with given name not found  No Content    500  Internal server error while updating the stream  No Content",
            "title": "Responses"
        },
        {
            "location": "/rest/restapis/#consumes_2",
            "text": "application/json",
            "title": "Consumes"
        },
        {
            "location": "/rest/restapis/#produces_8",
            "text": "application/json",
            "title": "Produces"
        },
        {
            "location": "/rest/restapis/#tags_10",
            "text": "Streams",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#example-http-request_9",
            "text": "",
            "title": "Example HTTP request"
        },
        {
            "location": "/rest/restapis/#request-path_9",
            "text": "/scopes/string/streams/string",
            "title": "Request path"
        },
        {
            "location": "/rest/restapis/#request-body_2",
            "text": "json :\n{\n  \"scalingPolicy\" : {\n    \"type\" : \"string\",\n    \"targetRate\" : 0,\n    \"scaleFactor\" : 0,\n    \"minSegments\" : 0\n  },\n  \"retentionPolicy\" : {\n    \"type\" : \"string\",\n    \"value\" : 0\n  }\n}",
            "title": "Request body"
        },
        {
            "location": "/rest/restapis/#example-http-response_8",
            "text": "",
            "title": "Example HTTP response"
        },
        {
            "location": "/rest/restapis/#response-200_6",
            "text": "json :\n{\n  \"scopeName\" : \"string\",\n  \"streamName\" : \"string\",\n  \"scalingPolicy\" : {\n    \"type\" : \"string\",\n    \"targetRate\" : 0,\n    \"scaleFactor\" : 0,\n    \"minSegments\" : 0\n  },\n  \"retentionPolicy\" : {\n    \"type\" : \"string\",\n    \"value\" : 0\n  }\n}",
            "title": "Response 200"
        },
        {
            "location": "/rest/restapis/#delete-scopesscopenamestreamsstreamname",
            "text": "",
            "title": "DELETE /scopes/{scopeName}/streams/{streamName}"
        },
        {
            "location": "/rest/restapis/#description_10",
            "text": "Delete a stream",
            "title": "Description"
        },
        {
            "location": "/rest/restapis/#parameters_9",
            "text": "Type  Name  Description  Schema      Path  scopeName    required  Scope name  string    Path  streamName    required  Stream name  string",
            "title": "Parameters"
        },
        {
            "location": "/rest/restapis/#responses_10",
            "text": "HTTP Code  Description  Schema      204  Successfully deleted the stream  No Content    404  Stream not found  No Content    412  Cannot delete stream since it is not sealed  No Content    500  Internal server error while deleting the stream  No Content",
            "title": "Responses"
        },
        {
            "location": "/rest/restapis/#tags_11",
            "text": "Streams",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#example-http-request_10",
            "text": "",
            "title": "Example HTTP request"
        },
        {
            "location": "/rest/restapis/#request-path_10",
            "text": "/scopes/string/streams/string",
            "title": "Request path"
        },
        {
            "location": "/rest/restapis/#get-scopesscopenamestreamsstreamnamescaling-events",
            "text": "",
            "title": "GET /scopes/{scopeName}/streams/{streamName}/scaling-events"
        },
        {
            "location": "/rest/restapis/#description_11",
            "text": "Get scaling events for a given datetime period.",
            "title": "Description"
        },
        {
            "location": "/rest/restapis/#parameters_10",
            "text": "Type  Name  Description  Schema      Path  scopeName    required  Scope name  string    Path  streamName    required  Stream name  string    Query  from    required  Parameter to display scaling events from that particular datetime. Input should be milliseconds from Jan 1 1970.  integer (int64)    Query  to    required  Parameter to display scaling events to that particular datetime. Input should be milliseconds from Jan 1 1970.  integer (int64)",
            "title": "Parameters"
        },
        {
            "location": "/rest/restapis/#responses_11",
            "text": "HTTP Code  Description  Schema      200  Successfully fetched list of scaling events.  ScalingEventList    404  Scope/Stream not found.  No Content    500  Internal Server error while fetching scaling events.  No Content",
            "title": "Responses"
        },
        {
            "location": "/rest/restapis/#produces_9",
            "text": "application/json",
            "title": "Produces"
        },
        {
            "location": "/rest/restapis/#tags_12",
            "text": "Streams",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#example-http-request_11",
            "text": "",
            "title": "Example HTTP request"
        },
        {
            "location": "/rest/restapis/#request-path_11",
            "text": "/scopes/string/streams/string/scaling-events",
            "title": "Request path"
        },
        {
            "location": "/rest/restapis/#request-query_1",
            "text": "json :\n{\n  \"from\" : 0,\n  \"to\" : 0\n}",
            "title": "Request query"
        },
        {
            "location": "/rest/restapis/#example-http-response_9",
            "text": "",
            "title": "Example HTTP response"
        },
        {
            "location": "/rest/restapis/#response-200_7",
            "text": "json :\n{\n  \"scalingEvents\" : [ {\n    \"timestamp\" : 0,\n    \"segmentList\" : [ {\n      \"number\" : 0,\n      \"startTime\" : 0,\n      \"keyStart\" : 0,\n      \"keyEnd\" : 0\n    } ]\n  } ]\n}",
            "title": "Response 200"
        },
        {
            "location": "/rest/restapis/#put-scopesscopenamestreamsstreamnamestate",
            "text": "",
            "title": "PUT /scopes/{scopeName}/streams/{streamName}/state"
        },
        {
            "location": "/rest/restapis/#description_12",
            "text": "Updates the current state of the stream",
            "title": "Description"
        },
        {
            "location": "/rest/restapis/#parameters_11",
            "text": "Type  Name  Description  Schema      Path  scopeName    required  Scope name  string    Path  streamName    required  Stream name  string    Body  UpdateStreamStateRequest    required  The state info to be updated  StreamState",
            "title": "Parameters"
        },
        {
            "location": "/rest/restapis/#responses_12",
            "text": "HTTP Code  Description  Schema      200  Successfully updated the stream state  StreamState    404  Scope or stream with given name not found  No Content    500  Internal server error while updating the stream state  No Content",
            "title": "Responses"
        },
        {
            "location": "/rest/restapis/#consumes_3",
            "text": "application/json",
            "title": "Consumes"
        },
        {
            "location": "/rest/restapis/#produces_10",
            "text": "application/json",
            "title": "Produces"
        },
        {
            "location": "/rest/restapis/#tags_13",
            "text": "Streams",
            "title": "Tags"
        },
        {
            "location": "/rest/restapis/#example-http-request_12",
            "text": "",
            "title": "Example HTTP request"
        },
        {
            "location": "/rest/restapis/#request-path_12",
            "text": "/scopes/string/streams/string/state",
            "title": "Request path"
        },
        {
            "location": "/rest/restapis/#request-body_3",
            "text": "json :\n{\n  \"streamState\" : \"string\"\n}",
            "title": "Request body"
        },
        {
            "location": "/rest/restapis/#example-http-response_10",
            "text": "",
            "title": "Example HTTP response"
        },
        {
            "location": "/rest/restapis/#response-200_8",
            "text": "json :\n{\n  \"streamState\" : \"string\"\n}",
            "title": "Response 200"
        },
        {
            "location": "/rest/restapis/#definitions",
            "text": "",
            "title": "Definitions"
        },
        {
            "location": "/rest/restapis/#readergroupproperty",
            "text": "Name  Description  Schema      onlineReaderIds    optional  Example  :  [ \"string\" ]  < string > array    readerGroupName    optional  Example  :  \"string\"  string    scopeName    optional  Example  :  \"string\"  string    streamList    optional  Example  :  [ \"string\" ]  < string > array",
            "title": "ReaderGroupProperty"
        },
        {
            "location": "/rest/restapis/#readergroupslist",
            "text": "Name  Description  Schema      readerGroups    optional  Example  :  [ \"object\" ]  <  readerGroups  > array      readerGroups     Name  Description  Schema      readerGroupName    optional  Example  :  \"string\"  string",
            "title": "ReaderGroupsList"
        },
        {
            "location": "/rest/restapis/#retentionconfig",
            "text": "Name  Description  Schema      type    optional  Example  :  \"string\"  enum (LIMITED_DAYS, LIMITED_SIZE_MB)    value    optional  Example  :  0  integer (int64)",
            "title": "RetentionConfig"
        },
        {
            "location": "/rest/restapis/#scalemetadata",
            "text": "Name  Description  Schema      segmentList    optional  Example  :  [ \"[segment](#segment)\" ]  <  Segment  > array    timestamp    optional  Example  :  0  integer (int64)",
            "title": "ScaleMetadata"
        },
        {
            "location": "/rest/restapis/#scalingconfig",
            "text": "Name  Description  Schema      minSegments    optional  Example  :  0  integer (int32)    scaleFactor    optional  Example  :  0  integer (int32)    targetRate    optional  Example  :  0  integer (int32)    type    optional  Example  :  \"string\"  enum (FIXED_NUM_SEGMENTS, BY_RATE_IN_KBYTES_PER_SEC, BY_RATE_IN_EVENTS_PER_SEC)",
            "title": "ScalingConfig"
        },
        {
            "location": "/rest/restapis/#scalingeventlist",
            "text": "Name  Description  Schema      scalingEvents    optional  Example  :  [ \"[scalemetadata](#scalemetadata)\" ]  <  ScaleMetadata  > array",
            "title": "ScalingEventList"
        },
        {
            "location": "/rest/restapis/#scopeproperty",
            "text": "Name  Description  Schema      scopeName    optional  Example  :  \"string\"  string",
            "title": "ScopeProperty"
        },
        {
            "location": "/rest/restapis/#scopeslist",
            "text": "Name  Description  Schema      scopes    optional  Example  :  [ \"[scopeproperty](#scopeproperty)\" ]  <  ScopeProperty  > array",
            "title": "ScopesList"
        },
        {
            "location": "/rest/restapis/#segment",
            "text": "Name  Description  Schema      keyEnd    optional  Example  :  0  integer (double)    keyStart    optional  Example  :  0  integer (double)    number    optional  Example  :  0  integer (int32)    startTime    optional  Example  :  0  integer (int64)",
            "title": "Segment"
        },
        {
            "location": "/rest/restapis/#streamproperty",
            "text": "Name  Description  Schema      retentionPolicy    optional  Example  :  \"[retentionconfig](#retentionconfig)\"  RetentionConfig    scalingPolicy    optional  Example  :  \"[scalingconfig](#scalingconfig)\"  ScalingConfig    scopeName    optional  Example  :  \"string\"  string    streamName    optional  Example  :  \"string\"  string",
            "title": "StreamProperty"
        },
        {
            "location": "/rest/restapis/#streamstate",
            "text": "Name  Description  Schema      streamState    optional  Example  :  \"string\"  enum (SEALED)",
            "title": "StreamState"
        },
        {
            "location": "/rest/restapis/#streamslist",
            "text": "Name  Description  Schema      streams    optional  Example  :  [ \"[streamproperty](#streamproperty)\" ]  <  StreamProperty  > array",
            "title": "StreamsList"
        },
        {
            "location": "/connectors/",
            "text": "Pravega Connectors\n\n\nConnectors allow integrating Pravega with different data sources and sinks.  \n\n\nThe initial connector supported is Flink which enables building end-to-end stream processing pipelines with Pravega.  This also allows reading and writing data to external data sources and sinks via Flink Streaming Connectors.\n\n\n\n\nFlink Connector\n\n\nLogstash - comming soon",
            "title": "Pravega Connectors"
        },
        {
            "location": "/connectors/#pravega-connectors",
            "text": "Connectors allow integrating Pravega with different data sources and sinks.    The initial connector supported is Flink which enables building end-to-end stream processing pipelines with Pravega.  This also allows reading and writing data to external data sources and sinks via Flink Streaming Connectors.   Flink Connector  Logstash - comming soon",
            "title": "Pravega Connectors"
        },
        {
            "location": "/basic-reader-and-writer/",
            "text": "Working with Pravega: Basic Reader and Writer\n\n\nLets examine how to build simple Pravega applications. \u00a0The simplest kind of\nPravega application uses a Pravega Reader to read from a Pravega Stream or a\nPravega Writer that writes to a Pravega Stream. \u00a0A simple example of both can be\nfound in the\u00a0Pravega Samples \"hello world\" app. These sample applications\nprovide a very basic example of how a Java application could use the Pravega\nJava Client Library to access Pravega functionality.\n\n\nInstructions for running the sample applications can be found in the \nPravega\nSamples\nreadme\n.\n\n\nYou really should be familiar with Pravega Concepts (see\u00a0Pravega Concepts)\nbefore continuing reading this page.\n\n\nHelloWorldWriter\n\n\nThe HelloWorldWriter application is a simple demonstration of using the\nEventStreamWriter to write an Event to Pravega.\n\n\nTaking a look first at the HelloWorldWriter example application, the key part of\nthe code is in the run() method:\n\n\npublic void run(String routingKey, String message) {\n    StreamManager streamManager = StreamManager.create(controllerURI);\n\n    final boolean scopeIsNew = streamManager.createScope(scope);\n    StreamConfiguration streamConfig = StreamConfiguration.builder()\n            .scalingPolicy(ScalingPolicy.fixed(1))\n            .build();\n    final boolean streamIsNew = streamManager.createStream(scope, streamName, streamConfig);\n\n    try (ClientFactory clientFactory = ClientFactory.withScope(scope, controllerURI);\n         EventStreamWriter<String> writer = clientFactory.createEventWriter(streamName,\n                                                          new JavaSerializer<String>(),\n                                                   EventWriterConfig.builder().build())) {\n\n         System.out.format(\"Writing message: '%s' with routing-key: '%s' to stream '%s / %s'%n\",\n                message, routingKey, scope, streamName);\n         final CompletableFuture<Void> writeFuture = writer.writeEvent(routingKey, message);\n    }\n}\n\n\n\n\nThe purpose of the run() method is to create a Stream (lines 2-9) and output the\ngiven Event to that Stream (lines 10-18).\n\n\nCreating a Stream and the StreamManager Interface\n\n\nA Stream is created in the context of a Scope; the Scope acts as a namespace\nmechanism so that different sets of Streams can be categorized for some purpose.\n\u00a0For example, I might have a separate scope for each application. \u00a0I might\nchoose to create a set of Scopes, one for each department in my organization.\n\u00a0In a multi-tenant environment, I might have a separate Scope per tenant. \u00a0As a\ndeveloper, I can choose whatever categorization scheme I need and use the Scope\nconcept for organizing my Streams along that categorization scheme.\n\n\nScopes and Streams are created and manipulated via the StreamManager Interface\nto the Pravega Controller. \u00a0You need to have a URI to any of the Pravega\nController instances in your cluster in order to create a StreamManager object.\n\u00a0This is shown in line 2.\n\n\nIn the setup for the HelloWorld sample applications, the controllerURI is\nconfigured as a command line parameter when the sample application is launched.\n\u00a0For the \"single node\" deployment of Pravega, the Controller is listening on\nlocalhost, port 9090.\n\n\nThe StreamManager provides access to various control plane functions in Pravega\nrelated to Scopes and Streams:\n\n\n\n\n\n\n\n\nMethod\n\n\nParameters\n\n\nDiscussion\n\n\n\n\n\n\n\n\n\n\n(static) create\n\n\n(URI controller)\n\n\nGiven a URI to one of the Pravega Controller instances in the Pravega Cluster, create a Stream Manager object.\n\n\n\n\n\n\ncreateScope\n\n\n(String scopeName)\n\n\nCreates a Scope with the given name.\n\n\n\n\n\n\n\n\n\n\nReturns true if the Scope is created, returns false if the Scope already exists.\n\n\n\n\n\n\n\n\n\n\nYou can call this method even if the Scope already exists, it won't harm anything.\n\n\n\n\n\n\ndeleteScope\n\n\n(String scopeName)\n\n\nDeletes a Scope with the given name.\n\n\n\n\n\n\n\n\n\n\nReturns true if the scope was deleted, false otherwise.\n\n\n\n\n\n\n\n\n\n\nNote, if the Scope contains Streams, the deleteScope operation will fail with an exception.\n\n\n\n\n\n\n\n\n\n\nIf you delete a nonexistent Scope, the method will succeed and return false.\n\n\n\n\n\n\ncreateStream\n\n\n(String scopeName, String streamName, StreamConfiguration config)\n\n\nCreate a Stream within a given Scope.\n\n\n\n\n\n\n\n\n\n\nNote that both scope name and stream name are limited by the following pattern: [a-zA-Z0-9]+ (i.e. letters and numbers only, no punctuation)\n\n\n\n\n\n\n\n\n\n\nNote also: the Scope must exist, an exception is thrown if you create a Stream in a nonexistent scope.\n\n\n\n\n\n\n\n\n\n\nA StreamConfiguration is built using a builder pattern\n\n\n\n\n\n\n\n\n\n\nReturns true if the Stream is created, returns false if the Stream already exists.\n\n\n\n\n\n\n\n\n\n\nYou can call this method even if the Stream already exists, it won't harm anything.\n\n\n\n\n\n\nupdateStream\n\n\n(String scopeName, String streamName, StreamConfiguration config)\n\n\nSwap out the Stream's configuration.\n\n\n\n\n\n\n\n\n\n\nNote the Stream must already exist, an exception is thrown if you update a nonexistent stream.\n\n\n\n\n\n\n\n\n\n\nReturns true if the Stream was changed\n\n\n\n\n\n\nsealStream\n\n\n(String scopeName, String streamName)\n\n\nPrevent any further writes to a Stream\n\n\n\n\n\n\n\n\n\n\nNote the Stream must already exist, an exception is thrown if you seal a nonexistent stream.\n\n\n\n\n\n\n\n\n\n\nReturns true if the Stream is successfully sealed\n\n\n\n\n\n\ndeleteStream\n\n\n(String scopeName, String streamName)\n\n\nRemove the Stream from Pravega and recover any resources used by that Stream\n\n\n\n\n\n\n\n\n\n\nNote the Stream must already exist, an exception is thrown if you delete a nonexistent stream.\n\n\n\n\n\n\n\n\n\n\nReturns true if the stream was deleted.\n\n\n\n\n\n\n\n\nAfter line 3 in the code is finished, we have established that the Scope exists,\nwe can then go on and create the Stream in lines 5-8.\u00a0\n\n\nThe StreamManager needs 3 things to create a Stream, the Scope's name, the\nStream's name and a StreamConfiguration. \u00a0The most interesting task is to create\nthe StreamConfiguration.\n\n\nLike many objects in Pravega, a Stream takes a configuration object that allows\na developer to control various behaviors of the Stream. \u00a0All configuration\nobjects in Pravega use a builder pattern for construction. \u00a0There are really two\nimportant configuration items related to streams: Retention Policy and Scaling\nPolicy. \u00a0\n\n\nRetention Policy allows the developer to control how long data is kept in a\nStream before it is deleted. \u00a0S/he can specify data should be kept for a certain\nperiod of time (ideal for situations like regulatory compliance that mandate\ncertain retention periods) or to retain data until a certain number of bytes\nhave been consumed. \u00a0At the moment, Retention Policy is not completely\nimplemented. \u00a0By default, the RetentionPolicy is set as \"unlimited\" meaning, data\nwill not be removed from the Stream.\n\n\nScaling Policy is the way developers configure a Stream to take advantage\nPravega's auto-scaling feature. \u00a0In line 6, we use a fixed policy, meaning the\nStream is configured with the given number of Stream Segments and that won't\nchange. \u00a0The other options are to scale by a given number of Events per second\nor a given number of Kilobytes per second. \u00a0In these two policies, the developer\nspecifies a target rate, a scaling factor and a minimum number of Segments. \u00a0The\ntarget rate is straight forward, if ingest rate exceeds a certain number of\nEvents or Kilobytes of data for a sustained period of time, Pravega will attempt\nto add new Stream Segments to the Stream. \u00a0If the rate drops below that\nthreshold for a sustained period of time, Pravega will attempt to merge adjacent\nStream Segments. \u00a0The scaling factor is a setting on the Scaling Policy that\ndetermines how many Stream Segments should be added when the target rate (of\nEvents or Kilobytes) is exceeded. \u00a0The minimum number of Segments is a factor\nthat sets the minimum degree of read parallelism to be maintained; if this value\nis set at 3, for example, there will always be 3 Stream Segments available on\nthe Stream. \u00a0Currently, this property is effective only when the stream is\ncreated; at some point in the future, update stream will allow this factor to be\nused to change the minimum degree of read parallelism on an existing Stream.\n\n\nOnce the StreamConfiguration object is created, creating the Stream is straight\nforward (line 8). \u00a0After the Stream is created, we are all set to start writing\nEvent(s) to the Stream.\n\n\nWriting an Event using EventWriter\n\n\nApplications use an EventStreamWriter object to write Events to a Stream. \u00a0The\nkey object to creating the EventStreamWriter is the ClientFactory. \u00a0The\nClientFactory is used to create Readers, Writers and other types of Pravega\nClient objects such as the State Synchronizer (see\u00a0\nWorking with Pravega: State\nSynchronizer\n).\n\n\nLine 10\u00a0shows the creation of a ClientFactory. \u00a0A ClientFactory is created in\nthe context of a Scope, since all Readers, Writers and other Clients created by\nthe ClientFactory are created in the context of that Scope. \u00a0The ClientFactory\nalso needs a URI to one of the Pravega Controllers, just like StreamManager.\n\n\nBecause ClientFactory and the objects it creates consumes resources from\nPravega, it is a good practice to create these objects in a try-with-resources\nstatement. \u00a0Because ClientFactory and the objects it creates all implement\nAutocloseable, the try-with-resources approach makes sure that regardless of how\nyour application ends, the Pravega resources will be properly closed in the\nright order.\n\n\nNow that we have a ClientFactory, we can use it to create a Writer. \u00a0There are\nseveral things a developer needs to know before s/he creates a Writer:\n\n\n\n\n\n\nWhat is the name of the Stream to write to? \u00a0Note: the Scope has already\n    been determined when the ClientFactory was created\n\n\n\n\n\n\nWhat Type of Event objects will be written to the Stream?\n\n\n\n\n\n\nWhat serializer will be used to convert an Event object to bytes? \u00a0Recall\n    that Pravega only knows about sequences of bytes, it does not really know\n    anything about Java objects.\n\n\n\n\n\n\nDoes the Writer need to be configured with any special behavior?\n\n\n\n\n\n\nIn our example, lines 11-13\u00a0show all these decisions. \u00a0This Writer writes to the\nStream specified in the configuration of the HelloWorldWriter object itself (by\ndefault the stream is named \"helloStream\" in the \"examples\" Scope). \u00a0The Writer\nprocesses Java String objects as Events and uses the built in Java serializer\nfor Strings. \u00a0\n\n\nThe EventWriterConfig allows the developer to specify things like the number of\nattempts to retry a request before giving up and associated exponential back\nsettings. \u00a0Pravega takes care to retry requests in the case where connection\nfailures or Pravega component outages may temporarily prevent a request from\nsucceeding, so application logic doesn't need to be complicated with dealing\nwith intermittent cluster failures. \u00a0In our case, we took the default settings\nfor EventWriterConfig in line 13.\n\n\nNow we can write the Event to the Stream as shown in line 17. \u00a0EventStreamWriter\nprovides a writeEvent() operation that writes the given non-null Event object to\nthe Stream using a given routing key to determine which Stream Segment it should\nappear on. \u00a0Many operations in Pravega, such as writeEvent(), are asynchronous\nand return some sort of Future object. \u00a0If the application needed to make sure\nthe Event was durably written to Pravega and available for Readers, it could\nwait on the Future before proceeding. \u00a0In the case of our simple \"hello world\"\nexample, we don't bother waiting.\n\n\nEventStreamWriter can also be used to begin a Transaction. \u00a0We cover\nTransactions in more detail elsewhere (\nWorking with Pravega:\nTransactions\n).\n\n\nThat's it for writing Events. \u00a0Now lets take a look at how to read Events using\nPravega.\n\n\nHelloWorldReader\n\n\nThe HelloWorldReader is a simple demonstration of using the EventStreamReader.\nThe application simply reads Events from the given Stream and prints a string\nrepresentation of those Events onto the console.\n\n\nJust like the HelloWorldWriter example, the key part of the HelloWorldReader app\nis in the run() method:\n\n\npublic void run() {\n   StreamManager streamManager = StreamManager.create(controllerURI);\n\n   final boolean scopeIsNew = streamManager.createScope(scope);\n   StreamConfiguration streamConfig = StreamConfiguration.builder()\n           .scalingPolicy(ScalingPolicy.fixed(1))\n           .build();\n   final boolean streamIsNew = streamManager.createStream(scope, streamName, streamConfig);\n\n   final String readerGroup = UUID.randomUUID().toString().replace(\"-\", \"\");\n   final ReaderGroupConfig readerGroupConfig = ReaderGroupConfig.builder().startingPosition(Sequence.MIN_VALUE)\n           .build();\n   try (ReaderGroupManager readerGroupManager = ReaderGroupManager.withScope(scope, controllerURI)) {\n       readerGroupManager.createReaderGroup(readerGroup, readerGroupConfig, Collections.singleton(streamName));\n   }\n\n   try (ClientFactory clientFactory = ClientFactory.withScope(scope, controllerURI);\n        EventStreamReader<String> reader = clientFactory.createReader(\"reader\",\n                                                                      readerGroup,\n                                                     new JavaSerializer<String>(),\n                                                  ReaderConfig.builder().build())) {\n        System.out.format(\"Reading all the events from %s/%s%n\", scope, streamName);\n        EventRead<String> event = null;\n        do {\n           try {\n               event = reader.readNextEvent(READER_TIMEOUT_MS);\n               if (event.getEvent() != null) {\n                   System.out.format(\"Read event '%s'%n\", event.getEvent());\n               }\n           } catch (ReinitializationRequiredException e) {\n               //There are certain circumstances where the reader needs to be reinitialized\n               e.printStackTrace();\n           }\n       } while (event.getEvent() != null);\n       System.out.format(\"No more events from %s/%s%n\", scope, streamName);\n   }\n\n\n\n\nLines 2-8 set up the Scope and Stream just like in the HelloWorldWriter\napplication. \u00a0Lines 10-15 set up the ReaderGroup as the prerequisite to creating\nthe EventStreamReader and using it to read Events from the Stream (lines 17-36).\n\n\nReaderGroup Basics\n\n\nAny Reader in Pravega belongs to some ReaderGroup. \u00a0A ReaderGroup is a grouping\nof one or more Readers that consume from a Stream in parallel. \u00a0Before we create\na Reader, we need to either create a ReaderGroup (or be aware of the name of an\nexisting ReaderGroup). \u00a0This application only uses the basics from ReaderGroup.\n\n\nLines 10-15 show basic ReaderGroup creation. \u00a0ReaderGroup objects are created\nfrom a ReaderGroupManager object. \u00a0The ReaderGroupManager object, in turn, is\ncreated on a given Scope with a URI to one of the Pravega Controllers, very much\nlike a ClientFactory is created. \u00a0A ReaderGroupManager object is created on line\n14. \u00a0Note the creation is also in a try-with-resources statement to make sure\nthe ReaderGroupManager is properly cleaned up. \u00a0 The ReaderGroupManager allows a\ndeveloper to create, delete and retrieve ReaderGroup objects by name.\n\n\nTo create a ReaderGroup, the developer needs a name for the ReaderGroup, a\nconfiguration and a set of 1 or more Streams to read from. \u00a0\n\n\nThe ReaderGroup's name might be meaningful to the application, like\n\"WebClickStreamReaders\". \u00a0In our case, on line 10, we have a simple UUID as the\nname (note the modification of the UUID string to remove the \"-\" character\nbecause ReaderGroup names can only have letters and numbers). \u00a0In cases where\nyou will have multiple Readers reading in parallel and each Reader in a separate\nprocess, it is helpful to have a human readable name for the ReaderGroup. \u00a0In\nour case, we have one Reader, reading in isolation, so a UUID is a safe way to\nname the ReaderGroup. \u00a0Since the ReaderGroup is created via the\nReaderGroupManager and since the ReaderGroupManager is created within the\ncontext of a Scope, we can safely conclude that ReaderGroup names are namespaced\nby that Scope. \u00a0\n\n\nThe ReaderGroupConfig right now doesn't have much behavior. \u00a0The developer\nspecifies where in the Stream Reader should start consuming from (a starting\nposition). \u00a0In our case, on line 11, we start at the beginning of the Stream.\n\u00a0Other configuration items, such as specifying checkpointing etc. are options\nthat will be available through the ReaderGroupConfig. \u00a0But for now, we keep it\nsimple.\n\n\nThe fact that a ReaderGroup can be configured to read from multiple Streams is\nkind of cool. \u00a0Imagine a situation where I have a collection of Stream of sensor\ndata coming from a factory floor, each machine has its own Stream of sensor\ndata. \u00a0I can build applications that use a ReaderGroup per Stream so that the\napp reasons about data from exactly one machine. \u00a0I can build other apps that\nuse a ReaderGroup configured to read from all of the Streams. \u00a0In our case, on\nline 14, the ReaderGroup only reads from one Stream.\n\n\nYou can call createReaderGroup() with the same parameters multiple times, it\ndoesn't hurt anything, and the same ReaderGroup will be returned each time after\nit is initially created.\n\n\nNote that in other cases, if the developer knows the name of the ReaderGroup to\nuse and knows it has already been created, s/he can use getReaderGroup() on\nReaderGroupManager to retrieve the ReaderGroup object by name.\n\n\nSo at this point in the code, we have the Scope and Stream set up, we have the\nReaderGroup created and now we need to create a Reader and start reading Events.\n\n\nReading Event using an EventStreamReader\n\n\nLines 17-36 show an example of setting up an EventStreamReader and reading\nEvents using that EventStreamReader.\n\n\nFirst, we create a ClientFactory on line 17, in the same way we did it in the\nHelloWorldWriter app. \u00a0\n\n\nThen we use the ClientFactory to create an EventStreamReader object. \u00a0There are\nfour things the developer needs to create a Reader: a name for the reader, the\nreaderGroup it should be part of, the type of object expected on the Stream, the\nserializer to use to convert from the bytes stored in Pravega into the Event\nobjects and a ReaderConfig. \u00a0Lines 18-21 show the creation of an\nEventStreamReader. \u00a0The name of the Reader can be any valid Pravega name\n(numbers and letters). \u00a0Of course, the name of the reader is namespaced within\nthe Scope. \u00a0We talked about the creation of the ReaderGroup in the previous\nsection. \u00a0Just like with the EventStreamWriter, EventStreamReader uses Java\ngeneric types to allow a developer to specify a type safe Reader. \u00a0In our case,\nwe read Strings from the stream and use the standard Java String Serializer to\nconvert the bytes read from the stream into String objects. \u00a0Finally, the\nReaderConfig is created, but at the moment, there are no configuration items\nassociated with a Reader, so the empty ReaderConfig is just a place holder as\nPravega evolves to include configuration items on Readers.\n\n\nNote that you cannot create the same Reader multiple times. \u00a0Basically overtime\nyou call createReader() it tries to add the Reader to the ReaderGroup. \u00a0If the\nReaderGroup already contains a Reader with that name, an exception is thrown.\n\n\nNow that we have an EventStreamReader created, we can start using it to read\nEvents from the stream. \u00a0This is done on line 26. \u00a0The readNextEvent() operation\nreturns the next Event available on the Stream, or if there is no such Event,\nblocks for a specified timeout period. \u00a0If, after the timeout period has expired\nand no Event is available for reading, null is returned. That is why there is a\nnull check on line 27 (to avoid printing out a spurious \"null\" event message to\nthe console). \u00a0It is also used as the termination of the loop on line 34. \u00a0Note\nthat the Event itself is wrapped in an EventRead object. \n\n\nIt is worth noting that readNextEvent() may throw an exception (handled in lines\n30-33). \u00a0This exception would be handled in cases where the Readers in the\nReaderGroup need to be reset to a checkpoint or the ReaderGroup itself has been\naltered and the set of Streams being read has therefore been changed. \n\n\nSo that's it. \u00a0The simple HelloWorldReader loops, reading Events from a Stream\nuntil there are no more Events, and then the application terminates.\n\n\nExperimental batch reader\n\n\nFor applications that want to perform batch reads of historical stream data, the BatchClient provides a way to do this.\nIt allows for listing all of the segments in a stream, and reading their data. \n\n\nWhen the data is read this way, rather than joining a reader group which automatically partitions the data, the underlying structure of the stream is exposed and it is up to the application to decide how to process it. So events read in this way need not be read in order.\n\n\nObviously this API is not for every application, the main advantage is that it allows for low level integration with batch processing frameworks such as MapReduce. \n\n\nAs an example to iterate over the segments in the stream:\n\n\nIterator<SegmentInfo> segments = client.listSegments(stream);\nSegmentInfo segmentInfo = segments.next();\n\n\n\n\nOr to read the events from a segment:\n\n\nSegmentIterator<T> events = client.readSegment(segmentInfo.getSegment(), deserializer);\nwhile (events.hasNext()) {\n    processEvent(events.next());\n}",
            "title": "Working with Reader and Writer"
        },
        {
            "location": "/basic-reader-and-writer/#working-with-pravega-basic-reader-and-writer",
            "text": "Lets examine how to build simple Pravega applications. \u00a0The simplest kind of\nPravega application uses a Pravega Reader to read from a Pravega Stream or a\nPravega Writer that writes to a Pravega Stream. \u00a0A simple example of both can be\nfound in the\u00a0Pravega Samples \"hello world\" app. These sample applications\nprovide a very basic example of how a Java application could use the Pravega\nJava Client Library to access Pravega functionality.  Instructions for running the sample applications can be found in the  Pravega\nSamples\nreadme .  You really should be familiar with Pravega Concepts (see\u00a0Pravega Concepts)\nbefore continuing reading this page.",
            "title": "Working with Pravega: Basic Reader and Writer"
        },
        {
            "location": "/basic-reader-and-writer/#helloworldwriter",
            "text": "The HelloWorldWriter application is a simple demonstration of using the\nEventStreamWriter to write an Event to Pravega.  Taking a look first at the HelloWorldWriter example application, the key part of\nthe code is in the run() method:  public void run(String routingKey, String message) {\n    StreamManager streamManager = StreamManager.create(controllerURI);\n\n    final boolean scopeIsNew = streamManager.createScope(scope);\n    StreamConfiguration streamConfig = StreamConfiguration.builder()\n            .scalingPolicy(ScalingPolicy.fixed(1))\n            .build();\n    final boolean streamIsNew = streamManager.createStream(scope, streamName, streamConfig);\n\n    try (ClientFactory clientFactory = ClientFactory.withScope(scope, controllerURI);\n         EventStreamWriter<String> writer = clientFactory.createEventWriter(streamName,\n                                                          new JavaSerializer<String>(),\n                                                   EventWriterConfig.builder().build())) {\n\n         System.out.format(\"Writing message: '%s' with routing-key: '%s' to stream '%s / %s'%n\",\n                message, routingKey, scope, streamName);\n         final CompletableFuture<Void> writeFuture = writer.writeEvent(routingKey, message);\n    }\n}  The purpose of the run() method is to create a Stream (lines 2-9) and output the\ngiven Event to that Stream (lines 10-18).",
            "title": "HelloWorldWriter"
        },
        {
            "location": "/basic-reader-and-writer/#creating-a-stream-and-the-streammanager-interface",
            "text": "A Stream is created in the context of a Scope; the Scope acts as a namespace\nmechanism so that different sets of Streams can be categorized for some purpose.\n\u00a0For example, I might have a separate scope for each application. \u00a0I might\nchoose to create a set of Scopes, one for each department in my organization.\n\u00a0In a multi-tenant environment, I might have a separate Scope per tenant. \u00a0As a\ndeveloper, I can choose whatever categorization scheme I need and use the Scope\nconcept for organizing my Streams along that categorization scheme.  Scopes and Streams are created and manipulated via the StreamManager Interface\nto the Pravega Controller. \u00a0You need to have a URI to any of the Pravega\nController instances in your cluster in order to create a StreamManager object.\n\u00a0This is shown in line 2.  In the setup for the HelloWorld sample applications, the controllerURI is\nconfigured as a command line parameter when the sample application is launched.\n\u00a0For the \"single node\" deployment of Pravega, the Controller is listening on\nlocalhost, port 9090.  The StreamManager provides access to various control plane functions in Pravega\nrelated to Scopes and Streams:     Method  Parameters  Discussion      (static) create  (URI controller)  Given a URI to one of the Pravega Controller instances in the Pravega Cluster, create a Stream Manager object.    createScope  (String scopeName)  Creates a Scope with the given name.      Returns true if the Scope is created, returns false if the Scope already exists.      You can call this method even if the Scope already exists, it won't harm anything.    deleteScope  (String scopeName)  Deletes a Scope with the given name.      Returns true if the scope was deleted, false otherwise.      Note, if the Scope contains Streams, the deleteScope operation will fail with an exception.      If you delete a nonexistent Scope, the method will succeed and return false.    createStream  (String scopeName, String streamName, StreamConfiguration config)  Create a Stream within a given Scope.      Note that both scope name and stream name are limited by the following pattern: [a-zA-Z0-9]+ (i.e. letters and numbers only, no punctuation)      Note also: the Scope must exist, an exception is thrown if you create a Stream in a nonexistent scope.      A StreamConfiguration is built using a builder pattern      Returns true if the Stream is created, returns false if the Stream already exists.      You can call this method even if the Stream already exists, it won't harm anything.    updateStream  (String scopeName, String streamName, StreamConfiguration config)  Swap out the Stream's configuration.      Note the Stream must already exist, an exception is thrown if you update a nonexistent stream.      Returns true if the Stream was changed    sealStream  (String scopeName, String streamName)  Prevent any further writes to a Stream      Note the Stream must already exist, an exception is thrown if you seal a nonexistent stream.      Returns true if the Stream is successfully sealed    deleteStream  (String scopeName, String streamName)  Remove the Stream from Pravega and recover any resources used by that Stream      Note the Stream must already exist, an exception is thrown if you delete a nonexistent stream.      Returns true if the stream was deleted.     After line 3 in the code is finished, we have established that the Scope exists,\nwe can then go on and create the Stream in lines 5-8.\u00a0  The StreamManager needs 3 things to create a Stream, the Scope's name, the\nStream's name and a StreamConfiguration. \u00a0The most interesting task is to create\nthe StreamConfiguration.  Like many objects in Pravega, a Stream takes a configuration object that allows\na developer to control various behaviors of the Stream. \u00a0All configuration\nobjects in Pravega use a builder pattern for construction. \u00a0There are really two\nimportant configuration items related to streams: Retention Policy and Scaling\nPolicy. \u00a0  Retention Policy allows the developer to control how long data is kept in a\nStream before it is deleted. \u00a0S/he can specify data should be kept for a certain\nperiod of time (ideal for situations like regulatory compliance that mandate\ncertain retention periods) or to retain data until a certain number of bytes\nhave been consumed. \u00a0At the moment, Retention Policy is not completely\nimplemented. \u00a0By default, the RetentionPolicy is set as \"unlimited\" meaning, data\nwill not be removed from the Stream.  Scaling Policy is the way developers configure a Stream to take advantage\nPravega's auto-scaling feature. \u00a0In line 6, we use a fixed policy, meaning the\nStream is configured with the given number of Stream Segments and that won't\nchange. \u00a0The other options are to scale by a given number of Events per second\nor a given number of Kilobytes per second. \u00a0In these two policies, the developer\nspecifies a target rate, a scaling factor and a minimum number of Segments. \u00a0The\ntarget rate is straight forward, if ingest rate exceeds a certain number of\nEvents or Kilobytes of data for a sustained period of time, Pravega will attempt\nto add new Stream Segments to the Stream. \u00a0If the rate drops below that\nthreshold for a sustained period of time, Pravega will attempt to merge adjacent\nStream Segments. \u00a0The scaling factor is a setting on the Scaling Policy that\ndetermines how many Stream Segments should be added when the target rate (of\nEvents or Kilobytes) is exceeded. \u00a0The minimum number of Segments is a factor\nthat sets the minimum degree of read parallelism to be maintained; if this value\nis set at 3, for example, there will always be 3 Stream Segments available on\nthe Stream. \u00a0Currently, this property is effective only when the stream is\ncreated; at some point in the future, update stream will allow this factor to be\nused to change the minimum degree of read parallelism on an existing Stream.  Once the StreamConfiguration object is created, creating the Stream is straight\nforward (line 8). \u00a0After the Stream is created, we are all set to start writing\nEvent(s) to the Stream.",
            "title": "Creating a Stream and the StreamManager Interface"
        },
        {
            "location": "/basic-reader-and-writer/#writing-an-event-using-eventwriter",
            "text": "Applications use an EventStreamWriter object to write Events to a Stream. \u00a0The\nkey object to creating the EventStreamWriter is the ClientFactory. \u00a0The\nClientFactory is used to create Readers, Writers and other types of Pravega\nClient objects such as the State Synchronizer (see\u00a0 Working with Pravega: State\nSynchronizer ).  Line 10\u00a0shows the creation of a ClientFactory. \u00a0A ClientFactory is created in\nthe context of a Scope, since all Readers, Writers and other Clients created by\nthe ClientFactory are created in the context of that Scope. \u00a0The ClientFactory\nalso needs a URI to one of the Pravega Controllers, just like StreamManager.  Because ClientFactory and the objects it creates consumes resources from\nPravega, it is a good practice to create these objects in a try-with-resources\nstatement. \u00a0Because ClientFactory and the objects it creates all implement\nAutocloseable, the try-with-resources approach makes sure that regardless of how\nyour application ends, the Pravega resources will be properly closed in the\nright order.  Now that we have a ClientFactory, we can use it to create a Writer. \u00a0There are\nseveral things a developer needs to know before s/he creates a Writer:    What is the name of the Stream to write to? \u00a0Note: the Scope has already\n    been determined when the ClientFactory was created    What Type of Event objects will be written to the Stream?    What serializer will be used to convert an Event object to bytes? \u00a0Recall\n    that Pravega only knows about sequences of bytes, it does not really know\n    anything about Java objects.    Does the Writer need to be configured with any special behavior?    In our example, lines 11-13\u00a0show all these decisions. \u00a0This Writer writes to the\nStream specified in the configuration of the HelloWorldWriter object itself (by\ndefault the stream is named \"helloStream\" in the \"examples\" Scope). \u00a0The Writer\nprocesses Java String objects as Events and uses the built in Java serializer\nfor Strings. \u00a0  The EventWriterConfig allows the developer to specify things like the number of\nattempts to retry a request before giving up and associated exponential back\nsettings. \u00a0Pravega takes care to retry requests in the case where connection\nfailures or Pravega component outages may temporarily prevent a request from\nsucceeding, so application logic doesn't need to be complicated with dealing\nwith intermittent cluster failures. \u00a0In our case, we took the default settings\nfor EventWriterConfig in line 13.  Now we can write the Event to the Stream as shown in line 17. \u00a0EventStreamWriter\nprovides a writeEvent() operation that writes the given non-null Event object to\nthe Stream using a given routing key to determine which Stream Segment it should\nappear on. \u00a0Many operations in Pravega, such as writeEvent(), are asynchronous\nand return some sort of Future object. \u00a0If the application needed to make sure\nthe Event was durably written to Pravega and available for Readers, it could\nwait on the Future before proceeding. \u00a0In the case of our simple \"hello world\"\nexample, we don't bother waiting.  EventStreamWriter can also be used to begin a Transaction. \u00a0We cover\nTransactions in more detail elsewhere ( Working with Pravega:\nTransactions ).  That's it for writing Events. \u00a0Now lets take a look at how to read Events using\nPravega.",
            "title": "Writing an Event using EventWriter"
        },
        {
            "location": "/basic-reader-and-writer/#helloworldreader",
            "text": "The HelloWorldReader is a simple demonstration of using the EventStreamReader.\nThe application simply reads Events from the given Stream and prints a string\nrepresentation of those Events onto the console.  Just like the HelloWorldWriter example, the key part of the HelloWorldReader app\nis in the run() method:  public void run() {\n   StreamManager streamManager = StreamManager.create(controllerURI);\n\n   final boolean scopeIsNew = streamManager.createScope(scope);\n   StreamConfiguration streamConfig = StreamConfiguration.builder()\n           .scalingPolicy(ScalingPolicy.fixed(1))\n           .build();\n   final boolean streamIsNew = streamManager.createStream(scope, streamName, streamConfig);\n\n   final String readerGroup = UUID.randomUUID().toString().replace(\"-\", \"\");\n   final ReaderGroupConfig readerGroupConfig = ReaderGroupConfig.builder().startingPosition(Sequence.MIN_VALUE)\n           .build();\n   try (ReaderGroupManager readerGroupManager = ReaderGroupManager.withScope(scope, controllerURI)) {\n       readerGroupManager.createReaderGroup(readerGroup, readerGroupConfig, Collections.singleton(streamName));\n   }\n\n   try (ClientFactory clientFactory = ClientFactory.withScope(scope, controllerURI);\n        EventStreamReader<String> reader = clientFactory.createReader(\"reader\",\n                                                                      readerGroup,\n                                                     new JavaSerializer<String>(),\n                                                  ReaderConfig.builder().build())) {\n        System.out.format(\"Reading all the events from %s/%s%n\", scope, streamName);\n        EventRead<String> event = null;\n        do {\n           try {\n               event = reader.readNextEvent(READER_TIMEOUT_MS);\n               if (event.getEvent() != null) {\n                   System.out.format(\"Read event '%s'%n\", event.getEvent());\n               }\n           } catch (ReinitializationRequiredException e) {\n               //There are certain circumstances where the reader needs to be reinitialized\n               e.printStackTrace();\n           }\n       } while (event.getEvent() != null);\n       System.out.format(\"No more events from %s/%s%n\", scope, streamName);\n   }  Lines 2-8 set up the Scope and Stream just like in the HelloWorldWriter\napplication. \u00a0Lines 10-15 set up the ReaderGroup as the prerequisite to creating\nthe EventStreamReader and using it to read Events from the Stream (lines 17-36).",
            "title": "HelloWorldReader"
        },
        {
            "location": "/basic-reader-and-writer/#readergroup-basics",
            "text": "Any Reader in Pravega belongs to some ReaderGroup. \u00a0A ReaderGroup is a grouping\nof one or more Readers that consume from a Stream in parallel. \u00a0Before we create\na Reader, we need to either create a ReaderGroup (or be aware of the name of an\nexisting ReaderGroup). \u00a0This application only uses the basics from ReaderGroup.  Lines 10-15 show basic ReaderGroup creation. \u00a0ReaderGroup objects are created\nfrom a ReaderGroupManager object. \u00a0The ReaderGroupManager object, in turn, is\ncreated on a given Scope with a URI to one of the Pravega Controllers, very much\nlike a ClientFactory is created. \u00a0A ReaderGroupManager object is created on line\n14. \u00a0Note the creation is also in a try-with-resources statement to make sure\nthe ReaderGroupManager is properly cleaned up. \u00a0 The ReaderGroupManager allows a\ndeveloper to create, delete and retrieve ReaderGroup objects by name.  To create a ReaderGroup, the developer needs a name for the ReaderGroup, a\nconfiguration and a set of 1 or more Streams to read from. \u00a0  The ReaderGroup's name might be meaningful to the application, like\n\"WebClickStreamReaders\". \u00a0In our case, on line 10, we have a simple UUID as the\nname (note the modification of the UUID string to remove the \"-\" character\nbecause ReaderGroup names can only have letters and numbers). \u00a0In cases where\nyou will have multiple Readers reading in parallel and each Reader in a separate\nprocess, it is helpful to have a human readable name for the ReaderGroup. \u00a0In\nour case, we have one Reader, reading in isolation, so a UUID is a safe way to\nname the ReaderGroup. \u00a0Since the ReaderGroup is created via the\nReaderGroupManager and since the ReaderGroupManager is created within the\ncontext of a Scope, we can safely conclude that ReaderGroup names are namespaced\nby that Scope. \u00a0  The ReaderGroupConfig right now doesn't have much behavior. \u00a0The developer\nspecifies where in the Stream Reader should start consuming from (a starting\nposition). \u00a0In our case, on line 11, we start at the beginning of the Stream.\n\u00a0Other configuration items, such as specifying checkpointing etc. are options\nthat will be available through the ReaderGroupConfig. \u00a0But for now, we keep it\nsimple.  The fact that a ReaderGroup can be configured to read from multiple Streams is\nkind of cool. \u00a0Imagine a situation where I have a collection of Stream of sensor\ndata coming from a factory floor, each machine has its own Stream of sensor\ndata. \u00a0I can build applications that use a ReaderGroup per Stream so that the\napp reasons about data from exactly one machine. \u00a0I can build other apps that\nuse a ReaderGroup configured to read from all of the Streams. \u00a0In our case, on\nline 14, the ReaderGroup only reads from one Stream.  You can call createReaderGroup() with the same parameters multiple times, it\ndoesn't hurt anything, and the same ReaderGroup will be returned each time after\nit is initially created.  Note that in other cases, if the developer knows the name of the ReaderGroup to\nuse and knows it has already been created, s/he can use getReaderGroup() on\nReaderGroupManager to retrieve the ReaderGroup object by name.  So at this point in the code, we have the Scope and Stream set up, we have the\nReaderGroup created and now we need to create a Reader and start reading Events.",
            "title": "ReaderGroup Basics"
        },
        {
            "location": "/basic-reader-and-writer/#reading-event-using-an-eventstreamreader",
            "text": "Lines 17-36 show an example of setting up an EventStreamReader and reading\nEvents using that EventStreamReader.  First, we create a ClientFactory on line 17, in the same way we did it in the\nHelloWorldWriter app. \u00a0  Then we use the ClientFactory to create an EventStreamReader object. \u00a0There are\nfour things the developer needs to create a Reader: a name for the reader, the\nreaderGroup it should be part of, the type of object expected on the Stream, the\nserializer to use to convert from the bytes stored in Pravega into the Event\nobjects and a ReaderConfig. \u00a0Lines 18-21 show the creation of an\nEventStreamReader. \u00a0The name of the Reader can be any valid Pravega name\n(numbers and letters). \u00a0Of course, the name of the reader is namespaced within\nthe Scope. \u00a0We talked about the creation of the ReaderGroup in the previous\nsection. \u00a0Just like with the EventStreamWriter, EventStreamReader uses Java\ngeneric types to allow a developer to specify a type safe Reader. \u00a0In our case,\nwe read Strings from the stream and use the standard Java String Serializer to\nconvert the bytes read from the stream into String objects. \u00a0Finally, the\nReaderConfig is created, but at the moment, there are no configuration items\nassociated with a Reader, so the empty ReaderConfig is just a place holder as\nPravega evolves to include configuration items on Readers.  Note that you cannot create the same Reader multiple times. \u00a0Basically overtime\nyou call createReader() it tries to add the Reader to the ReaderGroup. \u00a0If the\nReaderGroup already contains a Reader with that name, an exception is thrown.  Now that we have an EventStreamReader created, we can start using it to read\nEvents from the stream. \u00a0This is done on line 26. \u00a0The readNextEvent() operation\nreturns the next Event available on the Stream, or if there is no such Event,\nblocks for a specified timeout period. \u00a0If, after the timeout period has expired\nand no Event is available for reading, null is returned. That is why there is a\nnull check on line 27 (to avoid printing out a spurious \"null\" event message to\nthe console). \u00a0It is also used as the termination of the loop on line 34. \u00a0Note\nthat the Event itself is wrapped in an EventRead object.   It is worth noting that readNextEvent() may throw an exception (handled in lines\n30-33). \u00a0This exception would be handled in cases where the Readers in the\nReaderGroup need to be reset to a checkpoint or the ReaderGroup itself has been\naltered and the set of Streams being read has therefore been changed.   So that's it. \u00a0The simple HelloWorldReader loops, reading Events from a Stream\nuntil there are no more Events, and then the application terminates.",
            "title": "Reading Event using an EventStreamReader"
        },
        {
            "location": "/basic-reader-and-writer/#experimental-batch-reader",
            "text": "For applications that want to perform batch reads of historical stream data, the BatchClient provides a way to do this.\nIt allows for listing all of the segments in a stream, and reading their data.   When the data is read this way, rather than joining a reader group which automatically partitions the data, the underlying structure of the stream is exposed and it is up to the application to decide how to process it. So events read in this way need not be read in order.  Obviously this API is not for every application, the main advantage is that it allows for low level integration with batch processing frameworks such as MapReduce.   As an example to iterate over the segments in the stream:  Iterator<SegmentInfo> segments = client.listSegments(stream);\nSegmentInfo segmentInfo = segments.next();  Or to read the events from a segment:  SegmentIterator<T> events = client.readSegment(segmentInfo.getSegment(), deserializer);\nwhile (events.hasNext()) {\n    processEvent(events.next());\n}",
            "title": "Experimental batch reader"
        },
        {
            "location": "/reader-group-notifications/",
            "text": "Working with Pravega: ReaderGroup Notifications\n\n\nThe ReaderGroup api supports different types of notifications. Currently, we\nhave two types implemented, but we plan to add more over time.\nThe types we currently support are the following:\n\n\n\n\nSegment Notification\n\n\n\n\nA segment notification is triggered when the total number of segments managed by the\nreader group changes. During a scale operation segments can be split into\nmultiple or merged into some other segment causing the total number of segments\nto change. The total number of segments can also change when the configuration\nof the reader group changes, for example, when it adds or removes a stream.\n\n\nThe method for subscribing to segment notifications is shown below\n\n\n@Cleanup\nReaderGroupManager groupManager = new ReaderGroupManagerImpl(SCOPE, controller, clientFactory,\n        connectionFactory);\nReaderGroup readerGroup = groupManager.createReaderGroup(GROUP_NAME, ReaderGroupConfig\n        .builder().build(), Collections.singleton(STREAM));\n\nreaderGroup.getSegmentNotifier(executor).registerListener(segmentNotification -> {\n       int numOfReaders = segmentNotification.getNumOfReaders();\n       int segments = segmentNotification.getNumOfSegments();\n       if (numOfReaders < segments) {\n          //Scale up number of readers based on application capacity\n       } else {\n         //More readers available time to shut down some\n       }\n});\n\n\n\n\n\nThe application can register a listener to be notified of \nSegmentNotification\n using\nthe \nregisterListener\n api. This api takes\n\nio.pravega.client.stream.notifications.Listener\n as a parameter. Here the\napplication can add custom logic to change the set of online readers according\nto the number of segments. For example, if the number of segments increases,\nthen application might consider increasing the number of online readers. If the\nnumber of segments instead decreases according to a segment notification, then the\napplication might want to change the set of online readers accordingly.\n\n\n\n\nEndOfData Notification\n\n\n\n\nAn end of data notifier is triggered when the readers have read all the data of\nthe stream(s) managed by the reader group. This is useful to process the stream\ndata with a batch job where the application wants to read data of sealed\nstream(s).\n\n\nThe method for subscribing to end of data notifications is shown below\n\n\n@Cleanup\nReaderGroupManager groupManager = new ReaderGroupManagerImpl(SCOPE, controller, clientFactory,\n        connectionFactory);\nReaderGroup readerGroup = groupManager.createReaderGroup(GROUP_NAME, ReaderGroupConfig\n        .builder().build(), Collections.singleton(SEALED_STREAM));\n\nreaderGroup.getEndOfDataNotifier(executor).registerListener(notification -> {\n      //custom action e.g: close all readers\n});\n\n\n\n\n\nThe application can register a listener to be notified of \nEndOfDataNotification\n using\nthe \nregisterListener\n api. This api takes\n\nio.pravega.client.stream.notifications.Listener\n as a parameter. Here the\napplication can add custom logic that can be invoked once all the data of the\nsealed streams are read.",
            "title": "Working with Reader Group notifications"
        },
        {
            "location": "/reader-group-notifications/#working-with-pravega-readergroup-notifications",
            "text": "The ReaderGroup api supports different types of notifications. Currently, we\nhave two types implemented, but we plan to add more over time.\nThe types we currently support are the following:   Segment Notification   A segment notification is triggered when the total number of segments managed by the\nreader group changes. During a scale operation segments can be split into\nmultiple or merged into some other segment causing the total number of segments\nto change. The total number of segments can also change when the configuration\nof the reader group changes, for example, when it adds or removes a stream.  The method for subscribing to segment notifications is shown below  @Cleanup\nReaderGroupManager groupManager = new ReaderGroupManagerImpl(SCOPE, controller, clientFactory,\n        connectionFactory);\nReaderGroup readerGroup = groupManager.createReaderGroup(GROUP_NAME, ReaderGroupConfig\n        .builder().build(), Collections.singleton(STREAM));\n\nreaderGroup.getSegmentNotifier(executor).registerListener(segmentNotification -> {\n       int numOfReaders = segmentNotification.getNumOfReaders();\n       int segments = segmentNotification.getNumOfSegments();\n       if (numOfReaders < segments) {\n          //Scale up number of readers based on application capacity\n       } else {\n         //More readers available time to shut down some\n       }\n});  The application can register a listener to be notified of  SegmentNotification  using\nthe  registerListener  api. This api takes io.pravega.client.stream.notifications.Listener  as a parameter. Here the\napplication can add custom logic to change the set of online readers according\nto the number of segments. For example, if the number of segments increases,\nthen application might consider increasing the number of online readers. If the\nnumber of segments instead decreases according to a segment notification, then the\napplication might want to change the set of online readers accordingly.   EndOfData Notification   An end of data notifier is triggered when the readers have read all the data of\nthe stream(s) managed by the reader group. This is useful to process the stream\ndata with a batch job where the application wants to read data of sealed\nstream(s).  The method for subscribing to end of data notifications is shown below  @Cleanup\nReaderGroupManager groupManager = new ReaderGroupManagerImpl(SCOPE, controller, clientFactory,\n        connectionFactory);\nReaderGroup readerGroup = groupManager.createReaderGroup(GROUP_NAME, ReaderGroupConfig\n        .builder().build(), Collections.singleton(SEALED_STREAM));\n\nreaderGroup.getEndOfDataNotifier(executor).registerListener(notification -> {\n      //custom action e.g: close all readers\n});  The application can register a listener to be notified of  EndOfDataNotification  using\nthe  registerListener  api. This api takes io.pravega.client.stream.notifications.Listener  as a parameter. Here the\napplication can add custom logic that can be invoked once all the data of the\nsealed streams are read.",
            "title": "Working with Pravega: ReaderGroup Notifications"
        },
        {
            "location": "/state-synchronizer/",
            "text": "Working with Pravega: State Synchronizer\n\n\nYou can think about Pravega as a streaming storage primitive, because it is a\ngreat way to durably persist data. \u00a0You can think about Pravega as a great\npub-sub messaging system, because with Readers, Writers and ReaderGroups it is a\ngreat way to do messaging at scale. \u00a0But you can also think about Pravega as a\nway to implement shared state in a consistent fashion across multiple\ncooperating processes distributed in a cluster. \u00a0It is this latter category that\nwe explore with this document.\n\n\nInstructions for running the sample applications can be found in the\n\u00a0Pravega\nSamples\nreadme\n.\n\n\nYou really should be familiar with Pravega Concepts (see\u00a0\nPravega\nConcepts\n) before continuing reading this page.\n\u00a0In particular, you should be somewhat familiar with the \nState\nSynchronizer\n\nconcept.\n\n\nShared State and Pravega\n\n\nState Synchronizer is a facility provided by the Pravega programming model to\nmake it easy for developers to use Pravega to coordinate shared state between\nprocesses.\n\n\n\n\nThe idea is that a Stream is used to persist a sequence of changes to shared\nstate and that various applications use their Pravega Java Client Library to\nconcurrently read and write the shared state in a consistent fashion.\u00a0\n\n\nSharedStateMap and Shared Configuration Example\n\n\nBefore we dive into the details about how to use State Synchronizer, let's take\na quick look at an example application that uses State Synchronizer. \u00a0We have\nprovided a simple yet illustrative example of using State\nSynchronizer\u00a0\nhere.\n\n\nThe example uses State Synchronizer to build an implementation of Java's Map\ndata structure called SharedMap. \u00a0We use that primitive SharedMap data structure\nto build a Shared Config, that allows a set of processes to consistently\nread/write a shared, configuration object of key/value pair properties. \u00a0Also as\npart of that example, we provide a simple command line-based application that\nallows you to play around with the SharedConfig app.\n\n\n\n\nHere is a menu of the available commands in the SharedConfigCLI application:\n\n\nEnter one of the following commands at the command line prompt:\n\nGET_ALL - prints out all of the properties in the Shared Config.\nGET {key} - print out the configuration property for the given key.\nPUT {key} , {value} - update the Shared Config with the given key/value pair.  Print out previous value (if it existed).\nPUT_IF_ABSENT {key} , {value} - update the Shared Config with the given key/value pair, only if the property is not already defined.\nREMOVE {key} [ , {currentValue}] - remove the given property from the Shared Config.  If {currentValue} is given, remove only if the property's current value matches {currentValue}..\nREPLACE {key} , {newValue} [ , {currentValue}] - update the value of the property.  If {currentValue} is given, update only if the property's current value matches {cuurentValue}.\nCLEAR - remove all the keys from the Shared Config.\nREFRESH - force an update from the Synchronized State.\nHELP - print out a list of commands.\nQUIT - terminate the program.\n\n\n\n\nInstall the Pravega-Samples and launch two instances of the SharedConfigCLI\nusing the same scope and stream name. \u00a0This will simulate how two different\nprocesses can coordinate their local copy of the SharedConfig with one shared\nstate object. \u00a0You can follow these steps to get a feel for how the SharedConfig\nis coordinated:\n\n\n\n\n\n\n\n\n#\n\n\nProcess 1\n\n\nProcess 2\n\n\nDiscussion\n\n\n\n\n\n\n\n\n\n\n1\n\n\nGET_ALL\n\n\nGET_ALL\n\n\nShows that both processes see an empty SharedConfig\n\n\n\n\n\n\n2\n\n\nPUT\n p1,v1\n\n\n\n\nProcess 1 adds a property named p1\n\n\n\n\n\n\n3\n\n\nGET\n p1\n\n\nGET\n p1\n\n\nProcess 1 sees value v1 for the property\n\n\n\n\n\n\n\n\n\n\n\n\nProcess 2 does not have a property named p1. Why? Because it has not refreshed its state with the shared state\n\n\n\n\n\n\n4\n\n\n\n\nREFRESH\n\n\nRe-synchronize Process 2's state with the shared state\n\n\n\n\n\n\n5\n\n\n\n\nGET\n p1\n\n\nNow Process 2 sees the change Process 1 made in step 2\n\n\n\n\n\n\n6\n\n\n\n\nREPLACE\n p1, newVal, v1\n\n\nProcess 2 attempts to change the value of p1, but uses a conditional replace, meaning the change should be made only if the old value of p1 is v1 (which it is at this point)\n\n\n\n\n\n\n7\n\n\n\n\nGET\n p1\n\n\nSure enough, the value of p1 was changed to newVal\n\n\n\n\n\n\n8\n\n\nREPLACE\n p1, anotherVal, v1\n\n\n\n\nProcess 1 tries to change the value of p1 in the same way Process 2 did in step 6. This will fail because the value of p1 in shared state is no longer v1\n\n\n\n\n\n\n9\n\n\nGET\n p1\n\n\n\n\nThe failed replace operation in step 8 caused Process 1's copy of the shared state to be updated, its value is now newVal because of step 6.\n\n\n\n\n\n\n\n\nYou can repeat with a similar sequence to explore the semantics of\nPUT_IF_ABSENT and other operations that modify shared state.\n\n\nThe idea is that modifications to the SharedConfig succeed only if they operate\non the latest value. \u00a0We use optimistic concurrency to implement efficient\nconsistency across multiple consumers of the SharedConfig object.\n\n\nYou can have multiple different SharedConfig state objects running\nsimultaneously, each separate SharedConfig uses State Synchronizer objects based\non a different Pravega Stream. \u00a0Of course if you launch two applications using\nState Synchronizer objects backed by the same Stream, you get two processes\nconcurrently accessing the shared state. \u00a0This is exactly the situation we\nillustrated above.\n\n\nUsing State Synchronizer to Build the SharedMap\n\n\nWe used the State Synchronizer to build the SharedMap object in Pravega-Samples.\n\u00a0State Synchronizer can be used to build a shared version of almost any data\nstructure. \u00a0Maybe your app needs to share just a simple integer count of\nsomething; we can use State Synchronizer to build a simple shared counter.\n\u00a0Maybe the data you are sharing is a Set of currently running servers in a\ncluster; \u00a0we can use State Synchronizer to build \u00a0a shared Set. \u00a0The\npossibilities are many.\n\n\nLet's explore how to build shared objects using State Synchronizer by examining\nhow we built Shared Map.\n\n\nState Synchronizer\n\n\nState Synchronizer is a type of Pravega client, similar to an EventStreamReader\nor EventStreamWriter. \u00a0A State Synchronizer is created via a ClientFactory\nobject. \u00a0Each State Synchronizer has a unique name within a Scope. \u00a0A\nSynchronizerConfig object is used to tailor the behavior of a StateSynchronizer\n(although currently, there are no properties on a State Synchronizer that are\nconfigurable). \u00a0State Synchronizer uses Java generic types to allow a developer\nto specify a type specific State Synchronizer. \u00a0All of these things are done in\na fashion similar to how EventStreamReaders and EventStreamWriters are used.\n\n\nStateT\n\n\nWhen designing an application that uses State Synchronizer, the developer needs\nto decide what type of state is going to be synchronized (shared). \u00a0Are we\nsharing a Map? \u00a0A Set? \u00a0A Pojo? What is the data structure that is being shared.\n\u00a0This defines the core \"type\" of the State Synchronizer (the StateT generic type\nin the State Synchronizer interface). \u00a0The StateT object can be any Java object\nthat implements the \nRevisioned\n interface defined by Pravega. \u00a0\nRevisioned\n is\na simple interface that allows Pravega to ensure it can properly compare two\ndifferent StateT objects.\n\n\nIn our example, the SharedMap is the State Synchronizer application. \u00a0It defines\na simple Map object presenting the typical get(key), set (key, value) etc.\noperations you would expect from a key-value pair map object. \u00a0It\nimplements\u00a0\u00a0the\u00a0\nRevisioned\n\u00a0interface, as required to use the State\nSynchronizer, and uses a simple ConcurrentHashMap as its internal implementation\nof the Map. \u00a0So in our example, StateT corresponds to SharedStateMap\\<K,V>.\n\n\nUpdateT and InitialUpdateT\n\n\nIn addition to StateT, there are two other generic types that need to be defined\nby a StateSynchronizer app: an Update type and an InitialUpdate type). \u00a0The\nUpdateType represents the \"delta\" or change objects that are persisted on the\nPravega Stream. \u00a0The InitialUpdateType is a special update object used to to\nstart the State Synchronizer off. \u00a0Both UpdateType and InitialUpdateType are\ndefined in terms of StateT.\n\n\nThe StateSynchronizer uses a single Segment on a Stream to store updates\n(changes) to the shared state object. \u00a0Changes, in the form of Initial or Update\ntype objects, are written to the Stream based on whether the update is relative\nto the most current copy of the state in the Stream. \u00a0If an update is presented\nthat is based on an older version of the state, the update is not made.\n\n\nThe StateSynchronizer object itself keeps a local in memory copy of the state,\nit also keeps version metadata about that copy of the state. \u00a0Local state can be\nretrieved using the getState() operation. \u00a0The local in memory copy could be\nstale, and it can be refreshed by an application using the fetchUpdates()\noperation, that retrieves all the changes made to the given version of the\nstate.\n\n\nMost changes from the application are made through the updateState() operation.\n\u00a0The updateState() operation takes a Function as parameter. \u00a0The Function is\ninvoked with the latest state object, and computes the updates to be applied.\n\n\nIn our example, InitialUpdateT is implemented as:\n\n\n/**\n * Create a Map. This is used by StateSynchronizer to initialize shared state.\n */\nprivate static class CreateState<K, V> implements InitialUpdate<SharedStateMap<K,V>>, Serializable {\n    private static final long serialVersionUID = 1L;\n    private final ConcurrentHashMap<K, V> impl;\n\n    public CreateState(ConcurrentHashMap<K, V> impl) {\n        this.impl = impl;\n    }\n\n    @Override\n    public SharedStateMap<K, V> create(String scopedStreamName, Revision revision) {\n        return new SharedStateMap<K, V>(scopedStreamName, impl, revision);\n    }\n}\n\n\n\n\nIn this case, the CreateState class is used to initialize the shared state in\nthe Stream by creating a new, empty SharedStateMap object. \u00a0You could imagine\nother examples of InitialUpdate that would set a counter to 1, or perhaps\ninitialize a Set to a fixed initial set of members.\n\n\nIt may seem a bit odd that functions like \"initialize\" and \"update\" are\nexpressed as classes, but when you think about it, that makes sense. \u00a0The\nchanges, like initialize and update, need to be stored in Pravega, therefore\nthey need to be serializable objects. \u00a0It must be possible for client\napplications to be able to start at any time, compute the current state and then\nkeep up as changes are written to the Stream. \u00a0If we just stored \"the latest\nstate value\" in the Stream, there would be no way to consistently provide\nconcurrent update and read using optimistic concurrency.\n\n\nUpdateT is a bit more tricky. \u00a0There isn't just one kind of update to a Map, but\nrather there are all sorts of updates: put of a key/value pair, put of a\ncollection of key/value pairs, removing a key/value pair and clearing all of the\nkey/value pairs, \u00a0Each of these \"kinds\" of updates are represented by their own\nClass. \u00a0We define an abstract class, called StateUpdate, from which all of these\n\"operational\" update classes inherit. \u00a0\n\n\nStateUpdate abstract class\n\n\n/**\n * A base class for all updates to the shared state. This allows for several different types of updates.\n */\nprivate static abstract class StateUpdate<K,V> implements Update<SharedStateMap<K,V>>, Serializable {\n    private static final long serialVersionUID = 1L;\n\n    @Override\n    public SharedStateMap<K,V> applyTo(SharedStateMap<K,V> oldState, Revision newRevision) {\n        ConcurrentHashMap<K, V> newState = new ConcurrentHashMap<K, V>(oldState.impl);\n        process(newState);\n        return new SharedStateMap<K,V>(oldState.getScopedStreamName(), newState, newRevision);\n    }\n\n    public abstract void process(ConcurrentHashMap<K, V> updatableList);\n}\n\n\n\n\nBy defining an abstract class, we can define UpdateT in terms of the abstract\nStateUpdate class. \u00a0The abstract class implements the \"applyTo\" method that is\ninvoked by the StateSynchronizer to apply the update to the current state object\nand return an updated state object. \u00a0The actual work is done on a copy of the\nold state's underlying Map (impl) object, a \"process\" operation is applied\n(specific to each subclass) to the impl object and a new version of the\nSharedState, using the post-processed impl as the internal state. \u00a0The abstract\nclass defines a process() method that actually does the work of whatever update\nneeds to be applied. \u00a0This method is implemented by the various concrete classes\nthat represent Put, PutAll etc. operations on the shared map.\n\n\nHere, for example, is the way we implement the Put(key,value) operation on the\nSharedMap object:\n\n\nPut as an Update Object\n\n\n/**\n * Add a key/value pair to the State.\n */\nprivate static class Put<K,V> extends StateUpdate<K,V> {\n    private static final long serialVersionUID = 1L;\n    private final K key;\n    private final V value;\n\n    public Put(K key, V value) {\n        this.key = key;\n        this.value = value;\n    }\n\n    @Override\n    public void process(ConcurrentHashMap<K, V> impl) {\n        impl.put(key, value);\n    }\n}\n\n\n\n\nHere, the process() operation is to add a key/value pair to the map, or if the\nkey already exists, change the value. \u00a0Each of the \"operations\" on the SharedMap\nis implemented in terms of creating instances of the various subclasses of\nStateUpdate.\n\n\nExecuting Operations on SharedMap\n\n\nSharedMap demonstrates the typical operations on a StateSynchronizer. \u00a0SharedMap\npresents an API, very similar to Java's Map\\<K,V> interface. \u00a0It implements the\nMap operations in terms of manipulating the StateSynchronizer, using the various\nsubclasses of StateUpdate to perform state change (write) operations.\n\n\nCreate/Initialize\n\n\nCreating a SharedMap\n\n\n/**\n  * Creates the shared state using a synchronizer based on the given stream name.\n  *\n  * @param clientFactory - the Pravega ClientFactory to use to create the StateSynchronizer.\n  * @param streamManager - the Pravega StreamManager to use to create the Scope and the Stream used by the StateSynchronizer\n  * @param scope - the Scope to use to create the Stream used by the StateSynchronizer.\n  * @param name - the name of the Stream to be used by the StateSynchronizer.\n  */\n public SharedMap(ClientFactory clientFactory, StreamManager streamManager, String scope, String name){\n     streamManager.createScope(scope);\n\n     StreamConfiguration streamConfig = StreamConfiguration.builder().scope(scope).streamName(name)\n             .scalingPolicy(ScalingPolicy.fixed(1))\n             .build();\n\n     streamManager.createStream(scope, name, streamConfig);\n\n     this.stateSynchronizer = clientFactory.createStateSynchronizer(name,\n                                             new JavaSerializer<StateUpdate<K,V>>(),\n                                             new JavaSerializer<CreateState<K,V>>(),\n                                             SynchronizerConfig.builder().build());\n\n     stateSynchronizer.initialize(new CreateState<K,V>(new ConcurrentHashMap<K,V>()));\n }\n\n\n\n\nA SharedMap object is created by defining the scope and stream (almost always\nthe case, the scope and stream probably already exist, so the steps in lines\n10-16 are usually no-ops. \u00a0The StateSynchronizer object itself is constructed in\nlines 18-21 using the ClientFactory in a fashion similar to the way a Pravega\nReader or Writer would be created. \u00a0Note that the UpdateT object and\nInitialUpdateT object can have separate Java serializers specified. \u00a0Currently,\nthe SynchronizerConfig object is pretty dull; there are no configuration items\ncurrently available on the StateSynchronizer.\n\n\nThe StateSynchronizer provides an initialize() API that takes an InitialUpdate\nobject. \u00a0This is called in the SharedMap constructor to make sure the\nSharedState is properly initialized. \u00a0Note, in many cases, the SharedMap object\nwill be created on a stream that already contains shared state for the\nSharedMap. \u00a0Even in this case, it is ok to call initialize() because\ninitialize() won't modify the shared state in the Stream.\n\n\nRead Operations\n\n\nThe read operations, operations that do not alter shared state, like get(key)\n\u00a0containsValue(value) etc., work against the local copy of the\nStateSynchronizer. \u00a0All of these operations retrieve the current local state\nusing getState() and then do the read operation from that state. \u00a0The local\nstate of the StateSynchronizer might be stale. \u00a0In these cases, the SharedMap\nclient would use refresh() to force the StateSynchronizer to refresh its state\nfrom shared state using the fetchUpdates() operation on the StateSynchronizer\nobject.\n\n\nNote, this is a design decision to trade off staleness for responsiveness. \u00a0We\ncould easily have implemented the read operations to instead always do a refresh\nbefore doing the read against local state. \u00a0That would be a very efficient\nstrategy if the developer expected that there will be frequent updates to the\nshared state. \u00a0In our case, we had imagined that the SharedMap would be read\nfrequently but updated relatively infrequently, and therefore chose to read\nagainst local state.\n\n\nWrite (update) Operations\n\n\nEach write operation is implemented in terms of the various concrete StateUpdate\nobjects we discussed earlier. \u00a0The clear() operation uses the Clear subclass of\nStateUpdate to remove all the key/value pairs, put() uses the Put class, etc.\n\n\nLets dive into the implementation of the put() operation to discuss\nStateSynchronizer programming in a bit more detail:\n\n\nImplementing put(key,value)\n\n\n/**\n * Associates the specified value with the specified key in this map.\n *\n * @param key - the key at which the value should be found.\n * @param value - the value to be entered into the map.\n * @return - the previous value (if it existed) for the given key or null if the key did not exist before this operation.\n */\npublic V put(K key, V value){\n    final AtomicReference<V> oldValue = new AtomicReference<V>(null);\n    stateSynchronizer.updateState(state -> {\n        oldValue.set(state.get(key));\n        return Collections.singletonList(new Put<K,V>(key,value));\n    });\n    return oldValue.get();\n}\n\n\n\n\nIt is important to note that the function provided to the StateSynchronizer's\nupdateState() will be call potentially multiple times. The result of applying the function to the old state is written\nonly when it is applied against the most current revision of the state. \nIf there was a race and the optimistic concurrency check fails, it will be called again.\nMost of the time there will only be a small number of invocations. \u00a0In some cases, the\ndeveloper may choose to use fetchUpdates() to synchronize the StateSynchronizer\nwith the latest copy of shared state from the stream before running\nupdateState(). \u00a0This is a matter of optimizing the tradeoff between how frequent\nupdates are expected and how efficient you want the update to be. \u00a0If you expect\na lot of updates, call fetchUpdates() before calling updateState(). \u00a0In our\ncase, we didn't expect a lot of updates and therefore we process potentially\nseveral invocations of the function each time put() is called.\n\n\nDelete Operations\n\n\nWe chose to implement the delete (remove) operations to also leverage the\ncompact() feature of StateSynchronizer. \u00a0We have a policy that after every 5\nremove operations, and after every clear() operation, we do a compact operation.\n\u00a0Now, we could have chosen to do a compact() operation after every 5 update\noperations, but we wanted to isolate the illustration of using compact() to just\ndelete operations.\n\n\nYou can think of compact() as a form of \"garbage collection\" in\nStateSynchronizer. \u00a0After a certain number of changes have been written to\nSharedState, it might be efficient to write out a new initial state, an\naccumulated representation of all the changes, to the Stream. \u00a0That way data\nolder than the compact operation can be ignored and eventually removed from the\nStream.\n\n\n\n\nAs a result of the compact() operation, a new initial sate (Initial2) is written\nto the stream. \u00a0Now, all the data from Change3 and older is no longer relevant\nand can be garbage collected out of the Stream.",
            "title": "Working with State Synchronizer"
        },
        {
            "location": "/state-synchronizer/#working-with-pravega-state-synchronizer",
            "text": "You can think about Pravega as a streaming storage primitive, because it is a\ngreat way to durably persist data. \u00a0You can think about Pravega as a great\npub-sub messaging system, because with Readers, Writers and ReaderGroups it is a\ngreat way to do messaging at scale. \u00a0But you can also think about Pravega as a\nway to implement shared state in a consistent fashion across multiple\ncooperating processes distributed in a cluster. \u00a0It is this latter category that\nwe explore with this document.  Instructions for running the sample applications can be found in the \u00a0Pravega\nSamples\nreadme .  You really should be familiar with Pravega Concepts (see\u00a0 Pravega\nConcepts ) before continuing reading this page.\n\u00a0In particular, you should be somewhat familiar with the  State\nSynchronizer \nconcept.",
            "title": "Working with Pravega: State Synchronizer"
        },
        {
            "location": "/state-synchronizer/#shared-state-and-pravega",
            "text": "State Synchronizer is a facility provided by the Pravega programming model to\nmake it easy for developers to use Pravega to coordinate shared state between\nprocesses.   The idea is that a Stream is used to persist a sequence of changes to shared\nstate and that various applications use their Pravega Java Client Library to\nconcurrently read and write the shared state in a consistent fashion.",
            "title": "Shared State and Pravega"
        },
        {
            "location": "/state-synchronizer/#sharedstatemap-and-shared-configuration-example",
            "text": "Before we dive into the details about how to use State Synchronizer, let's take\na quick look at an example application that uses State Synchronizer. \u00a0We have\nprovided a simple yet illustrative example of using State\nSynchronizer\u00a0 here.  The example uses State Synchronizer to build an implementation of Java's Map\ndata structure called SharedMap. \u00a0We use that primitive SharedMap data structure\nto build a Shared Config, that allows a set of processes to consistently\nread/write a shared, configuration object of key/value pair properties. \u00a0Also as\npart of that example, we provide a simple command line-based application that\nallows you to play around with the SharedConfig app.   Here is a menu of the available commands in the SharedConfigCLI application:  Enter one of the following commands at the command line prompt:\n\nGET_ALL - prints out all of the properties in the Shared Config.\nGET {key} - print out the configuration property for the given key.\nPUT {key} , {value} - update the Shared Config with the given key/value pair.  Print out previous value (if it existed).\nPUT_IF_ABSENT {key} , {value} - update the Shared Config with the given key/value pair, only if the property is not already defined.\nREMOVE {key} [ , {currentValue}] - remove the given property from the Shared Config.  If {currentValue} is given, remove only if the property's current value matches {currentValue}..\nREPLACE {key} , {newValue} [ , {currentValue}] - update the value of the property.  If {currentValue} is given, update only if the property's current value matches {cuurentValue}.\nCLEAR - remove all the keys from the Shared Config.\nREFRESH - force an update from the Synchronized State.\nHELP - print out a list of commands.\nQUIT - terminate the program.  Install the Pravega-Samples and launch two instances of the SharedConfigCLI\nusing the same scope and stream name. \u00a0This will simulate how two different\nprocesses can coordinate their local copy of the SharedConfig with one shared\nstate object. \u00a0You can follow these steps to get a feel for how the SharedConfig\nis coordinated:     #  Process 1  Process 2  Discussion      1  GET_ALL  GET_ALL  Shows that both processes see an empty SharedConfig    2  PUT  p1,v1   Process 1 adds a property named p1    3  GET  p1  GET  p1  Process 1 sees value v1 for the property       Process 2 does not have a property named p1. Why? Because it has not refreshed its state with the shared state    4   REFRESH  Re-synchronize Process 2's state with the shared state    5   GET  p1  Now Process 2 sees the change Process 1 made in step 2    6   REPLACE  p1, newVal, v1  Process 2 attempts to change the value of p1, but uses a conditional replace, meaning the change should be made only if the old value of p1 is v1 (which it is at this point)    7   GET  p1  Sure enough, the value of p1 was changed to newVal    8  REPLACE  p1, anotherVal, v1   Process 1 tries to change the value of p1 in the same way Process 2 did in step 6. This will fail because the value of p1 in shared state is no longer v1    9  GET  p1   The failed replace operation in step 8 caused Process 1's copy of the shared state to be updated, its value is now newVal because of step 6.     You can repeat with a similar sequence to explore the semantics of\nPUT_IF_ABSENT and other operations that modify shared state.  The idea is that modifications to the SharedConfig succeed only if they operate\non the latest value. \u00a0We use optimistic concurrency to implement efficient\nconsistency across multiple consumers of the SharedConfig object.  You can have multiple different SharedConfig state objects running\nsimultaneously, each separate SharedConfig uses State Synchronizer objects based\non a different Pravega Stream. \u00a0Of course if you launch two applications using\nState Synchronizer objects backed by the same Stream, you get two processes\nconcurrently accessing the shared state. \u00a0This is exactly the situation we\nillustrated above.",
            "title": "SharedStateMap and Shared Configuration Example"
        },
        {
            "location": "/state-synchronizer/#using-state-synchronizer-to-build-the-sharedmap",
            "text": "We used the State Synchronizer to build the SharedMap object in Pravega-Samples.\n\u00a0State Synchronizer can be used to build a shared version of almost any data\nstructure. \u00a0Maybe your app needs to share just a simple integer count of\nsomething; we can use State Synchronizer to build a simple shared counter.\n\u00a0Maybe the data you are sharing is a Set of currently running servers in a\ncluster; \u00a0we can use State Synchronizer to build \u00a0a shared Set. \u00a0The\npossibilities are many.  Let's explore how to build shared objects using State Synchronizer by examining\nhow we built Shared Map.",
            "title": "Using State Synchronizer to Build the SharedMap"
        },
        {
            "location": "/state-synchronizer/#state-synchronizer",
            "text": "State Synchronizer is a type of Pravega client, similar to an EventStreamReader\nor EventStreamWriter. \u00a0A State Synchronizer is created via a ClientFactory\nobject. \u00a0Each State Synchronizer has a unique name within a Scope. \u00a0A\nSynchronizerConfig object is used to tailor the behavior of a StateSynchronizer\n(although currently, there are no properties on a State Synchronizer that are\nconfigurable). \u00a0State Synchronizer uses Java generic types to allow a developer\nto specify a type specific State Synchronizer. \u00a0All of these things are done in\na fashion similar to how EventStreamReaders and EventStreamWriters are used.",
            "title": "State Synchronizer"
        },
        {
            "location": "/state-synchronizer/#statet",
            "text": "When designing an application that uses State Synchronizer, the developer needs\nto decide what type of state is going to be synchronized (shared). \u00a0Are we\nsharing a Map? \u00a0A Set? \u00a0A Pojo? What is the data structure that is being shared.\n\u00a0This defines the core \"type\" of the State Synchronizer (the StateT generic type\nin the State Synchronizer interface). \u00a0The StateT object can be any Java object\nthat implements the  Revisioned  interface defined by Pravega. \u00a0 Revisioned  is\na simple interface that allows Pravega to ensure it can properly compare two\ndifferent StateT objects.  In our example, the SharedMap is the State Synchronizer application. \u00a0It defines\na simple Map object presenting the typical get(key), set (key, value) etc.\noperations you would expect from a key-value pair map object. \u00a0It\nimplements\u00a0\u00a0the\u00a0 Revisioned \u00a0interface, as required to use the State\nSynchronizer, and uses a simple ConcurrentHashMap as its internal implementation\nof the Map. \u00a0So in our example, StateT corresponds to SharedStateMap\\<K,V>.",
            "title": "StateT"
        },
        {
            "location": "/state-synchronizer/#updatet-and-initialupdatet",
            "text": "In addition to StateT, there are two other generic types that need to be defined\nby a StateSynchronizer app: an Update type and an InitialUpdate type). \u00a0The\nUpdateType represents the \"delta\" or change objects that are persisted on the\nPravega Stream. \u00a0The InitialUpdateType is a special update object used to to\nstart the State Synchronizer off. \u00a0Both UpdateType and InitialUpdateType are\ndefined in terms of StateT.  The StateSynchronizer uses a single Segment on a Stream to store updates\n(changes) to the shared state object. \u00a0Changes, in the form of Initial or Update\ntype objects, are written to the Stream based on whether the update is relative\nto the most current copy of the state in the Stream. \u00a0If an update is presented\nthat is based on an older version of the state, the update is not made.  The StateSynchronizer object itself keeps a local in memory copy of the state,\nit also keeps version metadata about that copy of the state. \u00a0Local state can be\nretrieved using the getState() operation. \u00a0The local in memory copy could be\nstale, and it can be refreshed by an application using the fetchUpdates()\noperation, that retrieves all the changes made to the given version of the\nstate.  Most changes from the application are made through the updateState() operation.\n\u00a0The updateState() operation takes a Function as parameter. \u00a0The Function is\ninvoked with the latest state object, and computes the updates to be applied.  In our example, InitialUpdateT is implemented as:  /**\n * Create a Map. This is used by StateSynchronizer to initialize shared state.\n */\nprivate static class CreateState<K, V> implements InitialUpdate<SharedStateMap<K,V>>, Serializable {\n    private static final long serialVersionUID = 1L;\n    private final ConcurrentHashMap<K, V> impl;\n\n    public CreateState(ConcurrentHashMap<K, V> impl) {\n        this.impl = impl;\n    }\n\n    @Override\n    public SharedStateMap<K, V> create(String scopedStreamName, Revision revision) {\n        return new SharedStateMap<K, V>(scopedStreamName, impl, revision);\n    }\n}  In this case, the CreateState class is used to initialize the shared state in\nthe Stream by creating a new, empty SharedStateMap object. \u00a0You could imagine\nother examples of InitialUpdate that would set a counter to 1, or perhaps\ninitialize a Set to a fixed initial set of members.  It may seem a bit odd that functions like \"initialize\" and \"update\" are\nexpressed as classes, but when you think about it, that makes sense. \u00a0The\nchanges, like initialize and update, need to be stored in Pravega, therefore\nthey need to be serializable objects. \u00a0It must be possible for client\napplications to be able to start at any time, compute the current state and then\nkeep up as changes are written to the Stream. \u00a0If we just stored \"the latest\nstate value\" in the Stream, there would be no way to consistently provide\nconcurrent update and read using optimistic concurrency.  UpdateT is a bit more tricky. \u00a0There isn't just one kind of update to a Map, but\nrather there are all sorts of updates: put of a key/value pair, put of a\ncollection of key/value pairs, removing a key/value pair and clearing all of the\nkey/value pairs, \u00a0Each of these \"kinds\" of updates are represented by their own\nClass. \u00a0We define an abstract class, called StateUpdate, from which all of these\n\"operational\" update classes inherit. \u00a0  StateUpdate abstract class  /**\n * A base class for all updates to the shared state. This allows for several different types of updates.\n */\nprivate static abstract class StateUpdate<K,V> implements Update<SharedStateMap<K,V>>, Serializable {\n    private static final long serialVersionUID = 1L;\n\n    @Override\n    public SharedStateMap<K,V> applyTo(SharedStateMap<K,V> oldState, Revision newRevision) {\n        ConcurrentHashMap<K, V> newState = new ConcurrentHashMap<K, V>(oldState.impl);\n        process(newState);\n        return new SharedStateMap<K,V>(oldState.getScopedStreamName(), newState, newRevision);\n    }\n\n    public abstract void process(ConcurrentHashMap<K, V> updatableList);\n}  By defining an abstract class, we can define UpdateT in terms of the abstract\nStateUpdate class. \u00a0The abstract class implements the \"applyTo\" method that is\ninvoked by the StateSynchronizer to apply the update to the current state object\nand return an updated state object. \u00a0The actual work is done on a copy of the\nold state's underlying Map (impl) object, a \"process\" operation is applied\n(specific to each subclass) to the impl object and a new version of the\nSharedState, using the post-processed impl as the internal state. \u00a0The abstract\nclass defines a process() method that actually does the work of whatever update\nneeds to be applied. \u00a0This method is implemented by the various concrete classes\nthat represent Put, PutAll etc. operations on the shared map.  Here, for example, is the way we implement the Put(key,value) operation on the\nSharedMap object:  Put as an Update Object  /**\n * Add a key/value pair to the State.\n */\nprivate static class Put<K,V> extends StateUpdate<K,V> {\n    private static final long serialVersionUID = 1L;\n    private final K key;\n    private final V value;\n\n    public Put(K key, V value) {\n        this.key = key;\n        this.value = value;\n    }\n\n    @Override\n    public void process(ConcurrentHashMap<K, V> impl) {\n        impl.put(key, value);\n    }\n}  Here, the process() operation is to add a key/value pair to the map, or if the\nkey already exists, change the value. \u00a0Each of the \"operations\" on the SharedMap\nis implemented in terms of creating instances of the various subclasses of\nStateUpdate.",
            "title": "UpdateT and InitialUpdateT"
        },
        {
            "location": "/state-synchronizer/#executing-operations-on-sharedmap",
            "text": "SharedMap demonstrates the typical operations on a StateSynchronizer. \u00a0SharedMap\npresents an API, very similar to Java's Map\\<K,V> interface. \u00a0It implements the\nMap operations in terms of manipulating the StateSynchronizer, using the various\nsubclasses of StateUpdate to perform state change (write) operations.",
            "title": "Executing Operations on SharedMap"
        },
        {
            "location": "/state-synchronizer/#createinitialize",
            "text": "Creating a SharedMap  /**\n  * Creates the shared state using a synchronizer based on the given stream name.\n  *\n  * @param clientFactory - the Pravega ClientFactory to use to create the StateSynchronizer.\n  * @param streamManager - the Pravega StreamManager to use to create the Scope and the Stream used by the StateSynchronizer\n  * @param scope - the Scope to use to create the Stream used by the StateSynchronizer.\n  * @param name - the name of the Stream to be used by the StateSynchronizer.\n  */\n public SharedMap(ClientFactory clientFactory, StreamManager streamManager, String scope, String name){\n     streamManager.createScope(scope);\n\n     StreamConfiguration streamConfig = StreamConfiguration.builder().scope(scope).streamName(name)\n             .scalingPolicy(ScalingPolicy.fixed(1))\n             .build();\n\n     streamManager.createStream(scope, name, streamConfig);\n\n     this.stateSynchronizer = clientFactory.createStateSynchronizer(name,\n                                             new JavaSerializer<StateUpdate<K,V>>(),\n                                             new JavaSerializer<CreateState<K,V>>(),\n                                             SynchronizerConfig.builder().build());\n\n     stateSynchronizer.initialize(new CreateState<K,V>(new ConcurrentHashMap<K,V>()));\n }  A SharedMap object is created by defining the scope and stream (almost always\nthe case, the scope and stream probably already exist, so the steps in lines\n10-16 are usually no-ops. \u00a0The StateSynchronizer object itself is constructed in\nlines 18-21 using the ClientFactory in a fashion similar to the way a Pravega\nReader or Writer would be created. \u00a0Note that the UpdateT object and\nInitialUpdateT object can have separate Java serializers specified. \u00a0Currently,\nthe SynchronizerConfig object is pretty dull; there are no configuration items\ncurrently available on the StateSynchronizer.  The StateSynchronizer provides an initialize() API that takes an InitialUpdate\nobject. \u00a0This is called in the SharedMap constructor to make sure the\nSharedState is properly initialized. \u00a0Note, in many cases, the SharedMap object\nwill be created on a stream that already contains shared state for the\nSharedMap. \u00a0Even in this case, it is ok to call initialize() because\ninitialize() won't modify the shared state in the Stream.",
            "title": "Create/Initialize"
        },
        {
            "location": "/state-synchronizer/#read-operations",
            "text": "The read operations, operations that do not alter shared state, like get(key)\n\u00a0containsValue(value) etc., work against the local copy of the\nStateSynchronizer. \u00a0All of these operations retrieve the current local state\nusing getState() and then do the read operation from that state. \u00a0The local\nstate of the StateSynchronizer might be stale. \u00a0In these cases, the SharedMap\nclient would use refresh() to force the StateSynchronizer to refresh its state\nfrom shared state using the fetchUpdates() operation on the StateSynchronizer\nobject.  Note, this is a design decision to trade off staleness for responsiveness. \u00a0We\ncould easily have implemented the read operations to instead always do a refresh\nbefore doing the read against local state. \u00a0That would be a very efficient\nstrategy if the developer expected that there will be frequent updates to the\nshared state. \u00a0In our case, we had imagined that the SharedMap would be read\nfrequently but updated relatively infrequently, and therefore chose to read\nagainst local state.",
            "title": "Read Operations"
        },
        {
            "location": "/state-synchronizer/#write-update-operations",
            "text": "Each write operation is implemented in terms of the various concrete StateUpdate\nobjects we discussed earlier. \u00a0The clear() operation uses the Clear subclass of\nStateUpdate to remove all the key/value pairs, put() uses the Put class, etc.  Lets dive into the implementation of the put() operation to discuss\nStateSynchronizer programming in a bit more detail:  Implementing put(key,value)  /**\n * Associates the specified value with the specified key in this map.\n *\n * @param key - the key at which the value should be found.\n * @param value - the value to be entered into the map.\n * @return - the previous value (if it existed) for the given key or null if the key did not exist before this operation.\n */\npublic V put(K key, V value){\n    final AtomicReference<V> oldValue = new AtomicReference<V>(null);\n    stateSynchronizer.updateState(state -> {\n        oldValue.set(state.get(key));\n        return Collections.singletonList(new Put<K,V>(key,value));\n    });\n    return oldValue.get();\n}  It is important to note that the function provided to the StateSynchronizer's\nupdateState() will be call potentially multiple times. The result of applying the function to the old state is written\nonly when it is applied against the most current revision of the state. \nIf there was a race and the optimistic concurrency check fails, it will be called again.\nMost of the time there will only be a small number of invocations. \u00a0In some cases, the\ndeveloper may choose to use fetchUpdates() to synchronize the StateSynchronizer\nwith the latest copy of shared state from the stream before running\nupdateState(). \u00a0This is a matter of optimizing the tradeoff between how frequent\nupdates are expected and how efficient you want the update to be. \u00a0If you expect\na lot of updates, call fetchUpdates() before calling updateState(). \u00a0In our\ncase, we didn't expect a lot of updates and therefore we process potentially\nseveral invocations of the function each time put() is called.",
            "title": "Write (update) Operations"
        },
        {
            "location": "/state-synchronizer/#delete-operations",
            "text": "We chose to implement the delete (remove) operations to also leverage the\ncompact() feature of StateSynchronizer. \u00a0We have a policy that after every 5\nremove operations, and after every clear() operation, we do a compact operation.\n\u00a0Now, we could have chosen to do a compact() operation after every 5 update\noperations, but we wanted to isolate the illustration of using compact() to just\ndelete operations.  You can think of compact() as a form of \"garbage collection\" in\nStateSynchronizer. \u00a0After a certain number of changes have been written to\nSharedState, it might be efficient to write out a new initial state, an\naccumulated representation of all the changes, to the Stream. \u00a0That way data\nolder than the compact operation can be ignored and eventually removed from the\nStream.   As a result of the compact() operation, a new initial sate (Initial2) is written\nto the stream. \u00a0Now, all the data from Change3 and older is no longer relevant\nand can be garbage collected out of the Stream.",
            "title": "Delete Operations"
        },
        {
            "location": "/transactions/",
            "text": "Working with Pravega: Transactions\n\n\nThis article explores how to write a set of Events to a Stream atomically using\nPravega Transactions.\n\n\nInstructions for running the sample applications can be found in the\n\u00a0Pravega\nSamples\nreadme\n.\n\n\nYou really should be familiar with Pravega Concepts (see\u00a0\nPravega\nConcepts\n) before continuing reading this page.\n\n\nPravega Transactions and the Console Writer and Console Reader Apps\n\n\nWe have written a couple of applications, ConsoleReader and ConsoleWriter that\nhelp illustrate reading and writing data with Pravega and in particular to\nillustrate the Transaction facility in the Pravega programming model. \u00a0You can\nfind those applications\n\nhere\n.\n\n\nConsoleReader\n\n\nThe ConsoleReader app is very simple. \u00a0It uses the Pravega Java Client Library\nto read from a Stream and output each Event onto the console. \u00a0It runs\nindefinitely, so you have to kill the process to terminate the program.\n\n\nConsoleWriter\n\n\nThe ConsoleWriter app is a bit more sophisticated. \u00a0It uses the Pravega Java\nClient Library to write Events to a Stream, including Events written in the\ncontext of a Pravega Transaction. \u00a0To make manipulating Transactions a bit\neasier, we provide a console-based CLI. \u00a0The help text for the CLI is shown\nbelow:\n\n\nConsoleWriter Help text\n\n\nEnter one of the following commands at the command line prompt:\n\nIf no command is entered, the line is treated as a parameter to the WRITE_EVENT command.\n\nWRITE_EVENT {event} - write the {event} out to the Stream or the current Transaction.\nWRITE_EVENT_RK <<{routingKey}>> , {event} - write the {event} out to the Stream or the current Transaction using {routingKey}. Note << and >> around {routingKey}.\nBEGIN [{transactionTimeout}] [, {maxExecutionTime}] [, {scaleGracePeriod}] begin a Transaction. Only one Transaction at a time is supported by the CLI.\nGET_TXN_ID - output the current Transaction's Id (if a Transaction is running)\nFLUSH - flush the current Transaction (if a Transaction is running)\nPING [{lease}] - refresh the time remaining on the Transaction (if a Transaction is running)\nCOMMIT - commit the Transaction (if a Transaction is running)\nABORT - abort the Transaction (if a Transaction is running)\nSTATUS - check the status of the Transaction(if a Transaction is running)\nHELP - print out a list of commands.\nQUIT - terminate the program.\n\nexamples/someStream >\n\n\n\n\nSo writing a single Event is simple, just type some text (you don't even have to\ntype the WRITE_EVENT command if you don't want to).\n\n\nBut we really want to talk about Pravega Transactions, so lets dive into that.\n\n\nPravega Transactions\n\n\nThe idea with a Pravega Transaction is that it allows an application to prepare\na set of Events that can be written \"all at once\" to a Stream. \u00a0This allows an\napplication to \"commit\" a bunch of Events Atomically. This is done by writing them into the Transaction\nand calling commit to append them to the Stream. \u00a0An application might\nwant to do this in cases where it wants the Events to be durably stored and\nlater decided whether or not those Events should be\nappended to the Stream. \u00a0This allows the application\nto control when the set of Events are made visible to Readers.\n\n\nA Transaction is created via an EventStreamWriter. \u00a0Recall that an\nEventStreamWriter itself is created through a ClientFactory and is constructed\nto operate against a Stream. \u00a0Transactions are therefore bound to a Stream.\n\u00a0Once a Transaction is created, it acts a lot like a Writer. \u00a0Applications Write\nEvents to the Transaction and once acknowledged, the data is considered durably\npersisted in the Transaction. \u00a0Note that the data written to a Transaction will\nnot be visible to Readers until the Transaction is committed. \u00a0In addition to\nwriteEvent and writeEvent using a routing key, there are several Transaction\nspecific operations provided:\n\n\n\n\n\n\n\n\nOperation\n\n\nDiscussion\n\n\n\n\n\n\n\n\n\n\ngetTxnId()\n\n\nRetrieve the unique identifier for the Transaction.\n\n\n\n\n\n\n\n\nPravega generates a unique UUID for each Transaction.\n\n\n\n\n\n\nflush()\n\n\nEnsure that all Writes have been persisted.\n\n\n\n\n\n\nping()\n\n\nExtend the duration of a Transaction.\n\n\n\n\n\n\n\n\nNote that after a certain amount of idle time, the Transaction will automatically abort. This is to handle the case where the client has crashed and it is no longer appropriate to keep resources associated with the Transaction.\n\n\n\n\n\n\ncheckStatus()\n\n\nReturn the state of the Transaction. The Transaction can be in one of the following states: Open, Committing, Committed, Aborting or Aborted.\n\n\n\n\n\n\ncommit()\n\n\nAppend all of the Events written to the Transaction into the Stream. Either all of the Event data will be appended to the Stream or none of it will be.\n\n\n\n\n\n\nabort()\n\n\nTerminate the Transaction, the data written to the Transaction will be deleted.\n\n\n\n\n\n\n\n\nUsing the ConsoleWriter to Begin and Commit a Transaction\n\n\nAll of the Transaction API is reflected in the ConsoleWriter's CLI command set.\n\n\nTo begin a transaction, type BEGIN:\n\n\nBegin Transaction\n\n\nexamples/someStream >begin\n346d8561-3fd8-40b6-8c15-9343eeea2992 >\n\n\n\n\nWhen a Transaction is created, it returns a Transaction object parameterized to\nthe type of Event supported by the Stream. \u00a0In the case of the ConsoleWriter,\nthe type of Event is a Java String.\n\n\nThe command prompt changes to show the Transaction's id. \u00a0Now any of the\nTransaction related commands can be issued (GET_TXN_ID, FLUSH, PING, COMMIT,\nABORT and STATUS). \u00a0Note that the BEGIN command won't work because the\nConsoleWriter supports only one Transaction at a time (this is a limitation of\nthe app, not a limitation of Pravega). \u00a0When the ConsoleWriter is in a\nTransactional context, the WRITE_EVENT (remember if you don't type a command,\nConsoleWriter assumes you want to write the text as an Event) or the\nWRITE_EVENT_RK will be written to the Transaction:\n\n\nWrite Events to a Transaction\n\n\n346d8561-3fd8-40b6-8c15-9343eeea2992 >m1\n**** Wrote 'm1'\n346d8561-3fd8-40b6-8c15-9343eeea2992 >m2\n**** Wrote 'm2'\n346d8561-3fd8-40b6-8c15-9343eeea2992 >m3\n**** Wrote 'm3'\n\n\n\n\nAt this point, if you look at the Stream (by invoking the ConsoleReader app on\nthe Stream, for example), you won't see those Events written to the Stream.\n\n\nEvents not Written to the Stream (yet)\n\n\n$ bin/consoleReader\n...\n******** Reading events from examples/someStream\n\n\n\n\nBut when a COMMIT command is given, causing the Transaction to commit:\n\n\nDo the Commit\n\n\n346d8561-3fd8-40b6-8c15-9343eeea2992 >commit\n**** Transaction commit completed.\n\n\n\n\nthose Events are appended to the Stream and are now all available:\n\n\nAfter commit, the Events are Visible\n\n\n******** Reading events from examples/someStream\n'm1'\n'm2'\n'm3'\n\n\n\n\nMore on Begin Transaction\n\n\nThe Begin Transaction (beginTxn()) operation takes three parameters\n(ConsoleWriter chooses some reasonable defaults so in the CLI these are\noptional):\u00a0\n\n\n\n\n\n\n\n\nParam\n\n\nDiscussion\n\n\n\n\n\n\n\n\n\n\ntransactionTimeout\n\n\nThe amount of time a transaction should be allowed to run before it is automatically aborted by Pravega.\n\n\n\n\n\n\n\n\nThis is also referred to as a \"lease\".\n\n\n\n\n\n\nmaxExecutionTime\n\n\nThe amount of time allowed between ping operations.\n\n\n\n\n\n\nscaleGracePeriod\n\n\nAn additional amount of time, after a Stream scaling operation has taken place, that the Transaction can remain alive",
            "title": "Working with Transactions"
        },
        {
            "location": "/transactions/#working-with-pravega-transactions",
            "text": "This article explores how to write a set of Events to a Stream atomically using\nPravega Transactions.  Instructions for running the sample applications can be found in the \u00a0Pravega\nSamples\nreadme .  You really should be familiar with Pravega Concepts (see\u00a0 Pravega\nConcepts ) before continuing reading this page.",
            "title": "Working with Pravega: Transactions"
        },
        {
            "location": "/transactions/#pravega-transactions-and-the-console-writer-and-console-reader-apps",
            "text": "We have written a couple of applications, ConsoleReader and ConsoleWriter that\nhelp illustrate reading and writing data with Pravega and in particular to\nillustrate the Transaction facility in the Pravega programming model. \u00a0You can\nfind those applications here .",
            "title": "Pravega Transactions and the Console Writer and Console Reader Apps"
        },
        {
            "location": "/transactions/#consolereader",
            "text": "The ConsoleReader app is very simple. \u00a0It uses the Pravega Java Client Library\nto read from a Stream and output each Event onto the console. \u00a0It runs\nindefinitely, so you have to kill the process to terminate the program.",
            "title": "ConsoleReader"
        },
        {
            "location": "/transactions/#consolewriter",
            "text": "The ConsoleWriter app is a bit more sophisticated. \u00a0It uses the Pravega Java\nClient Library to write Events to a Stream, including Events written in the\ncontext of a Pravega Transaction. \u00a0To make manipulating Transactions a bit\neasier, we provide a console-based CLI. \u00a0The help text for the CLI is shown\nbelow:  ConsoleWriter Help text  Enter one of the following commands at the command line prompt:\n\nIf no command is entered, the line is treated as a parameter to the WRITE_EVENT command.\n\nWRITE_EVENT {event} - write the {event} out to the Stream or the current Transaction.\nWRITE_EVENT_RK <<{routingKey}>> , {event} - write the {event} out to the Stream or the current Transaction using {routingKey}. Note << and >> around {routingKey}.\nBEGIN [{transactionTimeout}] [, {maxExecutionTime}] [, {scaleGracePeriod}] begin a Transaction. Only one Transaction at a time is supported by the CLI.\nGET_TXN_ID - output the current Transaction's Id (if a Transaction is running)\nFLUSH - flush the current Transaction (if a Transaction is running)\nPING [{lease}] - refresh the time remaining on the Transaction (if a Transaction is running)\nCOMMIT - commit the Transaction (if a Transaction is running)\nABORT - abort the Transaction (if a Transaction is running)\nSTATUS - check the status of the Transaction(if a Transaction is running)\nHELP - print out a list of commands.\nQUIT - terminate the program.\n\nexamples/someStream >  So writing a single Event is simple, just type some text (you don't even have to\ntype the WRITE_EVENT command if you don't want to).  But we really want to talk about Pravega Transactions, so lets dive into that.",
            "title": "ConsoleWriter"
        },
        {
            "location": "/transactions/#pravega-transactions",
            "text": "The idea with a Pravega Transaction is that it allows an application to prepare\na set of Events that can be written \"all at once\" to a Stream. \u00a0This allows an\napplication to \"commit\" a bunch of Events Atomically. This is done by writing them into the Transaction\nand calling commit to append them to the Stream. \u00a0An application might\nwant to do this in cases where it wants the Events to be durably stored and\nlater decided whether or not those Events should be\nappended to the Stream. \u00a0This allows the application\nto control when the set of Events are made visible to Readers.  A Transaction is created via an EventStreamWriter. \u00a0Recall that an\nEventStreamWriter itself is created through a ClientFactory and is constructed\nto operate against a Stream. \u00a0Transactions are therefore bound to a Stream.\n\u00a0Once a Transaction is created, it acts a lot like a Writer. \u00a0Applications Write\nEvents to the Transaction and once acknowledged, the data is considered durably\npersisted in the Transaction. \u00a0Note that the data written to a Transaction will\nnot be visible to Readers until the Transaction is committed. \u00a0In addition to\nwriteEvent and writeEvent using a routing key, there are several Transaction\nspecific operations provided:     Operation  Discussion      getTxnId()  Retrieve the unique identifier for the Transaction.     Pravega generates a unique UUID for each Transaction.    flush()  Ensure that all Writes have been persisted.    ping()  Extend the duration of a Transaction.     Note that after a certain amount of idle time, the Transaction will automatically abort. This is to handle the case where the client has crashed and it is no longer appropriate to keep resources associated with the Transaction.    checkStatus()  Return the state of the Transaction. The Transaction can be in one of the following states: Open, Committing, Committed, Aborting or Aborted.    commit()  Append all of the Events written to the Transaction into the Stream. Either all of the Event data will be appended to the Stream or none of it will be.    abort()  Terminate the Transaction, the data written to the Transaction will be deleted.",
            "title": "Pravega Transactions"
        },
        {
            "location": "/transactions/#using-the-consolewriter-to-begin-and-commit-a-transaction",
            "text": "All of the Transaction API is reflected in the ConsoleWriter's CLI command set.  To begin a transaction, type BEGIN:  Begin Transaction  examples/someStream >begin\n346d8561-3fd8-40b6-8c15-9343eeea2992 >  When a Transaction is created, it returns a Transaction object parameterized to\nthe type of Event supported by the Stream. \u00a0In the case of the ConsoleWriter,\nthe type of Event is a Java String.  The command prompt changes to show the Transaction's id. \u00a0Now any of the\nTransaction related commands can be issued (GET_TXN_ID, FLUSH, PING, COMMIT,\nABORT and STATUS). \u00a0Note that the BEGIN command won't work because the\nConsoleWriter supports only one Transaction at a time (this is a limitation of\nthe app, not a limitation of Pravega). \u00a0When the ConsoleWriter is in a\nTransactional context, the WRITE_EVENT (remember if you don't type a command,\nConsoleWriter assumes you want to write the text as an Event) or the\nWRITE_EVENT_RK will be written to the Transaction:  Write Events to a Transaction  346d8561-3fd8-40b6-8c15-9343eeea2992 >m1\n**** Wrote 'm1'\n346d8561-3fd8-40b6-8c15-9343eeea2992 >m2\n**** Wrote 'm2'\n346d8561-3fd8-40b6-8c15-9343eeea2992 >m3\n**** Wrote 'm3'  At this point, if you look at the Stream (by invoking the ConsoleReader app on\nthe Stream, for example), you won't see those Events written to the Stream.  Events not Written to the Stream (yet)  $ bin/consoleReader\n...\n******** Reading events from examples/someStream  But when a COMMIT command is given, causing the Transaction to commit:  Do the Commit  346d8561-3fd8-40b6-8c15-9343eeea2992 >commit\n**** Transaction commit completed.  those Events are appended to the Stream and are now all available:  After commit, the Events are Visible  ******** Reading events from examples/someStream\n'm1'\n'm2'\n'm3'",
            "title": "Using the ConsoleWriter to Begin and Commit a Transaction"
        },
        {
            "location": "/transactions/#more-on-begin-transaction",
            "text": "The Begin Transaction (beginTxn()) operation takes three parameters\n(ConsoleWriter chooses some reasonable defaults so in the CLI these are\noptional):\u00a0     Param  Discussion      transactionTimeout  The amount of time a transaction should be allowed to run before it is automatically aborted by Pravega.     This is also referred to as a \"lease\".    maxExecutionTime  The amount of time allowed between ping operations.    scaleGracePeriod  An additional amount of time, after a Stream scaling operation has taken place, that the Transaction can remain alive",
            "title": "More on Begin Transaction"
        },
        {
            "location": "/deployment/deployment/",
            "text": "Pravega Deployment Overview\n\n\nThis guide describes the options for running Pravega for development, testing and in production.\n\n\nPravega Modes\n\n\nThere are two modes for running Pravega.\n\n\n\n\nStandalone - Standalone mode is suitable for development and testing Pravega applications. It can either be run from the source code, from the distribution package or as a docker container.\n\n\nDistributed - Distributed mode runs each component separately on a single or multiple nodes. This is suitable for production in addition for development and testing. There are deployment options available including a manual installation, running in a docker swarm or DC/OS.\n\n\n\n\nPrerequisites\n\n\nThe following prerequisites are required for running pravega in all modes.\n\n\n\n\nJava 8\n\n\n\n\nThe following prerequisites are required for running in production. These are only required for running in distributed mode.\n\n\n\n\nExternal HDFS 2.7\n\n\nZookeeper 3.5.1-alpha\n\n\nBookkeeper 4.4.0\n\n\n\n\nFor more details on the prerequisites and recommended configuration options for bookkeeper see the \nManual Install Guide\n.\n\n\nInstallation\n\n\nThere are multiple options provided for running Pravega in different environments. Most of these use the installation package from a Pravega release. You can find the latest Pravega release on the \ngithub releases page\n.\n\n\n\n\nLocal\n - Running Pravega locally is suitable for development and testing.\n\n\nRunning from source\n\n\nLocal Standalone Mode\n\n\nDocker Compose (Distributed Mode)\n\n\n\n\n\n\nProduction - Multi-node installation suitable for running in production.\n\n\nManual Installation\n\n\nDocker Swarm\n\n\nDC/OS\n\n\nCloud - \nAWS",
            "title": "Deployment Overview"
        },
        {
            "location": "/deployment/deployment/#pravega-deployment-overview",
            "text": "This guide describes the options for running Pravega for development, testing and in production.",
            "title": "Pravega Deployment Overview"
        },
        {
            "location": "/deployment/deployment/#pravega-modes",
            "text": "There are two modes for running Pravega.   Standalone - Standalone mode is suitable for development and testing Pravega applications. It can either be run from the source code, from the distribution package or as a docker container.  Distributed - Distributed mode runs each component separately on a single or multiple nodes. This is suitable for production in addition for development and testing. There are deployment options available including a manual installation, running in a docker swarm or DC/OS.",
            "title": "Pravega Modes"
        },
        {
            "location": "/deployment/deployment/#prerequisites",
            "text": "The following prerequisites are required for running pravega in all modes.   Java 8   The following prerequisites are required for running in production. These are only required for running in distributed mode.   External HDFS 2.7  Zookeeper 3.5.1-alpha  Bookkeeper 4.4.0   For more details on the prerequisites and recommended configuration options for bookkeeper see the  Manual Install Guide .",
            "title": "Prerequisites"
        },
        {
            "location": "/deployment/deployment/#installation",
            "text": "There are multiple options provided for running Pravega in different environments. Most of these use the installation package from a Pravega release. You can find the latest Pravega release on the  github releases page .   Local  - Running Pravega locally is suitable for development and testing.  Running from source  Local Standalone Mode  Docker Compose (Distributed Mode)    Production - Multi-node installation suitable for running in production.  Manual Installation  Docker Swarm  DC/OS  Cloud -  AWS",
            "title": "Installation"
        },
        {
            "location": "/deployment/run-local/",
            "text": "Running Pravega Locally\n\n\nRunning locally allows you to get started using pravega very quickly. Most of the options use the standalone mode which is suitable for most development and testing.\n\n\nAll of the options for running locally start the required prerequisites so you can get started right away.\n\n\nStandalone Mode\n\n\nFrom Source\n\n\nFirst you need to have the Pravega source code checked out:\n\n\ngit clone https://github.com/pravega/pravega.git\ncd pravega\n\n\n\n\nThis one command will download dependencies, compile pravega and start the standalone deployment.\n\n\n./gradlew startStandalone\n\n\n\n\nFrom Installation Package\n\n\nDownload the Pravega release from the \ngithub releases page\n. The tarball and zip are identical. Instructions are provided using tar but the same steps work with the zip file.\n\n\ntar xfvz pravega-0.1.0.tgz\n\n\n\n\nRun pravega standalone:\n\n\npravega-0.1.0/bin/pravega-standalone\n\n\n\n\nFrom Docker Container\n\n\nThis will download and run Pravega from the container image on docker hub.\n\n\nNote, you must replace the \n<ip>\n with the IP of your machine. This is so that you can connect to Pravega from your local system. Optionally you can replace \nlatest\n with the version of Pravega your desire\n\n\ndocker run -it -e HOST_IP=<ip> -p 9090:9090 -p 12345:12345 pravega/pravega:latest standalone\n\n\n\n\nDocker Compose (Distributed Mode)\n\n\nUnlike other options for running locally, the docker compose option runs a full Pravega install in distributed mode. It contains containers for running Zookeeper, Bookkeeper and HDFS so Pravega operates as if it would in production. This is as easy to get started with as the standalone option but requires additional resources available.\n\n\nTo use this you need to have Docker \n1.12\n or later.\n\n\nDownload the \ndocker-compose.yml\n from github. For example:\n\n\nwget https://github.com/pravega/pravega/tree/master/docker/compose/docker-compose.yml\n\n\n\n\nYou need to set the IP address of your local machine as the value of HOST_IP in the following command. To run:\n\n\nHOST_IP=1.2.3.4 docker-compose up\n\n\n\n\nClients can then connect to the controller at \n${HOST_IP}:9090\n.",
            "title": "Running Locally"
        },
        {
            "location": "/deployment/run-local/#running-pravega-locally",
            "text": "Running locally allows you to get started using pravega very quickly. Most of the options use the standalone mode which is suitable for most development and testing.  All of the options for running locally start the required prerequisites so you can get started right away.",
            "title": "Running Pravega Locally"
        },
        {
            "location": "/deployment/run-local/#standalone-mode",
            "text": "",
            "title": "Standalone Mode"
        },
        {
            "location": "/deployment/run-local/#from-source",
            "text": "First you need to have the Pravega source code checked out:  git clone https://github.com/pravega/pravega.git\ncd pravega  This one command will download dependencies, compile pravega and start the standalone deployment.  ./gradlew startStandalone",
            "title": "From Source"
        },
        {
            "location": "/deployment/run-local/#from-installation-package",
            "text": "Download the Pravega release from the  github releases page . The tarball and zip are identical. Instructions are provided using tar but the same steps work with the zip file.  tar xfvz pravega-0.1.0.tgz  Run pravega standalone:  pravega-0.1.0/bin/pravega-standalone",
            "title": "From Installation Package"
        },
        {
            "location": "/deployment/run-local/#from-docker-container",
            "text": "This will download and run Pravega from the container image on docker hub.  Note, you must replace the  <ip>  with the IP of your machine. This is so that you can connect to Pravega from your local system. Optionally you can replace  latest  with the version of Pravega your desire  docker run -it -e HOST_IP=<ip> -p 9090:9090 -p 12345:12345 pravega/pravega:latest standalone",
            "title": "From Docker Container"
        },
        {
            "location": "/deployment/run-local/#docker-compose-distributed-mode",
            "text": "Unlike other options for running locally, the docker compose option runs a full Pravega install in distributed mode. It contains containers for running Zookeeper, Bookkeeper and HDFS so Pravega operates as if it would in production. This is as easy to get started with as the standalone option but requires additional resources available.  To use this you need to have Docker  1.12  or later.  Download the  docker-compose.yml  from github. For example:  wget https://github.com/pravega/pravega/tree/master/docker/compose/docker-compose.yml  You need to set the IP address of your local machine as the value of HOST_IP in the following command. To run:  HOST_IP=1.2.3.4 docker-compose up  Clients can then connect to the controller at  ${HOST_IP}:9090 .",
            "title": "Docker Compose (Distributed Mode)"
        },
        {
            "location": "/deployment/manual-install/",
            "text": "Manual Installation\n\n\nThis page describes the prerequisites and installation steps to deploy Pravega in a multi-node production environment.\n\n\nPrerequisites\n\n\nHDFS\n\n\nSetup an HDFS storage cluster running \nHDFS version 2.7+\n. HDFS is used as Tier 2 storage and must have\nsufficient capacity to store contents of all streams. The storage cluster is recommended to be run\nalongside Pravega on separate nodes.\n\n\nJava\n\n\nInstall the latest Java 8 from \njava.oracle.com\n. Packages are available\nfor all major operating systems.\n\n\nZookeeper\n\n\nPravega requires \nZookeeper 3.5.1-alpha+\n. At least 3 Zookeeper nodes are recommended for a quorum. No special configuration is required for Zookeeper but it is recommended to use a dedicated cluster for Pravega.\n\n\nThis specific version of Zookeeper can be downloaded from Apache at \nzookeeper-3.5.1-alpha.tar.gz\n.\n\n\nFor installing Zookeeper see the \nGetting Started Guide\n.\n\n\nBookkeeper\n\n\nPravega requires \nBookkeeper 4.4.0+\n. At least 3 Bookkeeper servers are recommended for a quorum.\n\n\nThis specific version of Bookkeeper can be downloaded from Apache at \nbookkeeper-server-4.4.0-bin.tar.gz\n.\n\n\nFor installing Bookkeeper see the \nGetting Started Guide\n.\nSome specific Pravega instructions are shown below. All sets assuming being run from the \nbookkeeper-server-4.4.0\n directory.\n\n\nBookkeeper Configuration\n\n\nThe following configuration options should be changed in the \nconf/bk_server.conf\n file.\n\n\n# Comma separated list of <zp-ip>:<port> for all ZK servers\nzkServers=localhost:2181\n\n# Alternatively specify a different path to the storage for /bk\njournalDirectory=/bk/journal\nledgerDirectories=/bk/ledgers\nindexDirectories=/bk/index\n\nzkLedgersRootPath=/pravega/bookkeeper/ledgers\n\n\n\n\nInitializing Zookeeper paths\n\n\nThe following paths need to be created in Zookeeper. From the \nzookeeper-3.5.1-alpha\n directory on the Zookeeper servers run:\n\n\nbin/zkCli.sh -server $ZK_URL create /pravega\nbin/zkCli.sh -server $ZK_URL create /pravega/bookkeeper\n\n\n\n\nRunning Bookkeeper\n\n\nBefore starting the bookie, it needs to be formatted:\n\n\nbin/bookkeeper shell metaformat -nonInteractive\n\n\n\n\nStart the bookie:\n\n\nbin/bookkeeper bookie\n\n\n\n\n\n\nInstalling Pravega\n\n\nFor non-production systems, you can use the containers provided by the \ndocker\n installation to run non-production HDFS, Zookeeper or Bookkeeper.\n\n\nThere are two key components to Pravega that need to be run:\n- Controller - Control plane for Pravega. Installation requires at least one controller. Two or more are recommended for HA.\n- Segment Store - Storage node for Pravega. Installation requires at least one segment store.\n\n\nBefore you start, you need to download the latest Pravega release. You can find the latest Pravega release on the \ngithub releases page\n.\n\n\nRecommendations\n\n\nIf you are getting started with a simple 3 node cluster, you may want to layout your services like this:\n\n\n\n\n\n\n\n\n\n\nNode 1\n\n\nNode 2\n\n\nNode 3\n\n\n\n\n\n\n\n\n\n\nZookeeper\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nBookkeeper\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\nPravega Controller\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nPravega Segment Store\n\n\nX\n\n\nX\n\n\nX\n\n\n\n\n\n\n\n\nAll Nodes\n\n\nOn each node extract the distribution package to your desired directory:\n\n\ntar xfvz pravega-0.1.0.tgz\ncd pravega-0.1.0\n\n\n\n\nInstalling the Controller\n\n\nThe controller can simply be run using the following command. Replace \n<zk-ip>\n with the IP address of the Zookeeper nodes\n\n\nZK_URL=<zk-ip>:2181 bin/pravega-controller\n\n\n\n\nAlternatively, instead of specifying this on startup each time, you can edit the \nconf/controller.conf\n file and change the zk url there:\n\n\n    zk {\n      url = \"<zk-ip>:2181\"\n...\n    }\n\n\n\n\nThen you can run the controller with:\n\n\nbin/pravega-controller\n\n\n\n\nInstalling the Segment Store\n\n\nEdit the \nconf/config.properties\n file. The following properies need to be changed. Replace \n<zk-ip>\n, \n<controller-ip>\n and \n<hdfs-ip>\n with the IPs of the respective services:\n\n\npravegaservice.zkURL=<zk-ip>:2181\nbookkeeper.zkAddress=<zk-ip>:2181\nautoScale.controllerUri=tcp://<controller-ip>:9090\n\n# Settings required for HDFS\nhdfs.hdfsUrl=<hdfs-ip>:8020\n\n\n\n\nOnce the configuration changes have been made you can start the segment store with:\n\n\nbin/pravega-segmentstore",
            "title": "Manual Install"
        },
        {
            "location": "/deployment/manual-install/#manual-installation",
            "text": "This page describes the prerequisites and installation steps to deploy Pravega in a multi-node production environment.",
            "title": "Manual Installation"
        },
        {
            "location": "/deployment/manual-install/#prerequisites",
            "text": "",
            "title": "Prerequisites"
        },
        {
            "location": "/deployment/manual-install/#hdfs",
            "text": "Setup an HDFS storage cluster running  HDFS version 2.7+ . HDFS is used as Tier 2 storage and must have\nsufficient capacity to store contents of all streams. The storage cluster is recommended to be run\nalongside Pravega on separate nodes.",
            "title": "HDFS"
        },
        {
            "location": "/deployment/manual-install/#java",
            "text": "Install the latest Java 8 from  java.oracle.com . Packages are available\nfor all major operating systems.",
            "title": "Java"
        },
        {
            "location": "/deployment/manual-install/#zookeeper",
            "text": "Pravega requires  Zookeeper 3.5.1-alpha+ . At least 3 Zookeeper nodes are recommended for a quorum. No special configuration is required for Zookeeper but it is recommended to use a dedicated cluster for Pravega.  This specific version of Zookeeper can be downloaded from Apache at  zookeeper-3.5.1-alpha.tar.gz .  For installing Zookeeper see the  Getting Started Guide .",
            "title": "Zookeeper"
        },
        {
            "location": "/deployment/manual-install/#bookkeeper",
            "text": "Pravega requires  Bookkeeper 4.4.0+ . At least 3 Bookkeeper servers are recommended for a quorum.  This specific version of Bookkeeper can be downloaded from Apache at  bookkeeper-server-4.4.0-bin.tar.gz .  For installing Bookkeeper see the  Getting Started Guide .\nSome specific Pravega instructions are shown below. All sets assuming being run from the  bookkeeper-server-4.4.0  directory.",
            "title": "Bookkeeper"
        },
        {
            "location": "/deployment/manual-install/#bookkeeper-configuration",
            "text": "The following configuration options should be changed in the  conf/bk_server.conf  file.  # Comma separated list of <zp-ip>:<port> for all ZK servers\nzkServers=localhost:2181\n\n# Alternatively specify a different path to the storage for /bk\njournalDirectory=/bk/journal\nledgerDirectories=/bk/ledgers\nindexDirectories=/bk/index\n\nzkLedgersRootPath=/pravega/bookkeeper/ledgers",
            "title": "Bookkeeper Configuration"
        },
        {
            "location": "/deployment/manual-install/#initializing-zookeeper-paths",
            "text": "The following paths need to be created in Zookeeper. From the  zookeeper-3.5.1-alpha  directory on the Zookeeper servers run:  bin/zkCli.sh -server $ZK_URL create /pravega\nbin/zkCli.sh -server $ZK_URL create /pravega/bookkeeper",
            "title": "Initializing Zookeeper paths"
        },
        {
            "location": "/deployment/manual-install/#running-bookkeeper",
            "text": "Before starting the bookie, it needs to be formatted:  bin/bookkeeper shell metaformat -nonInteractive  Start the bookie:  bin/bookkeeper bookie",
            "title": "Running Bookkeeper"
        },
        {
            "location": "/deployment/manual-install/#installing-pravega",
            "text": "For non-production systems, you can use the containers provided by the  docker  installation to run non-production HDFS, Zookeeper or Bookkeeper.  There are two key components to Pravega that need to be run:\n- Controller - Control plane for Pravega. Installation requires at least one controller. Two or more are recommended for HA.\n- Segment Store - Storage node for Pravega. Installation requires at least one segment store.  Before you start, you need to download the latest Pravega release. You can find the latest Pravega release on the  github releases page .",
            "title": "Installing Pravega"
        },
        {
            "location": "/deployment/manual-install/#recommendations",
            "text": "If you are getting started with a simple 3 node cluster, you may want to layout your services like this:      Node 1  Node 2  Node 3      Zookeeper  X  X  X    Bookkeeper  X  X  X    Pravega Controller  X  X     Pravega Segment Store  X  X  X",
            "title": "Recommendations"
        },
        {
            "location": "/deployment/manual-install/#all-nodes",
            "text": "On each node extract the distribution package to your desired directory:  tar xfvz pravega-0.1.0.tgz\ncd pravega-0.1.0",
            "title": "All Nodes"
        },
        {
            "location": "/deployment/manual-install/#installing-the-controller",
            "text": "The controller can simply be run using the following command. Replace  <zk-ip>  with the IP address of the Zookeeper nodes  ZK_URL=<zk-ip>:2181 bin/pravega-controller  Alternatively, instead of specifying this on startup each time, you can edit the  conf/controller.conf  file and change the zk url there:      zk {\n      url = \"<zk-ip>:2181\"\n...\n    }  Then you can run the controller with:  bin/pravega-controller",
            "title": "Installing the Controller"
        },
        {
            "location": "/deployment/manual-install/#installing-the-segment-store",
            "text": "Edit the  conf/config.properties  file. The following properies need to be changed. Replace  <zk-ip> ,  <controller-ip>  and  <hdfs-ip>  with the IPs of the respective services:  pravegaservice.zkURL=<zk-ip>:2181\nbookkeeper.zkAddress=<zk-ip>:2181\nautoScale.controllerUri=tcp://<controller-ip>:9090\n\n# Settings required for HDFS\nhdfs.hdfsUrl=<hdfs-ip>:8020  Once the configuration changes have been made you can start the segment store with:  bin/pravega-segmentstore",
            "title": "Installing the Segment Store"
        },
        {
            "location": "/deployment/docker-swarm/",
            "text": "Deploying in a Docker Swarm\n\n\nDocker Swarm can be used to quickly spin up a distributed Pravega cluster that can easily scale up and down. Unlike\n\ndocker-compose\n, this is useful for more than just testing and development, and in the future will be suitable\nfor production workloads.\n\n\nPrerequisites\n\n\n\n\n\n\nA working single or multi-node Docker Swarm. See https://docs.docker.com/engine/swarm/swarm-tutorial.\n\n\n\n\n\n\nHDFS and ZooKeeper. We provide compose files for both of these, but both are single instance deploys that should only\nbe used for testing/development.\n\n\n\n\n\n\nTo deploy our HDFS and ZooKeeper:\n\n\ndocker stack up --compose-file hdfs.yml pravega\ndocker stack up --compose-file zookeeper.yml pravega\n\n\n\n\nThis runs a single node HDFS container and single node ZooKeeper inside the \npravega_default\n overlay network, and adds\nthem to the \npravega\nstack. HDFS is reachable inside the swarm as \nhdfs://hdfs:8020\n, and ZooKeeper at \n\ntcp://zookeeper:2181\n.\n\n\nYou may run one or both of these to get up and running, but these shouldn't be used for serious workloads.\n\n\nNetwork Considerations\n\n\nEach Pravega Segment Store needs to be directly reachable by clients. Docker Swarm runs all traffic coming into\nits overlay network through a load balancer, which makes it more or less impossible to reach a specific instance\nof a scaled service from outside the cluster. This means that Pravega clients must either run inside the swarm, or\nwe must run each Segment Store as a unique service on a distinct port.\n\n\nBoth approaches will be demonstrated.\n\n\nDeploying (swarm only clients)\n\n\nThe easiest way to deploy is to keep all traffic inside the swarm. This means your client apps must also run inside\nthe swarm.\n\n\nZK_URL=zookeeper:2181 HDFS_URL=hdfs:8020 docker stack up --compose-file pravega.yml pravega\n\n\nNote that \nZK_URL\n and \nHDFS_URL\n don't include the protocol. They default to \nzookeeper:2181\n and \nhdfs:8020\n, so you \ncan omit them if they're reachable at those addresses (which they will be if you've deployed\n\nzookeeper.yml\n/\nhdfs.yml\n).\n\n\nYour clients must then be deployed into the swarm, with something like:\n\n\ndocker service create --name=myapp --network=pravega_default mycompany/myapp\n\n\nThe crucial bit being \n--network=pravega_default\n. Your client should talk to Pravega at \ntcp://controller:9090\n.\n\n\nDeploying (external clients)\n\n\nIf you intend to run clients outside the swarm, you must provide two additional environment variables, \n\nPUBLISHED_ADDRESS\n and \nLISTENING_ADDRESS\n. \nPUBLISHED_ADDRESS\n must be an IP or hostname that resolves to one or more\nswarm nodes (or a load balancer that sits in front of them). \nLISTENING_ADDRESS\n should always be \n0\n, or \n0.0.0.0\n.\n\n\nPUBLISHED_ADDRESS=1.2.3.4 LISTENING_ADDRESS=0 ZK_URL=zookeeper:2181 HDFS_URL=hdfs:8020 docker stack up --compose-file pravega.yml pravega\n\n\nAs above, \nZK_URL\n and \nHDFS_URL\n can be omitted if the services are at their default locations.\n\n\nYour client should talk to Pravega at \ntcp://${PUBLISHED_ADDRESS}:9090\n.\n\n\nScaling BookKeeper\n\n\nBookKeeper can be scaled up or down with:\n\n\ndocker service scale pravega_bookkeeper=N\n\n\nAs configured in this package, Pravega requires at least 3 BookKeeper nodes, so \nN\n must be >= 3.\n\n\nScaling Pravega Controller\n\n\nPravega Controller can be scaled up or down with:\n\n\ndocker service scale pravega_controller=N\n\n\nScaling Pravega Segment Store (swarm only clients)\n\n\nIf you app will run inside the swarm and you didn't run with \nPUBLISHED_ADDRESS\n, you can scale the Segment Store\nthe usual way:\n\n\ndocker service scale pravega_segmentstore=N\n\n\nScaling Pravega Segment Store (external clients)\n\n\nIf you require access to Pravega from outside the swarm and have deployed with \nPUBLISHED_ADDRESS\n, each instance\nof the Segment Store must be deployed as a unique service. This is a cumbersome process, but we've provided a helper\nscript to make it fairly painless:\n\n\n./scale_segmentstore N\n\n\nTearing down\n\n\nAll services (including HDFS and ZooKeeper if you've deployed our package) can be destroyed with:\n\n\ndocker stack down pravega",
            "title": "Deployment in Docker Swarm"
        },
        {
            "location": "/deployment/docker-swarm/#deploying-in-a-docker-swarm",
            "text": "Docker Swarm can be used to quickly spin up a distributed Pravega cluster that can easily scale up and down. Unlike docker-compose , this is useful for more than just testing and development, and in the future will be suitable\nfor production workloads.",
            "title": "Deploying in a Docker Swarm"
        },
        {
            "location": "/deployment/docker-swarm/#prerequisites",
            "text": "A working single or multi-node Docker Swarm. See https://docs.docker.com/engine/swarm/swarm-tutorial.    HDFS and ZooKeeper. We provide compose files for both of these, but both are single instance deploys that should only\nbe used for testing/development.    To deploy our HDFS and ZooKeeper:  docker stack up --compose-file hdfs.yml pravega\ndocker stack up --compose-file zookeeper.yml pravega  This runs a single node HDFS container and single node ZooKeeper inside the  pravega_default  overlay network, and adds\nthem to the  pravega stack. HDFS is reachable inside the swarm as  hdfs://hdfs:8020 , and ZooKeeper at  tcp://zookeeper:2181 .  You may run one or both of these to get up and running, but these shouldn't be used for serious workloads.",
            "title": "Prerequisites"
        },
        {
            "location": "/deployment/docker-swarm/#network-considerations",
            "text": "Each Pravega Segment Store needs to be directly reachable by clients. Docker Swarm runs all traffic coming into\nits overlay network through a load balancer, which makes it more or less impossible to reach a specific instance\nof a scaled service from outside the cluster. This means that Pravega clients must either run inside the swarm, or\nwe must run each Segment Store as a unique service on a distinct port.  Both approaches will be demonstrated.",
            "title": "Network Considerations"
        },
        {
            "location": "/deployment/docker-swarm/#deploying-swarm-only-clients",
            "text": "The easiest way to deploy is to keep all traffic inside the swarm. This means your client apps must also run inside\nthe swarm.  ZK_URL=zookeeper:2181 HDFS_URL=hdfs:8020 docker stack up --compose-file pravega.yml pravega  Note that  ZK_URL  and  HDFS_URL  don't include the protocol. They default to  zookeeper:2181  and  hdfs:8020 , so you \ncan omit them if they're reachable at those addresses (which they will be if you've deployed zookeeper.yml / hdfs.yml ).  Your clients must then be deployed into the swarm, with something like:  docker service create --name=myapp --network=pravega_default mycompany/myapp  The crucial bit being  --network=pravega_default . Your client should talk to Pravega at  tcp://controller:9090 .",
            "title": "Deploying (swarm only clients)"
        },
        {
            "location": "/deployment/docker-swarm/#deploying-external-clients",
            "text": "If you intend to run clients outside the swarm, you must provide two additional environment variables,  PUBLISHED_ADDRESS  and  LISTENING_ADDRESS .  PUBLISHED_ADDRESS  must be an IP or hostname that resolves to one or more\nswarm nodes (or a load balancer that sits in front of them).  LISTENING_ADDRESS  should always be  0 , or  0.0.0.0 .  PUBLISHED_ADDRESS=1.2.3.4 LISTENING_ADDRESS=0 ZK_URL=zookeeper:2181 HDFS_URL=hdfs:8020 docker stack up --compose-file pravega.yml pravega  As above,  ZK_URL  and  HDFS_URL  can be omitted if the services are at their default locations.  Your client should talk to Pravega at  tcp://${PUBLISHED_ADDRESS}:9090 .",
            "title": "Deploying (external clients)"
        },
        {
            "location": "/deployment/docker-swarm/#scaling-bookkeeper",
            "text": "BookKeeper can be scaled up or down with:  docker service scale pravega_bookkeeper=N  As configured in this package, Pravega requires at least 3 BookKeeper nodes, so  N  must be >= 3.",
            "title": "Scaling BookKeeper"
        },
        {
            "location": "/deployment/docker-swarm/#scaling-pravega-controller",
            "text": "Pravega Controller can be scaled up or down with:  docker service scale pravega_controller=N",
            "title": "Scaling Pravega Controller"
        },
        {
            "location": "/deployment/docker-swarm/#scaling-pravega-segment-store-swarm-only-clients",
            "text": "If you app will run inside the swarm and you didn't run with  PUBLISHED_ADDRESS , you can scale the Segment Store\nthe usual way:  docker service scale pravega_segmentstore=N",
            "title": "Scaling Pravega Segment Store (swarm only clients)"
        },
        {
            "location": "/deployment/docker-swarm/#scaling-pravega-segment-store-external-clients",
            "text": "If you require access to Pravega from outside the swarm and have deployed with  PUBLISHED_ADDRESS , each instance\nof the Segment Store must be deployed as a unique service. This is a cumbersome process, but we've provided a helper\nscript to make it fairly painless:  ./scale_segmentstore N",
            "title": "Scaling Pravega Segment Store (external clients)"
        },
        {
            "location": "/deployment/docker-swarm/#tearing-down",
            "text": "All services (including HDFS and ZooKeeper if you've deployed our package) can be destroyed with:  docker stack down pravega",
            "title": "Tearing down"
        },
        {
            "location": "/deployment/dcos-install/",
            "text": "Deploying on DC/OS\n\n\nPravega can be run on DC/OS by leveraging Marathon.\n\nPravegaGroup.json defines the docker hub image locations and necessary application configration to start a simple Pravega cluster.\n\n\nDownload  \nPravegaGroup.json\n to your DC/OS cluster.  For example:\n\n\nwget https://github.com/pravega/pravega/blob/master/PravegaGroup.json\n\n\n\n\nAdd to Marathon using:\n\n\ndcos marathon group add PravegaGroup.json",
            "title": "Deployment on DC/OS"
        },
        {
            "location": "/deployment/dcos-install/#deploying-on-dcos",
            "text": "Pravega can be run on DC/OS by leveraging Marathon. \nPravegaGroup.json defines the docker hub image locations and necessary application configration to start a simple Pravega cluster.  Download   PravegaGroup.json  to your DC/OS cluster.  For example:  wget https://github.com/pravega/pravega/blob/master/PravegaGroup.json  Add to Marathon using:  dcos marathon group add PravegaGroup.json",
            "title": "Deploying on DC/OS"
        },
        {
            "location": "/deployment/aws-install/",
            "text": "Running on AWS\n\n\nPre-reqs: Have an AWS account and have Terraform installed. To install and download Terraform, follow the instructions here: https://www.terraform.io/downloads.html\n\n\nDeploy Steps\n\n\n\n\nRun \"sudo terraform apply\" under the deployment/aws directory, and then follow prompt instruction, enter the AWS account credentials.\n\n\n\n\nThere are four variables would be needed:\n\n\n\n\nAWS access key and AWS secret key, which can be obtained from AWS account\n\n\ncred_path, which is the absolute path of key pair file. It would be downloaded when key pair is created\n\n\nAWS region: Currently, we only support two regions: us-east-1 and us-west-1. We list below the instance types we recommend for them.\n\n\nRegion us-east-1:\n\n\nThree m3.xlarge for EMR\n\n\nThree m3.2xlarge for Pravega\n\n\nOne m3.medium for bootstrap, also as client\n\n\n\n\n\n\nRegion us-west-1:\n\n\nThree m3.xlarge for EMR\n\n\nThree i3.4xlarge for Pravega\n\n\nOne i3.xlarge for bootstrap, also as client\n\n\n\n\n\n\n\n\nOther instance types might present conflicts with the Linux Images used.\n\n\nHow to customize the pravega cluster\n\n\n\n\nChange default value of \"pravega_num\" in variable.tf\n\n\nDefine the your own nodes layout in installer/hosts-template, default hosts-template is under installer directory.\n\n\n\n\nThere are three sections of hosts-template:\n1. common-services is the section for zookeeper and bookkeeper\n2. pravega-controller is the section for pravega controller node\n3. pravega-hosts is the section for the pravega segment store node.\n\n\nHow to destroy the pravega cluster\n\n\nRun \"sudo terraform destroy\", then enter \"yes\"",
            "title": "Running in the Cloud (AWS)"
        },
        {
            "location": "/deployment/aws-install/#running-on-aws",
            "text": "Pre-reqs: Have an AWS account and have Terraform installed. To install and download Terraform, follow the instructions here: https://www.terraform.io/downloads.html",
            "title": "Running on AWS"
        },
        {
            "location": "/deployment/aws-install/#deploy-steps",
            "text": "Run \"sudo terraform apply\" under the deployment/aws directory, and then follow prompt instruction, enter the AWS account credentials.   There are four variables would be needed:   AWS access key and AWS secret key, which can be obtained from AWS account  cred_path, which is the absolute path of key pair file. It would be downloaded when key pair is created  AWS region: Currently, we only support two regions: us-east-1 and us-west-1. We list below the instance types we recommend for them.  Region us-east-1:  Three m3.xlarge for EMR  Three m3.2xlarge for Pravega  One m3.medium for bootstrap, also as client    Region us-west-1:  Three m3.xlarge for EMR  Three i3.4xlarge for Pravega  One i3.xlarge for bootstrap, also as client     Other instance types might present conflicts with the Linux Images used.",
            "title": "Deploy Steps"
        },
        {
            "location": "/deployment/aws-install/#how-to-customize-the-pravega-cluster",
            "text": "Change default value of \"pravega_num\" in variable.tf  Define the your own nodes layout in installer/hosts-template, default hosts-template is under installer directory.   There are three sections of hosts-template:\n1. common-services is the section for zookeeper and bookkeeper\n2. pravega-controller is the section for pravega controller node\n3. pravega-hosts is the section for the pravega segment store node.",
            "title": "How to customize the pravega cluster"
        },
        {
            "location": "/deployment/aws-install/#how-to-destroy-the-pravega-cluster",
            "text": "Run \"sudo terraform destroy\", then enter \"yes\"",
            "title": "How to destroy the pravega cluster"
        },
        {
            "location": "/contributing/",
            "text": "Contributing to Pravega\n\n\n\n\nContributions guidelines\n\n\nIssue triaging and labeling\n\n\nReview process\n\n\n\n\nHappy hacking!",
            "title": "Coding guildelines"
        },
        {
            "location": "/contributing/#contributing-to-pravega",
            "text": "Contributions guidelines  Issue triaging and labeling  Review process   Happy hacking!",
            "title": "Contributing to Pravega"
        },
        {
            "location": "/roadmap/",
            "text": "Pravega Roadmap\n\n\nVersion 0.3\n\n\nThe following will be the primary feature focus areas for our upcoming release.\n\n\nRetention Policy Implementation\n\n\nRetention policies allow an operator to define a specific Stream size or data age.  Any data beyond this threshold will automatically be purged.\n\n\nTransactions API\n\n\nThe current transactions API is functional, however it is cumbersome and requires detailed knowledge of Pravega to configure appropriate values such as timeouts.  This work will simplify the API and automate as many timeouts as possible.\n\n\nExactly Once Guarantees\n\n\nFocus is on testing and strengthening exactly once guarantees and correctness under failure conditions.\n\n\nLow-level Reader API\n\n\nThis will expose a new low-level reader API that provides access the low level byte stream as opposed to the event semantics offered by the Java Client API.  This can also be leveraged in the future to build different flavors of reader groups.\n\n\nSecurity\n\n\nSecurity for this release will focus on support for securing Pravega external interfaces along with basic access controls on stream access and administration.\n-  Access Control on Stream operations\n-  Auth between Clients and Controller/SegmentStore\n-  Auth between SegmentStore and Tier 2 Storage\n\n\nPravega Connectors\n\n\nPravega ecosystem interconnectivity will be augmented with the following:\n-  Expanded Flink connector support (batch & table API support)\n-  Logstash connector\n-  Others also under consideration (Interested in writing a connector? Talk to us...)\n\n\nFuture Items\n\n\nThe following items are new features that we wish to build in upcoming Pravega releases, however no active work is currently underway.  Please reach out on the Pravega channels if you're interested in picking one of these up.\n\n\n\n\nOperational features\n\n\nNon-disruptive and rolling upgrades for Pravega\n\n\nProvide default Failure Detector\n\n\nExposing information for administration purposes\n\n\nAbility to define throughput quotas and other QoS guarantees\n\n\nPravega connectors / integration\n\n\nKafka API Compatibility (Producer and Consumer APIs)\n\n\nSpark connectors (source/sink)\n\n\nREST Proxy for Reader/Writer (REST proxy for Admin operations is already there)\n\n\nStream Management\n\n\nStream aliasing\n\n\nAbility to logically group multiple Streams\n\n\nAbility to assign arbitrary Key-Value pairs to streams - Tagging\n\n\nTiering Support\n\n\nPolicy driven tiering of Streams from Streaming Storage to Long-term storage\n\n\nSupport for additional Tier 2 Storage backends",
            "title": "Pravega Roadmap"
        },
        {
            "location": "/roadmap/#pravega-roadmap",
            "text": "",
            "title": "Pravega Roadmap"
        },
        {
            "location": "/roadmap/#version-03",
            "text": "The following will be the primary feature focus areas for our upcoming release.",
            "title": "Version 0.3"
        },
        {
            "location": "/roadmap/#retention-policy-implementation",
            "text": "Retention policies allow an operator to define a specific Stream size or data age.  Any data beyond this threshold will automatically be purged.",
            "title": "Retention Policy Implementation"
        },
        {
            "location": "/roadmap/#transactions-api",
            "text": "The current transactions API is functional, however it is cumbersome and requires detailed knowledge of Pravega to configure appropriate values such as timeouts.  This work will simplify the API and automate as many timeouts as possible.",
            "title": "Transactions API"
        },
        {
            "location": "/roadmap/#exactly-once-guarantees",
            "text": "Focus is on testing and strengthening exactly once guarantees and correctness under failure conditions.",
            "title": "Exactly Once Guarantees"
        },
        {
            "location": "/roadmap/#low-level-reader-api",
            "text": "This will expose a new low-level reader API that provides access the low level byte stream as opposed to the event semantics offered by the Java Client API.  This can also be leveraged in the future to build different flavors of reader groups.",
            "title": "Low-level Reader API"
        },
        {
            "location": "/roadmap/#security",
            "text": "Security for this release will focus on support for securing Pravega external interfaces along with basic access controls on stream access and administration.\n-  Access Control on Stream operations\n-  Auth between Clients and Controller/SegmentStore\n-  Auth between SegmentStore and Tier 2 Storage",
            "title": "Security"
        },
        {
            "location": "/roadmap/#pravega-connectors",
            "text": "Pravega ecosystem interconnectivity will be augmented with the following:\n-  Expanded Flink connector support (batch & table API support)\n-  Logstash connector\n-  Others also under consideration (Interested in writing a connector? Talk to us...)",
            "title": "Pravega Connectors"
        },
        {
            "location": "/roadmap/#future-items",
            "text": "The following items are new features that we wish to build in upcoming Pravega releases, however no active work is currently underway.  Please reach out on the Pravega channels if you're interested in picking one of these up.   Operational features  Non-disruptive and rolling upgrades for Pravega  Provide default Failure Detector  Exposing information for administration purposes  Ability to define throughput quotas and other QoS guarantees  Pravega connectors / integration  Kafka API Compatibility (Producer and Consumer APIs)  Spark connectors (source/sink)  REST Proxy for Reader/Writer (REST proxy for Admin operations is already there)  Stream Management  Stream aliasing  Ability to logically group multiple Streams  Ability to assign arbitrary Key-Value pairs to streams - Tagging  Tiering Support  Policy driven tiering of Streams from Streaming Storage to Long-term storage  Support for additional Tier 2 Storage backends",
            "title": "Future Items"
        },
        {
            "location": "/join-community/",
            "text": "Join the Pravega Community\n\n\nSlack Channel\n\n\nUser Groups\n\n\npravega-users@googlegroups.com\n\n\nDeveloper Mailing List\n\n\npravega-dev@googlegroups.com",
            "title": "Join the Community"
        },
        {
            "location": "/join-community/#join-the-pravega-community",
            "text": "Slack Channel",
            "title": "Join the Pravega Community"
        },
        {
            "location": "/join-community/#user-groups",
            "text": "pravega-users@googlegroups.com",
            "title": "User Groups"
        },
        {
            "location": "/join-community/#developer-mailing-list",
            "text": "pravega-dev@googlegroups.com",
            "title": "Developer Mailing List"
        }
    ]
}